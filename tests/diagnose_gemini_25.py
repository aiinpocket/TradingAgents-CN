#!/usr/bin/env python3
"""
è¨ºæ–·Gemini 2.5æ¨¡å‹å•é¡Œ
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# åŠ è¼‰ç’°å¢ƒè®Šé‡
load_dotenv(project_root / ".env", override=True)

def test_gemini_models():
    """æ¸¬è©¦ä¸åŒçš„Geminiæ¨¡å‹"""
    print("ğŸ§ª è¨ºæ–·Geminiæ¨¡å‹å•é¡Œ")
    print("=" * 60)
    
    models_to_test = [
        "gemini-2.5-pro",
        "gemini-2.5-flash", 
        "gemini-2.0-flash",
        "gemini-1.5-pro",
        "gemini-1.5-flash"
    ]
    
    google_api_key = os.getenv('GOOGLE_API_KEY')
    if not google_api_key:
        print("âŒ Google APIå¯†é‘°æœªé…ç½®")
        return
    
    print(f"âœ… Google APIå¯†é‘°å·²é…ç½®: {google_api_key[:20]}...")
    
    working_models = []
    
    for model_name in models_to_test:
        print(f"\nğŸ” æ¸¬è©¦æ¨¡å‹: {model_name}")
        print("-" * 40)
        
        try:
            # æ¸¬è©¦ç›´æ¥API
            print("ğŸ“ æ¸¬è©¦ç›´æ¥Google API...")
            import google.generativeai as genai
            genai.configure(api_key=google_api_key)
            
            model = genai.GenerativeModel(model_name)
            response = model.generate_content("è«‹ç”¨ä¸­æ–‡èªªï¼šä½ å¥½ï¼Œæˆ‘æ˜¯Geminiæ¨¡å‹")
            
            if response and response.text:
                print(f"âœ… ç›´æ¥APIæˆåŠŸ: {response.text[:100]}...")
                direct_success = True
            else:
                print("âŒ ç›´æ¥APIå¤±è´¥ï¼šç„¡éŸ¿æ‡‰")
                direct_success = False
                
        except Exception as e:
            print(f"âŒ ç›´æ¥APIå¤±è´¥: {e}")
            direct_success = False
        
        try:
            # æ¸¬è©¦LangChain
            print("ğŸ“ æ¸¬è©¦LangChainé›†æˆ...")
            from langchain_google_genai import ChatGoogleGenerativeAI
            
            llm = ChatGoogleGenerativeAI(
                model=model_name,
                temperature=0.1,
                max_tokens=200,
                google_api_key=google_api_key
            )
            
            response = llm.invoke("è«‹ç”¨ä¸­æ–‡ç°¡å–®ä»‹ç´¹ä¸€ä¸‹ä½ è‡ªå·±")
            
            if response and response.content:
                print(f"âœ… LangChainæˆåŠŸ: {response.content[:100]}...")
                langchain_success = True
            else:
                print("âŒ LangChainå¤±è´¥ï¼šç„¡éŸ¿æ‡‰")
                langchain_success = False
                
        except Exception as e:
            print(f"âŒ LangChainå¤±è´¥: {e}")
            langchain_success = False
        
        # è¨˜éŒ„çµæœ
        if direct_success or langchain_success:
            working_models.append({
                'name': model_name,
                'direct': direct_success,
                'langchain': langchain_success
            })
            print(f"âœ… {model_name} éƒ¨åˆ†æˆ–å®Œå…¨å¯ç”¨")
        else:
            print(f"âŒ {model_name} å®Œå…¨ä¸å¯ç”¨")
    
    return working_models

def test_best_working_model(working_models):
    """æ¸¬è©¦æœ€ä½³å¯ç”¨æ¨¡å‹"""
    if not working_models:
        print("\nâŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹")
        return None
    
    # é¸æ“‡æœ€ä½³æ¨¡å‹ï¼ˆå„ªå…ˆé¸æ“‡2.5ç‰ˆæœ¬ï¼Œç„¶å¾Œæ˜¯LangChainå¯ç”¨çš„ï¼‰
    best_model = None
    for model in working_models:
        if model['langchain']:  # LangChainå¯ç”¨
            if '2.5' in model['name']:  # å„ªå…ˆ2.5ç‰ˆæœ¬
                best_model = model['name']
                break
            elif best_model is None:  # å¦‚æœè¿˜æ²¡æœ‰é¸æ“‡ï¼Œå°±é¸é€™å€‹
                best_model = model['name']
    
    if best_model is None:
        # å¦‚æœæ²¡æœ‰LangChainå¯ç”¨çš„ï¼Œé¸æ“‡ç›´æ¥APIå¯ç”¨çš„
        for model in working_models:
            if model['direct']:
                best_model = model['name']
                break
    
    if best_model:
        print(f"\nğŸ¯ é¸æ“‡æœ€ä½³æ¨¡å‹é€²è¡Œè©³ç´°æ¸¬è©¦: {best_model}")
        print("=" * 60)
        
        try:
            from langchain_google_genai import ChatGoogleGenerativeAI
            
            llm = ChatGoogleGenerativeAI(
                model=best_model,
                temperature=0.1,
                max_tokens=800,
                google_api_key=os.getenv('GOOGLE_API_KEY')
            )
            
            # æ¸¬è©¦è‚¡ç¥¨åˆ†æ
            print("ğŸ“Š æ¸¬è©¦è‚¡ç¥¨åˆ†æèƒ½åŠ›...")
            response = llm.invoke("""
            è«‹ç”¨ä¸­æ–‡åˆ†æè‹¹æœå…¬å¸(AAPL)çš„æŠ•è³‡åƒ¹å€¼ã€‚
            è«‹ç°¡è¦åˆ†æï¼š
            1. å…¬å¸å„ªåŠ¿
            2. ä¸»è¦é¢¨éšª
            3. æŠ•è³‡å»ºè®®
            """)
            
            if response and response.content and len(response.content) > 100:
                print("âœ… è‚¡ç¥¨åˆ†ææ¸¬è©¦æˆåŠŸ")
                print(f"   éŸ¿æ‡‰é•·åº¦: {len(response.content)} å­—ç¬¦")
                print(f"   éŸ¿æ‡‰é è¦½: {response.content[:200]}...")
                return best_model
            else:
                print("âŒ è‚¡ç¥¨åˆ†ææ¸¬è©¦å¤±è´¥")
                return None
                
        except Exception as e:
            print(f"âŒ è©³ç´°æ¸¬è©¦å¤±è´¥: {e}")
            return None
    
    return None

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ§ª Geminiæ¨¡å‹è¨ºæ–·")
    print("=" * 70)
    
    # æ¸¬è©¦æ‰€æœ‰æ¨¡å‹
    working_models = test_gemini_models()
    
    # é¡¯ç¤ºçµæœ
    print(f"\nğŸ“Š æ¸¬è©¦çµæœæ€»çµ:")
    print("=" * 50)
    
    if working_models:
        print(f"âœ… æ‰¾åˆ° {len(working_models)} å€‹å¯ç”¨æ¨¡å‹:")
        for model in working_models:
            direct_status = "âœ…" if model['direct'] else "âŒ"
            langchain_status = "âœ…" if model['langchain'] else "âŒ"
            print(f"   {model['name']}: ç›´æ¥API {direct_status} | LangChain {langchain_status}")
        
        # æ¸¬è©¦æœ€ä½³æ¨¡å‹
        best_model = test_best_working_model(working_models)
        
        if best_model:
            print(f"\nğŸ‰ æ¨è–¦ä½¿ç”¨æ¨¡å‹: {best_model}")
            print(f"\nğŸ’¡ é…ç½®å»ºè®®:")
            print(f"   1. åœ¨Webç•Œé¢ä¸­é¸æ“‡'Google'ä½œç‚ºLLMæä¾›å•†")
            print(f"   2. ä½¿ç”¨æ¨¡å‹åç¨±: {best_model}")
            print(f"   3. è¯¥æ¨¡å‹å·²é€šéè‚¡ç¥¨åˆ†ææ¸¬è©¦")
        else:
            print(f"\nâš ï¸ è™½ç„¶æ‰¾åˆ°å¯ç”¨æ¨¡å‹ï¼Œä½†è©³ç´°æ¸¬è©¦å¤±è´¥")
            print(f"   å»ºè®®ä½¿ç”¨: {working_models[0]['name']}")
    else:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯ç”¨çš„Geminiæ¨¡å‹")
        print("ğŸ’¡ å¯èƒ½çš„åŸå› :")
        print("   1. APIå¯†é‘°æ¬Šé™ä¸è¶³")
        print("   2. ç¶²çµ¡é€£æ¥å•é¡Œ")
        print("   3. æ¨¡å‹åç¨±å·²æ›´æ–°")
        print("   4. APIé…é¡é™åˆ¶")

if __name__ == "__main__":
    main()
