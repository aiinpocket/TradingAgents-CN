#!/usr/bin/env python3
"""
æ—¥èªŒåˆ†æå·¥å…·
åˆ†æTradingAgents-CNçš„æ—¥èªŒæ–‡ä»¶ï¼Œæä¾›çµ±è¨ˆå’Œæ´å¯Ÿ
"""

import json
import re
import sys
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from collections import defaultdict, Counter
import argparse

# å°å…¥æ—¥èªŒæ¨¡å¡Š
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('scripts')



class LogAnalyzer:
    """æ—¥èªŒåˆ†æå™¨"""
    
    def __init__(self, log_file: Path):
        self.log_file = log_file
        self.entries = []
        self.structured_entries = []
        
    def parse_logs(self):
        """è§£ææ—¥èªŒæ–‡ä»¶"""
        if not self.log_file.exists():
            logger.error(f"âŒ æ—¥èªŒæ–‡ä»¶ä¸å­˜åœ¨: {self.log_file}")
            return
            
        logger.info(f"ğŸ“– è§£ææ—¥èªŒæ–‡ä»¶: {self.log_file}")
        
        with open(self.log_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                    
                # å˜—è©¦è§£æçµæ§‹åŒ–æ—¥èªŒï¼ˆJSONï¼‰
                if line.startswith('{'):
                    try:
                        entry = json.loads(line)
                        entry['line_number'] = line_num
                        self.structured_entries.append(entry)
                        continue
                    except json.JSONDecodeError:
                        pass
                
                # è§£ææ™®é€šæ—¥èªŒ
                entry = self._parse_regular_log(line, line_num)
                if entry:
                    self.entries.append(entry)
        
        logger.info(f"âœ… è§£æå®Œæˆ: {len(self.entries)} æ¢æ™®é€šæ—¥èªŒ, {len(self.structured_entries)} æ¢çµæ§‹åŒ–æ—¥èªŒ")
    
    def _parse_regular_log(self, line: str, line_num: int) -> Optional[Dict[str, Any]]:
        """è§£ææ™®é€šæ—¥èªŒè¡Œ"""
        # åŒ¹é…æ ¼å¼: 2025-01-15 10:30:45,123 | module_name | INFO | message
        pattern = r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3}) \| ([^|]+) \| ([^|]+) \| (.+)'
        match = re.match(pattern, line)
        
        if match:
            timestamp_str, logger_name, level, message = match.groups()
            try:
                timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S,%f')
            except ValueError:
                timestamp = None
                
            return {
                'timestamp': timestamp,
                'logger': logger_name.strip(),
                'level': level.strip(),
                'message': message.strip(),
                'line_number': line_num,
                'raw_line': line
            }
        
        return None
    
    def analyze_performance(self) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½ç›¸é—œæ—¥èªŒ"""
        logger.info(f"\nğŸ“Š æ€§èƒ½åˆ†æ")
        logger.info(f"=")
        
        analysis = {
            'slow_operations': [],
            'analysis_times': [],
            'token_usage': [],
            'cost_summary': {'total_cost': 0, 'by_provider': defaultdict(float)}
        }
        
        # åˆ†ææ‰€æœ‰æ—¥èªŒæ¢ç›®
        all_entries = self.entries + self.structured_entries
        
        for entry in all_entries:
            message = entry.get('message', '')
            
            # æª¢æ¸¬æ…¢æ“ä½œ
            if 'è€—æ™‚' in message or 'duration' in entry:
                duration = self._extract_duration(message, entry)
                if duration and duration > 5.0:  # è¶…é5ç§’
                    analysis['slow_operations'].append({
                        'timestamp': entry.get('timestamp'),
                        'duration': duration,
                        'message': message,
                        'logger': entry.get('logger', '')
                    })
            
            # åˆ†æå®Œæˆæ™‚é–“
            if 'åˆ†æå®Œæˆ' in message or 'analysis_complete' in entry.get('event_type', ''):
                duration = self._extract_duration(message, entry)
                if duration:
                    analysis['analysis_times'].append(duration)
            
            # Tokenä½¿ç”¨çµ±è¨ˆ
            if 'Tokenä½¿ç”¨' in message or 'token_usage' in entry.get('event_type', ''):
                cost = self._extract_cost(message, entry)
                provider = self._extract_provider(message, entry)
                if cost:
                    analysis['cost_summary']['total_cost'] += cost
                    if provider:
                        analysis['cost_summary']['by_provider'][provider] += cost
        
        # è¼¸å‡ºåˆ†æçµæœ
        if analysis['slow_operations']:
            logger.info(f"ğŸŒ æ…¢æ“ä½œ ({len(analysis['slow_operations'])} å€‹):")
            for op in analysis['slow_operations'][:5]:  # é¡¯ç¤ºå‰5å€‹
                logger.info(f"  - {op['duration']:.2f}s: {op['message'][:80]}...")
        
        if analysis['analysis_times']:
            avg_time = sum(analysis['analysis_times']) / len(analysis['analysis_times'])
            logger.info(f"â±ï¸  å¹³å‡åˆ†ææ™‚é–“: {avg_time:.2f}s")
            logger.info(f"ğŸ“ˆ åˆ†ææ¬¡æ•¸: {len(analysis['analysis_times'])}")
        
        if analysis['cost_summary']['total_cost'] > 0:
            logger.info(f"ğŸ’° æ€»æˆæœ¬: Â¥{analysis['cost_summary']['total_cost']:.4f}")
            for provider, cost in analysis['cost_summary']['by_provider'].items():
                logger.info(f"  - {provider}: Â¥{cost:.4f}")
        
        return analysis
    
    def analyze_errors(self) -> Dict[str, Any]:
        """åˆ†æéŒ¯èª¤æ—¥èªŒ"""
        logger.error(f"\nâŒ éŒ¯èª¤åˆ†æ")
        logger.info(f"=")
        
        error_entries = []
        warning_entries = []
        
        all_entries = self.entries + self.structured_entries
        
        for entry in all_entries:
            level = entry.get('level', '').upper()
            if level == 'ERROR':
                error_entries.append(entry)
            elif level == 'WARNING':
                warning_entries.append(entry)
        
        logger.error(f"ğŸ”´ éŒ¯èª¤æ•¸é‡: {len(error_entries)}")
        logger.warning(f"ğŸŸ¡ è­¦å‘Šæ•¸é‡: {len(warning_entries)}")
        
        # éŒ¯èª¤åˆ†é¡
        error_patterns = defaultdict(int)
        for entry in error_entries:
            message = entry.get('message', '')
            # ç°¡å–®çš„éŒ¯èª¤åˆ†é¡
            if 'API' in message or 'api' in message:
                error_patterns['APIéŒ¯èª¤'] += 1
            elif 'ç¶²çµ¡' in message or 'network' in message or 'connection' in message:
                error_patterns['ç¶²çµ¡éŒ¯èª¤'] += 1
            elif 'æ•¸æ“šåº«' in message or 'database' in message or 'mongodb' in message:
                error_patterns['æ•¸æ“šåº«éŒ¯èª¤'] += 1
            elif 'PDF' in message or 'pdf' in message:
                error_patterns['PDFå°å‡ºéŒ¯èª¤'] += 1
            else:
                error_patterns['å…¶ä»–éŒ¯èª¤'] += 1
        
        if error_patterns:
            logger.error(f"\néŒ¯èª¤åˆ†é¡:")
            for pattern, count in error_patterns.most_common():
                logger.info(f"  - {pattern}: {count}")
        
        # é¡¯ç¤ºæœ€è¿‘çš„éŒ¯èª¤
        if error_entries:
            logger.error(f"\næœ€è¿‘çš„éŒ¯èª¤:")
            recent_errors = sorted(error_entries, key=lambda x: x.get('timestamp', datetime.min))[-3:]
            for error in recent_errors:
                timestamp = error.get('timestamp', 'Unknown')
                message = error.get('message', '')[:100]
                logger.info(f"  - {timestamp}: {message}...")
        
        return {
            'error_count': len(error_entries),
            'warning_count': len(warning_entries),
            'error_patterns': dict(error_patterns),
            'recent_errors': error_entries[-5:] if error_entries else []
        }
    
    def analyze_usage(self) -> Dict[str, Any]:
        """åˆ†æä½¿ç”¨æƒ…å†µ"""
        logger.info(f"\nğŸ“ˆ ä½¿ç”¨æƒ…å†µåˆ†æ")
        logger.info(f"=")
        
        analysis = {
            'daily_usage': defaultdict(int),
            'hourly_usage': defaultdict(int),
            'module_usage': defaultdict(int),
            'analysis_types': defaultdict(int)
        }
        
        all_entries = self.entries + self.structured_entries
        
        for entry in all_entries:
            timestamp = entry.get('timestamp')
            if timestamp:
                # æŒ‰æ—¥çµ±è¨ˆ
                date_str = timestamp.strftime('%Y-%m-%d')
                analysis['daily_usage'][date_str] += 1
                
                # æŒ‰å°æ™‚çµ±è¨ˆ
                hour = timestamp.hour
                analysis['hourly_usage'][hour] += 1
            
            # æ¨¡å¡Šä½¿ç”¨çµ±è¨ˆ
            logger = entry.get('logger', '')
            if logger:
                analysis['module_usage'][logger] += 1
            
            # åˆ†æé¡å‹çµ±è¨ˆ
            message = entry.get('message', '')
            if 'é–‹å§‹åˆ†æ' in message or 'analysis_start' in entry.get('event_type', ''):
                analysis_type = entry.get('analysis_type', 'æœªçŸ¥')
                analysis['analysis_types'][analysis_type] += 1
        
        # è¼¸å‡ºçµæœ
        if analysis['daily_usage']:
            logger.info(f"ğŸ“… æ¯æ—¥ä½¿ç”¨é‡:")
            for date, count in sorted(analysis['daily_usage'].items())[-7:]:  # æœ€è¿‘7å¤©
                logger.info(f"  - {date}: {count}")
        
        if analysis['module_usage']:
            logger.info(f"\nğŸ“¦ æ¨¡å¡Šä½¿ç”¨æƒ…å†µ:")
            for module, count in Counter(analysis['module_usage']).most_common(5):
                logger.info(f"  - {module}: {count}")
        
        if analysis['analysis_types']:
            logger.debug(f"\nğŸ” åˆ†æé¡å‹:")
            for analysis_type, count in Counter(analysis['analysis_types']).most_common():
                logger.info(f"  - {analysis_type}: {count}")
        
        return analysis
    
    def _extract_duration(self, message: str, entry: Dict[str, Any]) -> Optional[float]:
        """å¾æ¶ˆæ¯ä¸­æå–è€—æ™‚"""
        # å¾çµæ§‹åŒ–æ—¥èªŒä¸­æå–
        if 'duration' in entry:
            return entry['duration']
        
        # å¾æ¶ˆæ¯ä¸­æå–
        match = re.search(r'è€—æ™‚[ï¼š:]\s*(\d+\.?\d*)s', message)
        if match:
            return float(match.group(1))
        
        return None
    
    def _extract_cost(self, message: str, entry: Dict[str, Any]) -> Optional[float]:
        """å¾æ¶ˆæ¯ä¸­æå–æˆæœ¬"""
        # å¾çµæ§‹åŒ–æ—¥èªŒä¸­æå–
        if 'cost' in entry:
            return entry['cost']
        
        # å¾æ¶ˆæ¯ä¸­æå–
        match = re.search(r'æˆæœ¬[ï¼š:]\s*Â¥(\d+\.?\d*)', message)
        if match:
            return float(match.group(1))
        
        return None
    
    def _extract_provider(self, message: str, entry: Dict[str, Any]) -> Optional[str]:
        """å¾æ¶ˆæ¯ä¸­æå–æä¾›å•†"""
        # å¾çµæ§‹åŒ–æ—¥èªŒä¸­æå–
        if 'provider' in entry:
            return entry['provider']
        
        # å¾æ¶ˆæ¯ä¸­æå–
        providers = ['DeepSeek', 'OpenAI', 'Tongyi', 'Gemini']
        for provider in providers:
            if provider in message:
                return provider
        
        return None
    
    def generate_report(self) -> str:
        """ç”Ÿæˆåˆ†æå ±å‘Š"""
        logger.info(f"\nğŸ“‹ ç”Ÿæˆåˆ†æå ±å‘Š")
        logger.info(f"=")
        
        performance = self.analyze_performance()
        errors = self.analyze_errors()
        usage = self.analyze_usage()
        
        report = f"""
# TradingAgents-CN æ—¥èªŒåˆ†æå ±å‘Š

ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
æ—¥èªŒæ–‡ä»¶: {self.log_file}

## æ¦‚è¦½
- æ™®é€šæ—¥èªŒæ¢ç›®: {len(self.entries)}
- çµæ§‹åŒ–æ—¥èªŒæ¢ç›®: {len(self.structured_entries)}
- éŒ¯èª¤æ•¸é‡: {errors['error_count']}
- è­¦å‘Šæ•¸é‡: {errors['warning_count']}

## æ€§èƒ½åˆ†æ
- æ…¢æ“ä½œæ•¸é‡: {len(performance['slow_operations'])}
- å¹³å‡åˆ†ææ™‚é–“: {sum(performance['analysis_times']) / len(performance['analysis_times']):.2f}s (å¦‚æœæœ‰æ•¸æ“š)
- æ€»æˆæœ¬: Â¥{performance['cost_summary']['total_cost']:.4f}

## ä½¿ç”¨æƒ…å†µ
- æ´»èºæ¨¡å¡Š: {len(usage['module_usage'])}
- åˆ†æé¡å‹: {len(usage['analysis_types'])}

## å»ºè®®
"""
        
        # æ·»åŠ å»ºè®®
        if len(performance['slow_operations']) > 10:
            report += "- âš ï¸ æª¢æ¸¬åˆ°è¼ƒå¤šæ…¢æ“ä½œï¼Œå»ºè®®å„ªåŒ–æ€§èƒ½\n"
        
        if errors['error_count'] > 0:
            report += f"- âŒ ç™¼ç¾ {errors['error_count']} å€‹éŒ¯èª¤ï¼Œå»ºè®®æª¢æŸ¥æ—¥èªŒ\n"
        
        if performance['cost_summary']['total_cost'] > 10:
            report += "- ğŸ’° APIæˆæœ¬è¼ƒé«˜ï¼Œå»ºè®®å„ªåŒ–èª¿ç”¨ç­–ç•¥\n"
        
        return report


def main():
    parser = argparse.ArgumentParser(description='TradingAgents-CN æ—¥èªŒåˆ†æå·¥å…·')
    parser.add_argument('log_file', help='æ—¥èªŒæ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--output', '-o', help='è¼¸å‡ºå ±å‘Šæ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--format', choices=['text', 'json'], default='text', help='è¼¸å‡ºæ ¼å¼')
    
    args = parser.parse_args()
    
    log_file = Path(args.log_file)
    analyzer = LogAnalyzer(log_file)
    
    try:
        analyzer.parse_logs()
        report = analyzer.generate_report()
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"ğŸ“„ å ±å‘Šå·²ä¿å­˜åˆ°: {args.output}")
        else:
            print(report)
            
    except Exception as e:
        logger.error(f"âŒ åˆ†æå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
