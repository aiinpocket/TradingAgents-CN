#!/usr/bin/env python3
"""
æ•¸æ“šç›®éŒ„é‡æ–°çµ„ç¹”é·ç§»è…³æœ¬
Data Directory Reorganization Migration Script

æ­¤è…³æœ¬å°‡é …ç›®ä¸­åˆ†æ•£çš„æ•¸æ“šç›®éŒ„é‡æ–°çµ„ç¹”ç‚ºçµ±ä¸€çš„çµæ§‹
"""

import os
import shutil
import json
from pathlib import Path
from typing import Dict, List, Tuple
import logging
from datetime import datetime

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('data_migration.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DataDirectoryMigrator:
    """æ•¸æ“šç›®éŒ„é·ç§»å™¨"""
    
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path(__file__).parent.parent
        self.backup_dir = self.project_root / f"data_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # æ–°çš„ç›®éŒ„çµæ§‹
        self.new_structure = {
            'data': {
                'cache': ['stock_data', 'news_data', 'fundamentals', 'metadata'],
                'analysis_results': ['summary', 'detailed', 'exports'],
                'databases': ['mongodb', 'redis'],
                'sessions': ['web_sessions', 'cli_sessions'],
                'logs': ['application', 'operations', 'user_activities'],
                'config': ['user_configs', 'system_configs'],
                'temp': ['downloads', 'processing']
            }
        }
        
        # é·ç§»æ˜ å°„ï¼š(æºè·¯å¾‘, ç›®æ¨™è·¯å¾‘)
        self.migration_map = [
            # ç·©å­˜æ•¸æ“šé·ç§»
            ('tradingagents/dataflows/data_cache', 'data/cache'),
            
            # åˆ†æçµæœé·ç§»
            ('results', 'data/analysis_results/detailed'),
            ('web/data/analysis_results', 'data/analysis_results/summary'),
            
            # æ•¸æ“šåº«æ•¸æ“šé·ç§»
            ('data/mongodb', 'data/databases/mongodb'),
            ('data/redis', 'data/databases/redis'),
            
            # æœƒè©±æ•¸æ“šé·ç§»
            ('data/sessions', 'data/sessions/cli_sessions'),
            ('web/data/sessions', 'data/sessions/web_sessions'),
            
            # æ—¥èªŒæ•¸æ“šé·ç§»
            ('web/data/operation_logs', 'data/logs/operations'),
            ('web/data/user_activities', 'data/logs/user_activities'),
            
            # å ±å‘Šæ•¸æ“šé·ç§»ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            ('data/reports', 'data/analysis_results/exports'),
        ]
    
    def create_backup(self) -> bool:
        """å‰µå»ºæ•¸æ“šå‚™ä»½"""
        try:
            logger.info(f"ğŸ”„ é–‹å§‹å‰µå»ºæ•¸æ“šå‚™ä»½åˆ°: {self.backup_dir}")
            self.backup_dir.mkdir(exist_ok=True)
            
            # å‚™ä»½ç¾æœ‰æ•¸æ“šç›®éŒ„
            backup_paths = ['data', 'web/data', 'results', 'tradingagents/dataflows/data_cache']
            
            for path in backup_paths:
                source = self.project_root / path
                if source.exists():
                    target = self.backup_dir / path
                    target.parent.mkdir(parents=True, exist_ok=True)
                    
                    if source.is_dir():
                        shutil.copytree(source, target, dirs_exist_ok=True)
                    else:
                        shutil.copy2(source, target)
                    
                    logger.info(f"  âœ… å·²å‚™ä»½: {path}")
            
            logger.info(f"âœ… æ•¸æ“šå‚™ä»½å®Œæˆ: {self.backup_dir}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å‚™ä»½å¤±æ•—: {e}")
            return False
    
    def create_new_structure(self) -> bool:
        """å‰µå»ºæ–°çš„ç›®éŒ„çµæ§‹"""
        try:
            logger.info("ğŸ”„ å‰µå»ºæ–°çš„ç›®éŒ„çµæ§‹...")
            
            for root_dir, subdirs in self.new_structure.items():
                root_path = self.project_root / root_dir
                root_path.mkdir(exist_ok=True)
                
                if isinstance(subdirs, dict):
                    for subdir, sub_subdirs in subdirs.items():
                        subdir_path = root_path / subdir
                        subdir_path.mkdir(exist_ok=True)
                        
                        for sub_subdir in sub_subdirs:
                            (subdir_path / sub_subdir).mkdir(exist_ok=True)
                            
                        logger.info(f"  âœ… å‰µå»ºç›®éŒ„: {subdir_path.relative_to(self.project_root)}")
                elif isinstance(subdirs, list):
                    for subdir in subdirs:
                        subdir_path = root_path / subdir
                        subdir_path.mkdir(exist_ok=True)
                        logger.info(f"  âœ… å‰µå»ºç›®éŒ„: {subdir_path.relative_to(self.project_root)}")
            
            logger.info("âœ… æ–°ç›®éŒ„çµæ§‹å‰µå»ºå®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å‰µå»ºç›®éŒ„çµæ§‹å¤±æ•—: {e}")
            return False
    
    def migrate_data(self) -> bool:
        """é·ç§»æ•¸æ“š"""
        try:
            logger.info("ğŸ”„ é–‹å§‹æ•¸æ“šé·ç§»...")
            
            for source_path, target_path in self.migration_map:
                source = self.project_root / source_path
                target = self.project_root / target_path
                
                if not source.exists():
                    logger.info(f"  â­ï¸ è·³éä¸å­˜åœ¨çš„è·¯å¾‘: {source_path}")
                    continue
                
                # ç¢ºä¿ç›®æ¨™ç›®éŒ„å­˜åœ¨
                target.parent.mkdir(parents=True, exist_ok=True)
                
                try:
                    if source.is_dir():
                        # å¦‚æœç›®æ¨™å·²å­˜åœ¨ï¼Œåˆä¸¦å…§å®¹
                        if target.exists():
                            self._merge_directories(source, target)
                        else:
                            shutil.copytree(source, target)
                    else:
                        shutil.copy2(source, target)
                    
                    logger.info(f"  âœ… é·ç§»å®Œæˆ: {source_path} â†’ {target_path}")
                    
                except Exception as e:
                    logger.error(f"  âŒ é·ç§»å¤±æ•—: {source_path} â†’ {target_path}, éŒ¯èª¤: {e}")
            
            logger.info("âœ… æ•¸æ“šé·ç§»å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šé·ç§»å¤±æ•—: {e}")
            return False
    
    def _merge_directories(self, source: Path, target: Path):
        """åˆä¸¦ç›®éŒ„å…§å®¹"""
        for item in source.rglob('*'):
            if item.is_file():
                relative_path = item.relative_to(source)
                target_file = target / relative_path
                target_file.parent.mkdir(parents=True, exist_ok=True)
                
                # å¦‚æœç›®æ¨™æ–‡ä»¶å·²å­˜åœ¨ï¼Œé‡å‘½å
                if target_file.exists():
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    target_file = target_file.with_name(f"{target_file.stem}_{timestamp}{target_file.suffix}")
                
                shutil.copy2(item, target_file)
    
    def update_env_file(self) -> bool:
        """æ›´æ–°.envæ–‡ä»¶"""
        try:
            logger.info("ğŸ”„ æ›´æ–°.envæ–‡ä»¶...")
            
            env_file = self.project_root / '.env'
            if not env_file.exists():
                logger.warning("âš ï¸ .envæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³éæ›´æ–°")
                return True
            
            # è®€å–ç¾æœ‰å…§å®¹
            with open(env_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ·»åŠ æ–°çš„ç’°å¢ƒè®Šé‡é…ç½®
            new_config = """
# ===== æ•¸æ“šç›®éŒ„é…ç½® (é‡æ–°çµ„ç¹”å¾Œ) =====
# çµ±ä¸€æ•¸æ“šæ ¹ç›®éŒ„
TRADINGAGENTS_DATA_DIR=./data

# å­ç›®éŒ„é…ç½®ï¼ˆå¯é¸ï¼Œä½¿ç”¨é»˜èªå€¼ï¼‰
TRADINGAGENTS_CACHE_DIR=${TRADINGAGENTS_DATA_DIR}/cache
TRADINGAGENTS_SESSIONS_DIR=${TRADINGAGENTS_DATA_DIR}/sessions
TRADINGAGENTS_LOGS_DIR=${TRADINGAGENTS_DATA_DIR}/logs
TRADINGAGENTS_CONFIG_DIR=${TRADINGAGENTS_DATA_DIR}/config
TRADINGAGENTS_TEMP_DIR=${TRADINGAGENTS_DATA_DIR}/temp

# æ›´æ–°çµæœç›®éŒ„é…ç½®
TRADINGAGENTS_RESULTS_DIR=${TRADINGAGENTS_DATA_DIR}/analysis_results
"""
            
            # å¦‚æœé‚„æ²’æœ‰é€™äº›é…ç½®ï¼Œå‰‡æ·»åŠ 
            if 'TRADINGAGENTS_DATA_DIR' not in content:
                content += new_config
                
                with open(env_file, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                logger.info("âœ… .envæ–‡ä»¶æ›´æ–°å®Œæˆ")
            else:
                logger.info("â„¹ï¸ .envæ–‡ä»¶å·²åŒ…å«æ•¸æ“šç›®éŒ„é…ç½®")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°.envæ–‡ä»¶å¤±æ•—: {e}")
            return False
    
    def create_migration_report(self) -> bool:
        """å‰µå»ºé·ç§»å ±å‘Š"""
        try:
            report = {
                'migration_date': datetime.now().isoformat(),
                'project_root': str(self.project_root),
                'backup_location': str(self.backup_dir),
                'new_structure': self.new_structure,
                'migration_map': self.migration_map,
                'status': 'completed'
            }
            
            report_file = self.project_root / 'data_migration_report.json'
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… é·ç§»å ±å‘Šå·²ä¿å­˜: {report_file}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å‰µå»ºé·ç§»å ±å‘Šå¤±æ•—: {e}")
            return False
    
    def cleanup_old_directories(self, confirm: bool = False) -> bool:
        """æ¸…ç†èˆŠç›®éŒ„ï¼ˆå¯é¸ï¼‰"""
        if not confirm:
            logger.info("âš ï¸ è·³éæ¸…ç†èˆŠç›®éŒ„ï¼ˆéœ€è¦æ‰‹å‹•ç¢ºèªï¼‰")
            return True
        
        try:
            logger.info("ğŸ”„ æ¸…ç†èˆŠç›®éŒ„...")
            
            # è¦æ¸…ç†çš„èˆŠç›®éŒ„
            old_dirs = [
                'web/data',
                'tradingagents/dataflows/data_cache'
            ]
            
            for old_dir in old_dirs:
                old_path = self.project_root / old_dir
                if old_path.exists():
                    shutil.rmtree(old_path)
                    logger.info(f"  âœ… å·²åˆªé™¤: {old_dir}")
            
            logger.info("âœ… èˆŠç›®éŒ„æ¸…ç†å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†èˆŠç›®éŒ„å¤±æ•—: {e}")
            return False
    
    def run_migration(self, cleanup_old: bool = False) -> bool:
        """é‹è¡Œå®Œæ•´çš„é·ç§»æµç¨‹"""
        logger.info("ğŸš€ é–‹å§‹æ•¸æ“šç›®éŒ„é‡æ–°çµ„ç¹”é·ç§»...")
        
        steps = [
            ("å‰µå»ºå‚™ä»½", self.create_backup),
            ("å‰µå»ºæ–°ç›®éŒ„çµæ§‹", self.create_new_structure),
            ("é·ç§»æ•¸æ“š", self.migrate_data),
            ("æ›´æ–°ç’°å¢ƒè®Šé‡", self.update_env_file),
            ("å‰µå»ºé·ç§»å ±å‘Š", self.create_migration_report),
        ]
        
        if cleanup_old:
            steps.append(("æ¸…ç†èˆŠç›®éŒ„", lambda: self.cleanup_old_directories(True)))
        
        for step_name, step_func in steps:
            logger.info(f"\nğŸ“‹ åŸ·è¡Œæ­¥é©Ÿ: {step_name}")
            if not step_func():
                logger.error(f"âŒ æ­¥é©Ÿå¤±æ•—: {step_name}")
                return False
        
        logger.info("\nğŸ‰ æ•¸æ“šç›®éŒ„é‡æ–°çµ„ç¹”å®Œæˆï¼")
        logger.info(f"ğŸ“ å‚™ä»½ä½ç½®: {self.backup_dir}")
        logger.info(f"ğŸ“Š æ–°æ•¸æ“šç›®éŒ„: {self.project_root / 'data'}")
        
        return True


def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•¸æ“šç›®éŒ„é‡æ–°çµ„ç¹”é·ç§»è…³æœ¬')
    parser.add_argument('--project-root', help='é …ç›®æ ¹ç›®éŒ„è·¯å¾‘')
    parser.add_argument('--cleanup-old', action='store_true', help='é·ç§»å¾Œæ¸…ç†èˆŠç›®éŒ„')
    parser.add_argument('--dry-run', action='store_true', help='åƒ…é¡¯ç¤ºé·ç§»è¨ˆåŠƒï¼Œä¸åŸ·è¡Œå¯¦éš›é·ç§»')
    
    args = parser.parse_args()
    
    migrator = DataDirectoryMigrator(args.project_root)
    
    if args.dry_run:
        logger.info("ğŸ” é·ç§»è¨ˆåŠƒé è¦½:")
        logger.info(f"ğŸ“ é …ç›®æ ¹ç›®éŒ„: {migrator.project_root}")
        logger.info(f"ğŸ“ å‚™ä»½ç›®éŒ„: {migrator.backup_dir}")
        logger.info("\nğŸ“‹ é·ç§»æ˜ å°„:")
        for source, target in migrator.migration_map:
            logger.info(f"  {source} â†’ {target}")
        return
    
    # åŸ·è¡Œé·ç§»
    success = migrator.run_migration(cleanup_old=args.cleanup_old)
    
    if success:
        logger.info("\nâœ… é·ç§»æˆåŠŸå®Œæˆï¼")
        logger.info("\nğŸ“ å¾ŒçºŒæ­¥é©Ÿ:")
        logger.info("1. é©—è­‰æ–°ç›®éŒ„çµæ§‹æ˜¯å¦æ­£ç¢º")
        logger.info("2. æ¸¬è©¦æ‡‰ç”¨ç¨‹åºåŠŸèƒ½")
        logger.info("3. ç¢ºèªç„¡èª¤å¾Œå¯åˆªé™¤å‚™ä»½ç›®éŒ„")
    else:
        logger.error("\nâŒ é·ç§»å¤±æ•—ï¼è«‹æª¢æŸ¥æ—¥èªŒä¸¦å¾å‚™ä»½æ¢å¾©")


if __name__ == '__main__':
    main()