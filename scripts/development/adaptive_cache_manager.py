#!/usr/bin/env python3
"""
è‡ªé©æ‡‰ç·©å­˜ç®¡ç†å™¨ - æ ¹æ“šå¯ç”¨æœå‹™è‡ªå‹•é¸æ“‡æœ€ä½³ç·©å­˜ç­–ç•¥
æ”¯æŒæ–‡ä»¶ç·©å­˜ã€Redisç·©å­˜ã€MongoDBç·©å­˜çš„æ™ºèƒ½åˆ‡æ›
"""

import os
import json
import pickle
import hashlib
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, Optional, Union
import pandas as pd

# å°å…¥æ—¥èªŒæ¨¡å¡Š
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('scripts')

# å°å…¥æ™ºèƒ½é…ç½®
try:
    from smart_config import get_smart_config, get_config
    SMART_CONFIG_AVAILABLE = True
except ImportError:
    SMART_CONFIG_AVAILABLE = False

class AdaptiveCacheManager:
    """è‡ªé©æ‡‰ç·©å­˜ç®¡ç†å™¨ - æ™ºèƒ½é¸æ“‡ç·©å­˜å¾Œç«¯"""
    
    def __init__(self, cache_dir: str = "data_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # è¨­ç½®æ—¥èªŒ
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ç²å–æ™ºèƒ½é…ç½®
        self._load_smart_config()
        
        # åˆå§‹åŒ–ç·©å­˜å¾Œç«¯
        self._init_backends()
        
        self.logger.info(f"ç·©å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œä¸»è¦å¾Œç«¯: {self.primary_backend}")
    
    def _load_smart_config(self):
        """åŠ è¼‰æ™ºèƒ½é…ç½®"""
        if SMART_CONFIG_AVAILABLE:
            try:
                config_manager = get_smart_config()
                self.config = config_manager.get_config()
                self.primary_backend = self.config["cache"]["primary_backend"]
                self.mongodb_enabled = self.config["database"]["mongodb"]["enabled"]
                self.redis_enabled = self.config["database"]["redis"]["enabled"]
                self.fallback_enabled = self.config["cache"]["fallback_enabled"]
                self.ttl_settings = self.config["cache"]["ttl_settings"]
                
                self.logger.info("âœ… æ™ºèƒ½é…ç½®åŠ è¼‰æˆåŠŸ")
                return
            except Exception as e:
                self.logger.warning(f"æ™ºèƒ½é…ç½®åŠ è¼‰å¤±æ•—: {e}")
        
        # é»˜èªé…ç½®ï¼ˆç´”æ–‡ä»¶ç·©å­˜ï¼‰
        self.config = {
            "cache": {
                "primary_backend": "file",
                "fallback_enabled": True,
                "ttl_settings": {
                    "us_stock_data": 7200,
                    "china_stock_data": 3600,
                    "us_news": 21600,
                    "china_news": 14400,
                    "us_fundamentals": 86400,
                    "china_fundamentals": 43200,
                }
            }
        }
        self.primary_backend = "file"
        self.mongodb_enabled = False
        self.redis_enabled = False
        self.fallback_enabled = True
        self.ttl_settings = self.config["cache"]["ttl_settings"]
        
        self.logger.info("ä½¿ç”¨é»˜èªé…ç½®ï¼ˆç´”æ–‡ä»¶ç·©å­˜ï¼‰")
    
    def _init_backends(self):
        """åˆå§‹åŒ–ç·©å­˜å¾Œç«¯"""
        self.mongodb_client = None
        self.redis_client = None
        
        # åˆå§‹åŒ–MongoDB
        if self.mongodb_enabled:
            try:
                import pymongo
                self.mongodb_client = pymongo.MongoClient(
                    'localhost', 27017, 
                    serverSelectionTimeoutMS=2000
                )
                # æ¸¬è©¦é€£æ¥
                self.mongodb_client.server_info()
                self.mongodb_db = self.mongodb_client.tradingagents
                self.logger.info("âœ… MongoDBå¾Œç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"MongoDBåˆå§‹åŒ–å¤±æ•—: {e}")
                self.mongodb_enabled = False
                self.mongodb_client = None
        
        # åˆå§‹åŒ–Redis
        if self.redis_enabled:
            try:
                import redis

                self.redis_client = redis.Redis(
                    host='localhost', port=6379, 
                    socket_timeout=2
                )
                # æ¸¬è©¦é€£æ¥
                self.redis_client.ping()
                self.logger.info("âœ… Rediså¾Œç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"Redisåˆå§‹åŒ–å¤±æ•—: {e}")
                self.redis_enabled = False
                self.redis_client = None
        
        # å¦‚æœä¸»è¦å¾Œç«¯ä¸å¯ç”¨ï¼Œè‡ªå‹•é™ç´š
        if self.primary_backend == "redis" and not self.redis_enabled:
            if self.mongodb_enabled:
                self.primary_backend = "mongodb"
                self.logger.info("Redisä¸å¯ç”¨ï¼Œé™ç´šåˆ°MongoDB")
            else:
                self.primary_backend = "file"
                self.logger.info("Redisä¸å¯ç”¨ï¼Œé™ç´šåˆ°æ–‡ä»¶ç·©å­˜")
        
        elif self.primary_backend == "mongodb" and not self.mongodb_enabled:
            if self.redis_enabled:
                self.primary_backend = "redis"
                self.logger.info("MongoDBä¸å¯ç”¨ï¼Œé™ç´šåˆ°Redis")
            else:
                self.primary_backend = "file"
                self.logger.info("MongoDBä¸å¯ç”¨ï¼Œé™ç´šåˆ°æ–‡ä»¶ç·©å­˜")
    
    def _get_cache_key(self, symbol: str, start_date: str, end_date: str, 
                      data_source: str = "default") -> str:
        """ç”Ÿæˆç·©å­˜éµ"""
        key_data = f"{symbol}_{start_date}_{end_date}_{data_source}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def _get_ttl_seconds(self, symbol: str, data_type: str = "stock_data") -> int:
        """ç²å–TTLç§’æ•¸"""
        # åˆ¤æ–·å¸‚å ´é¡å‹
        if len(symbol) == 6 and symbol.isdigit():
            market = "china"
        else:
            market = "us"
        
        # ç²å–TTLé…ç½®
        ttl_key = f"{market}_{data_type}"
        ttl_hours = self.ttl_settings.get(ttl_key, 7200)  # é»˜èª2å°æ™‚
        return ttl_hours
    
    def _is_cache_valid(self, cache_time: datetime, ttl_seconds: int) -> bool:
        """æª¢æŸ¥ç·©å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if cache_time is None:
            return False
        
        expiry_time = cache_time + timedelta(seconds=ttl_seconds)
        return datetime.now() < expiry_time
    
    def _save_to_file(self, cache_key: str, data: Any, metadata: Dict) -> bool:
        """ä¿å­˜åˆ°æ–‡ä»¶ç·©å­˜"""
        try:
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            cache_data = {
                'data': data,
                'metadata': metadata,
                'timestamp': datetime.now()
            }
            
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            
            return True
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶ç·©å­˜ä¿å­˜å¤±æ•—: {e}")
            return False
    
    def _load_from_file(self, cache_key: str) -> Optional[Dict]:
        """å¾æ–‡ä»¶ç·©å­˜åŠ è¼‰"""
        try:
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            if not cache_file.exists():
                return None
            
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            return cache_data
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶ç·©å­˜åŠ è¼‰å¤±æ•—: {e}")
            return None
    
    def _save_to_redis(self, cache_key: str, data: Any, metadata: Dict, ttl_seconds: int) -> bool:
        """ä¿å­˜åˆ°Redisç·©å­˜"""
        if not self.redis_client:
            return False
        
        try:
            cache_data = {
                'data': data,
                'metadata': metadata,
                'timestamp': datetime.now().isoformat()
            }
            
            serialized_data = pickle.dumps(cache_data)
            self.redis_client.setex(cache_key, ttl_seconds, serialized_data)
            return True
        except Exception as e:
            self.logger.error(f"Redisç·©å­˜ä¿å­˜å¤±æ•—: {e}")
            return False
    
    def _load_from_redis(self, cache_key: str) -> Optional[Dict]:
        """å¾Redisç·©å­˜åŠ è¼‰"""
        if not self.redis_client:
            return None
        
        try:
            serialized_data = self.redis_client.get(cache_key)
            if not serialized_data:
                return None
            
            cache_data = pickle.loads(serialized_data)
            # è½‰æ›æ™‚é–“æˆ³
            if isinstance(cache_data['timestamp'], str):
                cache_data['timestamp'] = datetime.fromisoformat(cache_data['timestamp'])
            
            return cache_data
        except Exception as e:
            self.logger.error(f"Redisç·©å­˜åŠ è¼‰å¤±æ•—: {e}")
            return None
    
    def save_stock_data(self, symbol: str, data: Any, start_date: str = None, 
                       end_date: str = None, data_source: str = "default") -> str:
        """ä¿å­˜è‚¡ç¥¨æ•¸æ“šåˆ°ç·©å­˜"""
        # ç”Ÿæˆç·©å­˜éµ
        cache_key = self._get_cache_key(symbol, start_date or "", end_date or "", data_source)
        
        # æº–å‚™å…ƒæ•¸æ“š
        metadata = {
            'symbol': symbol,
            'start_date': start_date,
            'end_date': end_date,
            'data_source': data_source,
            'data_type': 'stock_data'
        }
        
        # ç²å–TTL
        ttl_seconds = self._get_ttl_seconds(symbol, 'stock_data')
        
        # æ ¹æ“šä¸»è¦å¾Œç«¯ä¿å­˜
        success = False
        
        if self.primary_backend == "redis":
            success = self._save_to_redis(cache_key, data, metadata, ttl_seconds)
        elif self.primary_backend == "mongodb":
            # MongoDBä¿å­˜é‚è¼¯ï¼ˆç°¡åŒ–ç‰ˆï¼‰
            success = self._save_to_file(cache_key, data, metadata)
        
        # å¦‚æœä¸»è¦å¾Œç«¯å¤±æ•—ï¼Œä½¿ç”¨æ–‡ä»¶ç·©å­˜ä½œç‚ºå‚™ç”¨
        if not success and self.fallback_enabled:
            success = self._save_to_file(cache_key, data, metadata)
            if success:
                self.logger.info(f"ä½¿ç”¨æ–‡ä»¶ç·©å­˜å‚™ç”¨ä¿å­˜: {cache_key}")
        
        if success:
            self.logger.info(f"æ•¸æ“šä¿å­˜æˆåŠŸ: {symbol} -> {cache_key}")
        else:
            self.logger.error(f"æ•¸æ“šä¿å­˜å¤±æ•—: {symbol}")
        
        return cache_key
    
    def load_stock_data(self, cache_key: str) -> Optional[Any]:
        """å¾ç·©å­˜åŠ è¼‰è‚¡ç¥¨æ•¸æ“š"""
        cache_data = None
        
        # æ ¹æ“šä¸»è¦å¾Œç«¯åŠ è¼‰
        if self.primary_backend == "redis":
            cache_data = self._load_from_redis(cache_key)
        elif self.primary_backend == "mongodb":
            # MongoDBåŠ è¼‰é‚è¼¯ï¼ˆç°¡åŒ–ç‰ˆï¼‰
            cache_data = self._load_from_file(cache_key)
        
        # å¦‚æœä¸»è¦å¾Œç«¯å¤±æ•—ï¼Œå˜—è©¦æ–‡ä»¶ç·©å­˜
        if not cache_data and self.fallback_enabled:
            cache_data = self._load_from_file(cache_key)
            if cache_data:
                self.logger.info(f"ä½¿ç”¨æ–‡ä»¶ç·©å­˜å‚™ç”¨åŠ è¼‰: {cache_key}")
        
        if not cache_data:
            return None
        
        # æª¢æŸ¥ç·©å­˜æ˜¯å¦æœ‰æ•ˆ
        symbol = cache_data['metadata'].get('symbol', '')
        data_type = cache_data['metadata'].get('data_type', 'stock_data')
        ttl_seconds = self._get_ttl_seconds(symbol, data_type)
        
        if not self._is_cache_valid(cache_data['timestamp'], ttl_seconds):
            self.logger.info(f"ç·©å­˜å·²éæœŸ: {cache_key}")
            return None
        
        return cache_data['data']
    
    def find_cached_stock_data(self, symbol: str, start_date: str = None, 
                              end_date: str = None, data_source: str = "default") -> Optional[str]:
        """æŸ¥æ‰¾ç·©å­˜çš„è‚¡ç¥¨æ•¸æ“š"""
        cache_key = self._get_cache_key(symbol, start_date or "", end_date or "", data_source)
        
        # æª¢æŸ¥ç·©å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
        if self.load_stock_data(cache_key) is not None:
            return cache_key
        
        return None
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ç²å–ç·©å­˜çµ±è¨ˆä¿¡æ¯"""
        stats = {
            'primary_backend': self.primary_backend,
            'mongodb_enabled': self.mongodb_enabled,
            'redis_enabled': self.redis_enabled,
            'fallback_enabled': self.fallback_enabled,
            'cache_directory': str(self.cache_dir),
            'file_cache_count': len(list(self.cache_dir.glob("*.pkl"))),
        }
        
        # Redisçµ±è¨ˆ
        if self.redis_client:
            try:
                redis_info = self.redis_client.info()
                stats['redis_memory_used'] = redis_info.get('used_memory_human', 'N/A')
                stats['redis_keys'] = self.redis_client.dbsize()
            except:
                stats['redis_status'] = 'Error'
        
        return stats


# å…¨å±€ç·©å­˜ç®¡ç†å™¨å¯¦ä¾‹
_cache_manager = None

def get_cache() -> AdaptiveCacheManager:
    """ç²å–å…¨å±€è‡ªé©æ‡‰ç·©å­˜ç®¡ç†å™¨"""
    global _cache_manager
    if _cache_manager is None:
        _cache_manager = AdaptiveCacheManager()
    return _cache_manager


def main():
    """æ¸¬è©¦è‡ªé©æ‡‰ç·©å­˜ç®¡ç†å™¨"""
    logger.info(f"ğŸ”§ æ¸¬è©¦è‡ªé©æ‡‰ç·©å­˜ç®¡ç†å™¨")
    logger.info(f"=")
    
    # å‰µå»ºç·©å­˜ç®¡ç†å™¨
    cache = get_cache()
    
    # é¡¯ç¤ºç‹€æ…‹
    stats = cache.get_cache_stats()
    logger.info(f"\nğŸ“Š ç·©å­˜ç‹€æ…‹:")
    for key, value in stats.items():
        logger.info(f"  {key}: {value}")
    
    # æ¸¬è©¦ç·©å­˜åŠŸèƒ½
    logger.info(f"\nğŸ’¾ æ¸¬è©¦ç·©å­˜åŠŸèƒ½...")
    
    test_data = "æ¸¬è©¦è‚¡ç¥¨æ•¸æ“š - AAPL"
    cache_key = cache.save_stock_data(
        symbol="AAPL",
        data=test_data,
        start_date="2024-01-01",
        end_date="2024-12-31",
        data_source="test"
    )
    logger.info(f"âœ… æ•¸æ“šä¿å­˜: {cache_key}")
    
    # åŠ è¼‰æ•¸æ“š
    loaded_data = cache.load_stock_data(cache_key)
    if loaded_data == test_data:
        logger.info(f"âœ… æ•¸æ“šåŠ è¼‰æˆåŠŸ")
    else:
        logger.error(f"âŒ æ•¸æ“šåŠ è¼‰å¤±æ•—")
    
    # æŸ¥æ‰¾ç·©å­˜
    found_key = cache.find_cached_stock_data(
        symbol="AAPL",
        start_date="2024-01-01",
        end_date="2024-12-31",
        data_source="test"
    )
    
    if found_key:
        logger.info(f"âœ… ç·©å­˜æŸ¥æ‰¾æˆåŠŸ: {found_key}")
    else:
        logger.error(f"âŒ ç·©å­˜æŸ¥æ‰¾å¤±æ•—")
    
    logger.info(f"\nğŸ‰ è‡ªé©æ‡‰ç·©å­˜ç®¡ç†å™¨æ¸¬è©¦å®Œæˆ!")


if __name__ == "__main__":
    main()
