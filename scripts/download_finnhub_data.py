#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Finnhubæ•¸æ“šä¸‹è¼‰è…³æœ¬

é€™å€‹è…³æœ¬ç”¨æ–¼å¾Finnhub APIä¸‹è¼‰æ–°èæ•¸æ“šã€å…§éƒ¨äººæƒ…ç·’æ•¸æ“šå’Œå…§éƒ¨äººäº¤æ˜“æ•¸æ“šã€‚
æ”¯æŒæ‰¹é‡ä¸‹è¼‰å’Œå¢é‡æ›´æ–°ã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/download_finnhub_data.py --data-type news --symbols AAPL,TSLA,MSFT
    python scripts/download_finnhub_data.py --all
    python scripts/download_finnhub_data.py --force-refresh
"""

import os
import sys
import json
import argparse
import requests
import time
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å°å…¥é …ç›®æ¨¡å¡Š
try:
    from tradingagents.utils.logging_manager import get_logger
    from tradingagents.config.config_manager import config_manager
    logger = get_logger('finnhub_downloader')
except ImportError as e:
    print(f"âŒ å°å…¥æ¨¡å¡Šå¤±æ•—: {e}")
    print("è«‹ç¢ºä¿åœ¨é …ç›®æ ¹ç›®éŒ„é‹è¡Œæ­¤è…³æœ¬")
    sys.exit(1)

class FinnhubDataDownloader:
    """Finnhubæ•¸æ“šä¸‹è¼‰å™¨"""
    
    def __init__(self, api_key: str = None, data_dir: str = None):
        """
        åˆå§‹åŒ–ä¸‹è¼‰å™¨
        
        Args:
            api_key: Finnhub APIå¯†é‘°
            data_dir: æ•¸æ“šå­˜å„²ç›®éŒ„
        """
        # ç²å–APIå¯†é‘°
        self.api_key = api_key or os.getenv('FINNHUB_API_KEY')
        if not self.api_key:
            raise ValueError("âŒ æœªæ‰¾åˆ°Finnhub APIå¯†é‘°ï¼Œè«‹è¨­ç½®FINNHUB_API_KEYç’°å¢ƒè®Šé‡")
        
        # ç²å–æ•¸æ“šç›®éŒ„
        if data_dir:
            self.data_dir = data_dir
        else:
            # å„ªå…ˆä½¿ç”¨ç’°å¢ƒè®Šé‡ï¼Œç„¶å¾Œæ˜¯é …ç›®æ ¹ç›®éŒ„
            env_data_dir = os.getenv('TRADINGAGENTS_DATA_DIR')
            if env_data_dir:
                self.data_dir = env_data_dir
            else:
                # ä½¿ç”¨é …ç›®æ ¹ç›®éŒ„ä¸‹çš„dataç›®éŒ„
                self.data_dir = str(project_root / "data")

            logger.info(f"ğŸ” æ•¸æ“šç›®éŒ„ä¾†æº: {'ç’°å¢ƒè®Šé‡' if env_data_dir else 'é …ç›®æ ¹ç›®éŒ„'}")
        
        self.base_url = "https://finnhub.io/api/v1"
        self.session = requests.Session()
        
        logger.info(f"ğŸ“ æ•¸æ“šç›®éŒ„: {self.data_dir}")
        logger.info(f"ğŸ”‘ APIå¯†é‘°: {self.api_key[:8]}...")
    
    def _make_request(self, endpoint: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç™¼é€APIè«‹æ±‚
        
        Args:
            endpoint: APIç«¯é»
            params: è«‹æ±‚åƒæ•¸
            
        Returns:
            APIéŸ¿æ‡‰æ•¸æ“š
        """
        params['token'] = self.api_key
        url = f"{self.base_url}/{endpoint}"
        
        try:
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            # æª¢æŸ¥APIé™åˆ¶
            if response.status_code == 429:
                logger.warning("âš ï¸ APIèª¿ç”¨é »ç‡é™åˆ¶ï¼Œç­‰å¾…60ç§’...")
                time.sleep(60)
                return self._make_request(endpoint, params)
            
            return response.json()
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ APIè«‹æ±‚å¤±æ•—: {e}")
            return {}
    
    def download_news_data(self, symbols: List[str], days: int = 7, force_refresh: bool = False):
        """
        ä¸‹è¼‰æ–°èæ•¸æ“š
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç¢¼åˆ—è¡¨
            days: ä¸‹è¼‰å¤šå°‘å¤©çš„æ•¸æ“š
            force_refresh: æ˜¯å¦å¼·åˆ¶åˆ·æ–°
        """
        logger.info(f"ğŸ“° é–‹å§‹ä¸‹è¼‰æ–°èæ•¸æ“šï¼Œè‚¡ç¥¨: {symbols}, å¤©æ•¸: {days}")
        
        # å‰µå»ºç›®éŒ„
        news_dir = Path(self.data_dir) / "finnhub_data" / "news_data"
        news_dir.mkdir(parents=True, exist_ok=True)
        
        # è¨ˆç®—æ—¥æœŸç¯„åœ
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        for symbol in symbols:
            logger.info(f"ğŸ“° ä¸‹è¼‰ {symbol} çš„æ–°èæ•¸æ“š...")
            
            # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
            file_path = news_dir / f"{symbol}_data_formatted.json"
            if file_path.exists() and not force_refresh:
                # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å…§å®¹
                try:
                    file_size = file_path.stat().st_size
                    if file_size > 10:  # æ–‡ä»¶å¤§å°å¤§æ–¼10å­—ç¯€æ‰èªç‚ºæœ‰æ•ˆ
                        logger.info(f"ğŸ“„ {symbol} æ•¸æ“šæ–‡ä»¶å·²å­˜åœ¨ä¸”æœ‰æ•ˆ (å¤§å°: {file_size} å­—ç¯€)ï¼Œè·³éä¸‹è¼‰")
                        continue
                    else:
                        logger.warning(f"âš ï¸ {symbol} æ•¸æ“šæ–‡ä»¶å­˜åœ¨ä½†ç‚ºç©º (å¤§å°: {file_size} å­—ç¯€)ï¼Œé‡æ–°ä¸‹è¼‰")
                except Exception as e:
                    logger.warning(f"âš ï¸ æª¢æŸ¥ {symbol} æ–‡ä»¶ç‹€æ…‹å¤±æ•—: {e}ï¼Œé‡æ–°ä¸‹è¼‰")

            logger.info(f"ğŸ“¥ é–‹å§‹ä¸‹è¼‰ {symbol} çš„æ–°èæ•¸æ“š...")
            
            # ä¸‹è¼‰æ–°èæ•¸æ“š
            params = {
                'symbol': symbol,
                'from': start_date.strftime('%Y-%m-%d'),
                'to': end_date.strftime('%Y-%m-%d')
            }
            
            news_data = self._make_request('company-news', params)

            logger.info(f"ğŸ” APIéŸ¿æ‡‰é¡å‹: {type(news_data)}, é•·åº¦: {len(news_data) if isinstance(news_data, list) else 'N/A'}")

            if news_data and isinstance(news_data, list) and len(news_data) > 0:
                # æ ¼å¼åŒ–æ•¸æ“š
                formatted_data = []
                for item in news_data:
                    formatted_item = {
                        'datetime': item.get('datetime', 0),
                        'headline': item.get('headline', ''),
                        'summary': item.get('summary', ''),
                        'url': item.get('url', ''),
                        'source': item.get('source', ''),
                        'category': item.get('category', ''),
                        'sentiment': item.get('sentiment', {})
                    }
                    formatted_data.append(formatted_item)

                # ä¿å­˜æ•¸æ“š
                try:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        json.dump(formatted_data, f, ensure_ascii=False, indent=2)

                    # é©—è­‰æ–‡ä»¶ä¿å­˜
                    if file_path.exists():
                        file_size = file_path.stat().st_size
                        logger.info(f"âœ… {symbol} æ–°èæ•¸æ“šå·²ä¿å­˜: {len(formatted_data)} æ¢, æ–‡ä»¶å¤§å°: {file_size} å­—ç¯€")
                    else:
                        logger.error(f"âŒ {symbol} æ–‡ä»¶ä¿å­˜å¤±æ•—ï¼Œæ–‡ä»¶ä¸å­˜åœ¨")

                except Exception as e:
                    logger.error(f"âŒ {symbol} æ–‡ä»¶ä¿å­˜ç•°å¸¸: {e}")

            elif news_data and isinstance(news_data, dict):
                logger.warning(f"âš ï¸ {symbol} APIè¿”å›å­—å…¸è€Œéåˆ—è¡¨: {news_data}")
            else:
                logger.warning(f"âš ï¸ {symbol} æ–°èæ•¸æ“šä¸‹è¼‰å¤±æ•—æˆ–ç‚ºç©º")
            
            # é¿å…APIé™åˆ¶
            time.sleep(1)
    
    def download_insider_sentiment(self, symbols: List[str], force_refresh: bool = False):
        """
        ä¸‹è¼‰å…§éƒ¨äººæƒ…ç·’æ•¸æ“š
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç¢¼åˆ—è¡¨
            force_refresh: æ˜¯å¦å¼·åˆ¶åˆ·æ–°
        """
        logger.info(f"ğŸ’­ é–‹å§‹ä¸‹è¼‰å…§éƒ¨äººæƒ…ç·’æ•¸æ“šï¼Œè‚¡ç¥¨: {symbols}")
        
        # å‰µå»ºç›®éŒ„
        sentiment_dir = Path(self.data_dir) / "finnhub_data" / "insider_senti"
        sentiment_dir.mkdir(parents=True, exist_ok=True)
        
        for symbol in symbols:
            logger.info(f"ğŸ’­ ä¸‹è¼‰ {symbol} çš„å…§éƒ¨äººæƒ…ç·’æ•¸æ“š...")
            
            # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            file_path = sentiment_dir / f"{symbol}_data_formatted.json"
            if file_path.exists() and not force_refresh:
                logger.info(f"ğŸ“„ {symbol} æƒ…ç·’æ•¸æ“šæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰")
                continue
            
            # ä¸‹è¼‰æƒ…ç·’æ•¸æ“š
            params = {'symbol': symbol}
            sentiment_data = self._make_request('stock/insider-sentiment', params)
            
            if sentiment_data and 'data' in sentiment_data:
                # ä¿å­˜æ•¸æ“š
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(sentiment_data, f, ensure_ascii=False, indent=2)
                
                logger.info(f"âœ… {symbol} å…§éƒ¨äººæƒ…ç·’æ•¸æ“šå·²ä¿å­˜")
            else:
                logger.warning(f"âš ï¸ {symbol} å…§éƒ¨äººæƒ…ç·’æ•¸æ“šä¸‹è¼‰å¤±æ•—")
            
            # é¿å…APIé™åˆ¶
            time.sleep(1)
    
    def download_insider_transactions(self, symbols: List[str], force_refresh: bool = False):
        """
        ä¸‹è¼‰å…§éƒ¨äººäº¤æ˜“æ•¸æ“š
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç¢¼åˆ—è¡¨
            force_refresh: æ˜¯å¦å¼·åˆ¶åˆ·æ–°
        """
        logger.info(f"ğŸ’° é–‹å§‹ä¸‹è¼‰å…§éƒ¨äººäº¤æ˜“æ•¸æ“šï¼Œè‚¡ç¥¨: {symbols}")
        
        # å‰µå»ºç›®éŒ„
        trans_dir = Path(self.data_dir) / "finnhub_data" / "insider_trans"
        trans_dir.mkdir(parents=True, exist_ok=True)
        
        for symbol in symbols:
            logger.info(f"ğŸ’° ä¸‹è¼‰ {symbol} çš„å…§éƒ¨äººäº¤æ˜“æ•¸æ“š...")
            
            # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            file_path = trans_dir / f"{symbol}_data_formatted.json"
            if file_path.exists() and not force_refresh:
                logger.info(f"ğŸ“„ {symbol} äº¤æ˜“æ•¸æ“šæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰")
                continue
            
            # ä¸‹è¼‰äº¤æ˜“æ•¸æ“š
            params = {'symbol': symbol}
            trans_data = self._make_request('stock/insider-transactions', params)
            
            if trans_data and 'data' in trans_data:
                # ä¿å­˜æ•¸æ“š
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(trans_data, f, ensure_ascii=False, indent=2)
                
                logger.info(f"âœ… {symbol} å…§éƒ¨äººäº¤æ˜“æ•¸æ“šå·²ä¿å­˜")
            else:
                logger.warning(f"âš ï¸ {symbol} å…§éƒ¨äººäº¤æ˜“æ•¸æ“šä¸‹è¼‰å¤±æ•—")
            
            # é¿å…APIé™åˆ¶
            time.sleep(1)

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='Finnhubæ•¸æ“šä¸‹è¼‰è…³æœ¬')
    
    parser.add_argument('--data-type', 
                       choices=['news', 'sentiment', 'transactions', 'all'],
                       default='all',
                       help='è¦ä¸‹è¼‰çš„æ•¸æ“šé¡å‹')
    
    parser.add_argument('--symbols',
                       type=str,
                       default='AAPL,TSLA,MSFT,GOOGL,AMZN',
                       help='è‚¡ç¥¨ä»£ç¢¼ï¼Œç”¨é€—è™Ÿåˆ†éš”')
    
    parser.add_argument('--days',
                       type=int,
                       default=7,
                       help='ä¸‹è¼‰å¤šå°‘å¤©çš„æ–°èæ•¸æ“š')
    
    parser.add_argument('--force-refresh',
                       action='store_true',
                       help='å¼·åˆ¶åˆ·æ–°å·²å­˜åœ¨çš„æ•¸æ“š')
    
    parser.add_argument('--all',
                       action='store_true',
                       help='ä¸‹è¼‰æ‰€æœ‰é¡å‹çš„æ•¸æ“š')
    
    parser.add_argument('--api-key',
                       type=str,
                       help='Finnhub APIå¯†é‘°')
    
    parser.add_argument('--data-dir',
                       type=str,
                       help='æ•¸æ“šå­˜å„²ç›®éŒ„')
    
    args = parser.parse_args()
    
    # è§£æè‚¡ç¥¨ä»£ç¢¼
    symbols = [s.strip().upper() for s in args.symbols.split(',')]
    
    try:
        # å‰µå»ºä¸‹è¼‰å™¨
        downloader = FinnhubDataDownloader(
            api_key=args.api_key,
            data_dir=args.data_dir
        )
        
        # ç¢ºå®šè¦ä¸‹è¼‰çš„æ•¸æ“šé¡å‹
        if args.all:
            data_types = ['news', 'sentiment', 'transactions']
        else:
            data_types = [args.data_type] if args.data_type != 'all' else ['news', 'sentiment', 'transactions']
        
        logger.info(f"ğŸš€ é–‹å§‹ä¸‹è¼‰Finnhubæ•¸æ“š")
        logger.info(f"ğŸ“Š è‚¡ç¥¨ä»£ç¢¼: {symbols}")
        logger.info(f"ğŸ“‹ æ•¸æ“šé¡å‹: {data_types}")
        logger.info(f"ğŸ”„ å¼·åˆ¶åˆ·æ–°: {args.force_refresh}")
        
        # ä¸‹è¼‰æ•¸æ“š
        for data_type in data_types:
            if data_type == 'news':
                downloader.download_news_data(symbols, args.days, args.force_refresh)
            elif data_type == 'sentiment':
                downloader.download_insider_sentiment(symbols, args.force_refresh)
            elif data_type == 'transactions':
                downloader.download_insider_transactions(symbols, args.force_refresh)
        
        logger.info("ğŸ‰ æ•¸æ“šä¸‹è¼‰å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ ä¸‹è¼‰å¤±æ•—: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
