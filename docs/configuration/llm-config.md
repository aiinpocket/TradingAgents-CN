# Â§ßË™ûË®ÄÊ®°ÂûãÈÖçÁΩÆ (v0.1.7)

## Ê¶ÇËø∞

TradingAgents-CN Ê°ÜÊû∂ÊîØÊåÅÂ§öÁ®ÆÂ§ßË™ûË®ÄÊ®°ÂûãÊèê‰æõÂïÜÔºåÂåÖÊã¨ DeepSeek„ÄÅÈòøÈáåÁôæÁÇº„ÄÅGoogle AI„ÄÅOpenAI Âíå Anthropic„ÄÇÊú¨ÊñáÊ°£Ë©≥Á¥∞‰ªãÁ¥π‰∫ÜÂ¶Ç‰ΩïÈÖçÁΩÆÂíåÂÑ™Âåñ‰∏çÂêåÁöÑ LLM ‰ª•Áç≤ÂæóÊúÄ‰Ω≥ÊÄßËÉΩÂíåÊàêÊú¨ÊïàÁõä„ÄÇ

## üéØ v0.1.7 LLMÊîØÊåÅÊõ¥Êñ∞

- ‚úÖ **DeepSeek V3**: Êñ∞Â¢ûÊàêÊú¨ÂÑ™ÂåñÁöÑ‰∏≠ÊñáÊ®°Âûã
- ‚úÖ **Êô∫ËÉΩË∑ØÁî±**: Ê†πÊìö‰ªªÂãôËá™ÂãïÈÅ∏ÊìáÊúÄÂÑ™Ê®°Âûã
- ‚úÖ **ÊàêÊú¨ÊéßÂà∂**: Ë©≥Á¥∞ÁöÑÊàêÊú¨Áõ£ÊéßÂíåÈôêÂà∂
- ‚úÖ **Â∑•ÂÖ∑Ë™øÁî®**: ÂÆåÊï¥ÁöÑFunction CallingÊîØÊåÅ

## ÊîØÊåÅÁöÑ LLM Êèê‰æõÂïÜ

### 1. üá®üá≥ DeepSeek (v0.1.7Êñ∞Â¢ûÔºåÊé®Ëñ¶)

#### ÊîØÊåÅÁöÑÊ®°Âûã
```python
deepseek_models = {
    "deepseek-chat": {
        "description": "DeepSeek V3 Â∞çË©±Ê®°Âûã",
        "context_length": 64000,
        "cost_per_1k_tokens": {"input": 0.0014, "output": 0.0028},
        "recommended_for": ["‰∏≠ÊñáÂàÜÊûê", "Â∑•ÂÖ∑Ë™øÁî®", "ÊàêÊú¨ÊïèÊÑüÂ†¥ÊôØ"],
        "features": ["Â∑•ÂÖ∑Ë™øÁî®", "‰∏≠ÊñáÂÑ™Âåñ", "Êï∏Â≠∏Ë®àÁÆó"]
    },
    "deepseek-coder": {
        "description": "DeepSeek ‰ª£Á¢ºÁîüÊàêÊ®°Âûã",
        "context_length": 64000,
        "cost_per_1k_tokens": {"input": 0.0014, "output": 0.0028},
        "recommended_for": ["‰ª£Á¢ºÂàÜÊûê", "ÊäÄË°ìÊåáÊ®ôË®àÁÆó", "Êï∏ÊìöËôïÁêÜ"],
        "features": ["‰ª£Á¢ºÁîüÊàê", "ÈÄªËæëÊé®ÁêÜ", "Êï∏ÊìöÂàÜÊûê"]
    }
}
```

#### ÈÖçÁΩÆÁ§∫‰æã
```bash
# .env ÈÖçÁΩÆ
DEEPSEEK_API_KEY=sk-your_deepseek_api_key_here
DEEPSEEK_ENABLED=true
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_BASE_URL=https://api.deepseek.com
```

#### ÁâπËâ≤ÂäüËÉΩ
- **üîß Â∑•ÂÖ∑Ë™øÁî®**: Âº∫Â§ßÁöÑFunction CallingËÉΩÂäõ
- **üí∞ ÊàêÊú¨ÂÑ™Âåñ**: ÊØîGPT-4‰æøÂÆú90%‰ª•‰∏ä
- **üá®üá≥ ‰∏≠ÊñáÂÑ™Âåñ**: Â∞àÁÇ∫‰∏≠ÊñáÂ†¥ÊôØË®≠Ë®à
- **üìä Êï∏ÊìöÂàÜÊûê**: ÂÑ™ÁßÄÁöÑÊï∏Â≠∏ÂíåÈÄªËæëÊé®ÁêÜËÉΩÂäõ

### 2. üá®üá≥ ÈòøÈáåÁôæÁÇº (Êé®Ëñ¶)

#### ÊîØÊåÅÁöÑÊ®°Âûã
```python
qwen_models = {
    "qwen-plus": {
        "description": "ÈÄöÁæ©ÂçÉÂïèPlusÊ®°Âûã",
        "context_length": 32000,
        "cost_per_1k_tokens": {"input": 0.004, "output": 0.012},
        "recommended_for": ["‰∏≠ÊñáÁêÜËß£", "Âø´ÈÄüÈüøÊáâ", "Êó•Â∏∏ÂàÜÊûê"],
        "features": ["‰∏≠ÊñáÂÑ™Âåñ", "ÈüøÊáâÂø´ÈÄü", "ÁêÜËß£Ê∫ñÁ¢∫"]
    },
    "qwen-max": {
        "description": "ÈÄöÁæ©ÂçÉÂïèMaxÊ®°Âûã",
        "context_length": 8000,
        "cost_per_1k_tokens": {"input": 0.02, "output": 0.06},
        "recommended_for": ["Ë§áÈõúÊé®ÁêÜ", "Ê∑±Â∫¶ÂàÜÊûê", "È´òË¥®ÈáèËº∏Âá∫"],
        "features": ["Êé®ÁêÜËÉΩÂäõÂº∫", "Ëº∏Âá∫Ë¥®ÈáèÈ´ò", "ÈÄªËæëÊ∏ÖÊô∞"]
    }
}
```

### 3. üåç Google AI (Êé®Ëñ¶)

#### ÊîØÊåÅÁöÑÊ®°Âûã
```python
gemini_models = {
    "gemini-1.5-pro": {
        "description": "Gemini 1.5 ProÊ®°Âûã",
        "context_length": 1000000,
        "cost_per_1k_tokens": {"input": 0.0035, "output": 0.0105},
        "recommended_for": ["Ë§áÈõúÊé®ÁêÜ", "Èï∑ÊñáÊú¨ËôïÁêÜ", "Â§öÊ®°ÊÖãÂàÜÊûê"],
        "features": ["Ë∂ÖÈï∑‰∏ä‰∏ãÊñá", "Êé®ÁêÜËÉΩÂäõÂº∫", "Â§öÊ®°ÊÖãÊîØÊåÅ"]
    },
    "gemini-1.5-flash": {
        "description": "Gemini 1.5 FlashÊ®°Âûã",
        "context_length": 1000000,
        "cost_per_1k_tokens": {"input": 0.00035, "output": 0.00105},
        "recommended_for": ["Âø´ÈÄü‰ªªÂãô", "ÊâπÈáèËôïÁêÜ", "ÊàêÊú¨ÊïèÊÑü"],
        "features": ["ÈüøÊáâÂø´ÈÄü", "ÊàêÊú¨‰Ωé", "ÊÄßËÉΩÂùáË°°"]
    }
}
```

### 4. OpenAI

#### ÊîØÊåÅÁöÑÊ®°Âûã
```python
openai_models = {
    "gpt-4o": {
        "description": "ÊúÄÊñ∞ÁöÑ GPT-4 ÂÑ™ÂåñÁâàÊú¨",
        "context_length": 128000,
        "cost_per_1k_tokens": {"input": 0.005, "output": 0.015},
        "recommended_for": ["Ê∑±Â∫¶ÂàÜÊûê", "Ë§áÈõúÊé®ÁêÜ", "È´òË¥®ÈáèËº∏Âá∫"]
    },
    "gpt-4o-mini": {
        "description": "ËΩªÈáèÁ¥ö GPT-4 ÁâàÊú¨",
        "context_length": 128000,
        "cost_per_1k_tokens": {"input": 0.00015, "output": 0.0006},
        "recommended_for": ["Âø´ÈÄü‰ªªÂãô", "ÊàêÊú¨ÊïèÊÑüÂ†¥ÊôØ", "Â§ßÈáèAPIË™øÁî®"]
    },
    "gpt-4-turbo": {
        "description": "GPT-4 Turbo ÁâàÊú¨",
        "context_length": 128000,
        "cost_per_1k_tokens": {"input": 0.01, "output": 0.03},
        "recommended_for": ["Âπ≥Ë°°ÊÄßËÉΩÂíåÊàêÊú¨", "Ê®ôÊ∫ñÂàÜÊûê‰ªªÂãô"]
    },
    "gpt-3.5-turbo": {
        "description": "Á∂ìÊøüÂØ¶Áî®ÁöÑÈÅ∏Êìá",
        "context_length": 16385,
        "cost_per_1k_tokens": {"input": 0.0005, "output": 0.0015},
        "recommended_for": ["Á∞°ÂñÆ‰ªªÂãô", "È†êÁÆóÊúâÈôê", "Âø´ÈÄüÈüøÊáâ"]
    }
}
```

#### ÈÖçÁΩÆÁ§∫‰æã
```python
# OpenAI ÈÖçÁΩÆ
openai_config = {
    "llm_provider": "openai",
    "backend_url": "https://api.openai.com/v1",
    "deep_think_llm": "gpt-4o",           # Áî®ÊñºË§áÈõúÂàÜÊûê
    "quick_think_llm": "gpt-4o-mini",     # Áî®ÊñºÁ∞°ÂñÆ‰ªªÂãô
    "api_key": os.getenv("OPENAI_API_KEY"),
    
    # Ê®°ÂûãÂèÉÊï∏
    "model_params": {
        "temperature": 0.1,               # ‰ΩéÊ∫´Â∫¶‰øùË≠â‰∏ÄËá¥ÊÄß
        "max_tokens": 2000,               # ÊúÄÂ§ßËº∏Âá∫Èï∑Â∫¶
        "top_p": 0.9,                     # Ê†∏Êé°Ê®£ÂèÉÊï∏
        "frequency_penalty": 0.0,         # È†ªÁéáÊÉ©ÁΩö
        "presence_penalty": 0.0,          # Â≠òÂú®ÊÉ©ÁΩö
    },
    
    # ÈÄüÁéáÈôêÂà∂
    "rate_limits": {
        "requests_per_minute": 3500,      # ÊØèÂàÜÈêòË´ãÊ±ÇÊï∏
        "tokens_per_minute": 90000,       # ÊØèÂàÜÈêòtokenÊï∏
    },
    
    # ÈáçË©¶ÈÖçÁΩÆ
    "retry_config": {
        "max_retries": 3,
        "backoff_factor": 2,
        "timeout": 60
    }
}
```

### 2. Anthropic Claude

#### ÊîØÊåÅÁöÑÊ®°Âûã
```python
anthropic_models = {
    "claude-3-opus-20240229": {
        "description": "ÊúÄÂº∫Â§ßÁöÑ Claude Ê®°Âûã",
        "context_length": 200000,
        "cost_per_1k_tokens": {"input": 0.015, "output": 0.075},
        "recommended_for": ["ÊúÄË§áÈõúÁöÑÂàÜÊûê", "È´òË¥®ÈáèÊé®ÁêÜ", "ÂâµÊÑè‰ªªÂãô"]
    },
    "claude-3-sonnet-20240229": {
        "description": "Âπ≥Ë°°ÊÄßËÉΩÂíåÊàêÊú¨",
        "context_length": 200000,
        "cost_per_1k_tokens": {"input": 0.003, "output": 0.015},
        "recommended_for": ["Ê®ôÊ∫ñÂàÜÊûê‰ªªÂãô", "Âπ≥Ë°°‰ΩøÁî®Â†¥ÊôØ"]
    },
    "claude-3-haiku-20240307": {
        "description": "Âø´ÈÄü‰∏îÁ∂ìÊøüÁöÑÈÅ∏Êìá",
        "context_length": 200000,
        "cost_per_1k_tokens": {"input": 0.00025, "output": 0.00125},
        "recommended_for": ["Âø´ÈÄü‰ªªÂãô", "Â§ßÈáèË™øÁî®", "ÊàêÊú¨ÂÑ™Âåñ"]
    }
}
```

#### ÈÖçÁΩÆÁ§∫‰æã
```python
# Anthropic ÈÖçÁΩÆ
anthropic_config = {
    "llm_provider": "anthropic",
    "backend_url": "https://api.anthropic.com",
    "deep_think_llm": "claude-3-opus-20240229",
    "quick_think_llm": "claude-3-haiku-20240307",
    "api_key": os.getenv("ANTHROPIC_API_KEY"),
    
    # Ê®°ÂûãÂèÉÊï∏
    "model_params": {
        "temperature": 0.1,
        "max_tokens": 2000,
        "top_p": 0.9,
        "top_k": 40,
    },
    
    # ÈÄüÁéáÈôêÂà∂
    "rate_limits": {
        "requests_per_minute": 1000,
        "tokens_per_minute": 40000,
    }
}
```

### 3. Google AI (Gemini)

#### ÊîØÊåÅÁöÑÊ®°Âûã
```python
google_models = {
    "gemini-pro": {
        "description": "Google ÁöÑ‰∏ªÂäõÊ®°Âûã",
        "context_length": 32768,
        "cost_per_1k_tokens": {"input": 0.0005, "output": 0.0015},
        "recommended_for": ["Â§öÊ®°ÊÖã‰ªªÂãô", "‰ª£Á¢ºÂàÜÊûê", "Êé®ÁêÜ‰ªªÂãô"]
    },
    "gemini-pro-vision": {
        "description": "ÊîØÊåÅÂúñÂÉèÁöÑ Gemini ÁâàÊú¨",
        "context_length": 16384,
        "cost_per_1k_tokens": {"input": 0.0005, "output": 0.0015},
        "recommended_for": ["ÂúñË°®ÂàÜÊûê", "Â§öÊ®°ÊÖãËº∏ÂÖ•"]
    },
    "gemini-2.0-flash": {
        "description": "ÊúÄÊñ∞ÁöÑÂø´ÈÄüÁâàÊú¨",
        "context_length": 32768,
        "cost_per_1k_tokens": {"input": 0.0002, "output": 0.0008},
        "recommended_for": ["Âø´ÈÄüÈüøÊáâ", "ÂØ¶ÊôÇÂàÜÊûê"]
    }
}
```

#### ÈÖçÁΩÆÁ§∫‰æã
```python
# Google AI ÈÖçÁΩÆ
google_config = {
    "llm_provider": "google",
    "backend_url": "https://generativelanguage.googleapis.com/v1",
    "deep_think_llm": "gemini-pro",
    "quick_think_llm": "gemini-2.0-flash",
    "api_key": os.getenv("GOOGLE_API_KEY"),
    
    # Ê®°ÂûãÂèÉÊï∏
    "model_params": {
        "temperature": 0.1,
        "max_output_tokens": 2000,
        "top_p": 0.9,
        "top_k": 40,
    }
}
```

## LLM ÈÅ∏ÊìáÁ≠ñÁï•

### Âü∫Êñº‰ªªÂãôÈ°ûÂûãÁöÑÈÅ∏Êìá
```python
class LLMSelector:
    """LLM ÈÅ∏ÊìáÂô® - Ê†πÊìö‰ªªÂãôÈÅ∏ÊìáÊúÄÈÅ©ÂêàÁöÑÊ®°Âûã"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.task_model_mapping = self._initialize_task_mapping()
        
    def select_model(self, task_type: str, complexity: str = "medium") -> str:
        """Ê†πÊìö‰ªªÂãôÈ°ûÂûãÂíåË§áÈõúÂ∫¶ÈÅ∏ÊìáÊ®°Âûã"""
        
        task_config = self.task_model_mapping.get(task_type, {})
        
        if complexity == "high":
            return task_config.get("high_complexity", self.config["deep_think_llm"])
        elif complexity == "low":
            return task_config.get("low_complexity", self.config["quick_think_llm"])
        else:
            return task_config.get("medium_complexity", self.config["deep_think_llm"])
    
    def _initialize_task_mapping(self) -> Dict:
        """ÂàùÂßãÂåñ‰ªªÂãô-Ê®°ÂûãÊò†Â∞Ñ"""
        return {
            "fundamental_analysis": {
                "high_complexity": "gpt-4o",
                "medium_complexity": "gpt-4o-mini",
                "low_complexity": "gpt-3.5-turbo"
            },
            "technical_analysis": {
                "high_complexity": "claude-3-opus-20240229",
                "medium_complexity": "claude-3-sonnet-20240229",
                "low_complexity": "claude-3-haiku-20240307"
            },
            "news_analysis": {
                "high_complexity": "gpt-4o",
                "medium_complexity": "gpt-4o-mini",
                "low_complexity": "gemini-pro"
            },
            "social_sentiment": {
                "high_complexity": "claude-3-sonnet-20240229",
                "medium_complexity": "gpt-4o-mini",
                "low_complexity": "gemini-2.0-flash"
            },
            "risk_assessment": {
                "high_complexity": "gpt-4o",
                "medium_complexity": "claude-3-sonnet-20240229",
                "low_complexity": "gpt-4o-mini"
            },
            "trading_decision": {
                "high_complexity": "gpt-4o",
                "medium_complexity": "gpt-4o",
                "low_complexity": "claude-3-sonnet-20240229"
            }
        }
```

### ÊàêÊú¨ÂÑ™ÂåñÁ≠ñÁï•
```python
class CostOptimizer:
    """ÊàêÊú¨ÂÑ™ÂåñÂô® - Âú®ÊÄßËÉΩÂíåÊàêÊú¨ÈñìÊâæÂà∞Âπ≥Ë°°"""
    
    def __init__(self, budget_config: Dict):
        self.daily_budget = budget_config.get("daily_budget", 100)  # ÁæéÂÖÉ
        self.cost_tracking = {}
        self.model_costs = self._load_model_costs()
        
    def get_cost_optimized_config(self, current_usage: Dict) -> Dict:
        """Áç≤ÂèñÊàêÊú¨ÂÑ™ÂåñÁöÑÈÖçÁΩÆ"""
        
        remaining_budget = self._calculate_remaining_budget(current_usage)
        
        if remaining_budget > 50:  # È†êÁÆóÂÖÖË∂≥
            return {
                "deep_think_llm": "gpt-4o",
                "quick_think_llm": "gpt-4o-mini",
                "max_debate_rounds": 3
            }
        elif remaining_budget > 20:  # È†êÁÆó‰∏≠Á≠â
            return {
                "deep_think_llm": "gpt-4o-mini",
                "quick_think_llm": "gpt-4o-mini",
                "max_debate_rounds": 2
            }
        else:  # È†êÁÆóÁ∑äÂº†
            return {
                "deep_think_llm": "gpt-3.5-turbo",
                "quick_think_llm": "gpt-3.5-turbo",
                "max_debate_rounds": 1
            }
    
    def estimate_request_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:
        """‰º∞ÁÆóË´ãÊ±ÇÊàêÊú¨"""
        
        model_cost = self.model_costs.get(model, {"input": 0.001, "output": 0.002})
        
        input_cost = (input_tokens / 1000) * model_cost["input"]
        output_cost = (output_tokens / 1000) * model_cost["output"]
        
        return input_cost + output_cost
```

## ÊÄßËÉΩÂÑ™Âåñ

### ÊèêÁ§∫Ë©ûÂÑ™Âåñ
```python
class PromptOptimizer:
    """ÊèêÁ§∫Ë©ûÂÑ™ÂåñÂô®"""
    
    def __init__(self):
        self.prompt_templates = self._load_prompt_templates()
        
    def optimize_prompt(self, task_type: str, model: str, context: Dict) -> str:
        """ÂÑ™ÂåñÊèêÁ§∫Ë©û"""
        
        base_prompt = self.prompt_templates[task_type]["base"]
        
        # Ê†πÊìöÊ®°ÂûãÁâπÈªûË™øÊï¥ÊèêÁ§∫Ë©û
        if "gpt" in model.lower():
            optimized_prompt = self._optimize_for_gpt(base_prompt, context)
        elif "claude" in model.lower():
            optimized_prompt = self._optimize_for_claude(base_prompt, context)
        elif "gemini" in model.lower():
            optimized_prompt = self._optimize_for_gemini(base_prompt, context)
        else:
            optimized_prompt = base_prompt
        
        return optimized_prompt
    
    def _optimize_for_gpt(self, prompt: str, context: Dict) -> str:
        """ÁÇ∫ GPT Ê®°ÂûãÂÑ™ÂåñÊèêÁ§∫Ë©û"""
        
        # GPT ÂñúÊ¨¢ÁµêÊßãÂåñÁöÑÊåá‰ª§
        structured_prompt = f"""
‰ªªÂãô: {context.get('task_description', '')}

Êåá‰ª§:
1. ‰ªîÁ¥∞ÂàÜÊûêÊèê‰æõÁöÑÊï∏Êìö
2. ÊáâÁî®Áõ∏ÈóúÁöÑÈáëËûçÂàÜÊûêÊñπÊ≥ï
3. Êèê‰æõÊ∏ÖÊô∞ÁöÑÁµêË´ñÂíåÂª∫ËÆÆ
4. ÂåÖÂê´ÁΩÆ‰ø°Â∫¶Ë©ï‰º∞

Êï∏Êìö:
{context.get('data', '')}

Ë´ãÊåâÁÖß‰ª•‰∏ãÊ†ºÂºèÂõûÁ≠î:
- ÂàÜÊûêÁµêÊûú: [‰Ω†ÁöÑÂàÜÊûê]
- ÁµêË´ñ: [‰∏ªË¶ÅÁµêË´ñ]
- Âª∫ËÆÆ: [ÂÖ∑È´îÂª∫ËÆÆ]
- ÁΩÆ‰ø°Â∫¶: [0-1‰πãÈñìÁöÑÊï∏ÂÄº]
"""
        return structured_prompt
    
    def _optimize_for_claude(self, prompt: str, context: Dict) -> str:
        """ÁÇ∫ Claude Ê®°ÂûãÂÑ™ÂåñÊèêÁ§∫Ë©û"""
        
        # Claude ÂñúÊ¨¢Â∞çË©±ÂºèÁöÑÊèêÁ§∫
        conversational_prompt = f"""
ÊàëÈúÄË¶Å‰Ω†‰ΩúÁÇ∫‰∏ÄÂÄãÂ∞àÊ•≠ÁöÑÈáëËûçÂàÜÊûêÂ∏´‰æÜÂπ´Âä©ÊàëÂàÜÊûê‰ª•‰∏ãÊï∏Êìö„ÄÇ

{context.get('data', '')}

Ë´ã‰Ω†:
1. Ê∑±ÂÖ•ÂàÜÊûêÈÄô‰∫õÊï∏ÊìöÁöÑÂê´Áæ©
2. ËØÜÂà•ÈóúÈîÆÁöÑË∂ãÂäøÂíåÊ®°Âºè
3. Ë©ï‰º∞ÊΩúÂú®ÁöÑÈ¢®Èö™ÂíåÊ©üÊúÉ
4. Áµ¶Âá∫‰Ω†ÁöÑÂ∞àÊ•≠Âª∫ËÆÆ

Ë´ãÁî®Â∞àÊ•≠‰ΩÜÊòìÊáÇÁöÑË™ûË®ÄÂõûÁ≠îÔºå‰∏¶Ëß£Èáã‰Ω†ÁöÑÊé®ÁêÜÈÅéÁ®ã„ÄÇ
"""
        return conversational_prompt
```

### ‰∏¶ÁôºÊéßÂà∂
```python
class LLMConcurrencyManager:
    """LLM ‰∏¶ÁôºÁÆ°ÁêÜÂô®"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.semaphores = self._initialize_semaphores()
        self.rate_limiters = self._initialize_rate_limiters()
        
    def _initialize_semaphores(self) -> Dict:
        """ÂàùÂßãÂåñ‰ø°ËôüÈáèÊéßÂà∂‰∏¶Áôº"""
        return {
            "openai": asyncio.Semaphore(10),      # OpenAI ÊúÄÂ§ö10ÂÄã‰∏¶Áôº
            "anthropic": asyncio.Semaphore(5),    # Anthropic ÊúÄÂ§ö5ÂÄã‰∏¶Áôº
            "google": asyncio.Semaphore(8)        # Google ÊúÄÂ§ö8ÂÄã‰∏¶Áôº
        }
    
    async def execute_with_concurrency_control(self, provider: str, llm_call: callable) -> Any:
        """Âú®‰∏¶ÁôºÊéßÂà∂‰∏ãÂü∑Ë°åLLMË™øÁî®"""
        
        semaphore = self.semaphores.get(provider)
        rate_limiter = self.rate_limiters.get(provider)
        
        async with semaphore:
            await rate_limiter.acquire()
            try:
                result = await llm_call()
                return result
            except Exception as e:
                # ËôïÁêÜÈÄüÁéáÈôêÂà∂ÈåØË™§
                if "rate_limit" in str(e).lower():
                    await asyncio.sleep(60)  # Á≠âÂæÖ1ÂàÜÈêò
                    return await llm_call()
                else:
                    raise e
```

## Áõ£ÊéßÂíåË™øË©¶

### LLM ÊÄßËÉΩÁõ£Êéß
```python
class LLMMonitor:
    """LLM ÊÄßËÉΩÁõ£Êéß"""
    
    def __init__(self):
        self.metrics = {
            "request_count": defaultdict(int),
            "response_times": defaultdict(list),
            "token_usage": defaultdict(dict),
            "error_rates": defaultdict(float),
            "costs": defaultdict(float)
        }
    
    def record_request(self, model: str, response_time: float, 
                      input_tokens: int, output_tokens: int, cost: float):
        """Ë®òÈåÑË´ãÊ±ÇÊåáÊ®ô"""
        
        self.metrics["request_count"][model] += 1
        self.metrics["response_times"][model].append(response_time)
        
        if model not in self.metrics["token_usage"]:
            self.metrics["token_usage"][model] = {"input": 0, "output": 0}
        
        self.metrics["token_usage"][model]["input"] += input_tokens
        self.metrics["token_usage"][model]["output"] += output_tokens
        self.metrics["costs"][model] += cost
    
    def get_performance_report(self) -> Dict:
        """Áç≤ÂèñÊÄßËÉΩÂ†±Âëä"""
        
        report = {}
        
        for model in self.metrics["request_count"]:
            response_times = self.metrics["response_times"][model]
            
            report[model] = {
                "total_requests": self.metrics["request_count"][model],
                "avg_response_time": sum(response_times) / len(response_times) if response_times else 0,
                "total_input_tokens": self.metrics["token_usage"][model].get("input", 0),
                "total_output_tokens": self.metrics["token_usage"][model].get("output", 0),
                "total_cost": self.metrics["costs"][model],
                "avg_cost_per_request": self.metrics["costs"][model] / self.metrics["request_count"][model] if self.metrics["request_count"][model] > 0 else 0
            }
        
        return report
```

## ÊúÄ‰Ω≥ÂØ¶Ë∏ê

### 1. Ê®°ÂûãÈÅ∏ÊìáÂª∫ËÆÆ
- **È´òÁ≤æÂ∫¶‰ªªÂãô**: ‰ΩøÁî® GPT-4o Êàñ Claude-3-Opus
- **Âπ≥Ë°°Â†¥ÊôØ**: ‰ΩøÁî® GPT-4o-mini Êàñ Claude-3-Sonnet  
- **ÊàêÊú¨ÊïèÊÑü**: ‰ΩøÁî® GPT-3.5-turbo Êàñ Claude-3-Haiku
- **Âø´ÈÄüÈüøÊáâ**: ‰ΩøÁî® Gemini-2.0-flash

### 2. ÊàêÊú¨ÊéßÂà∂Á≠ñÁï•
- Ë®≠ÁΩÆÊØèÊó•È†êÁÆóÈôêÂà∂
- ‰ΩøÁî®ËºÉÂ∞èÊ®°ÂûãËôïÁêÜÁ∞°ÂñÆ‰ªªÂãô
- ÂØ¶ÊñΩÊô∫ËÉΩÁ∑©Â≠òÂáèÂ∞ëÈáçË§áË™øÁî®
- Áõ£Êéßtoken‰ΩøÁî®Èáè

### 3. ÊÄßËÉΩÂÑ™ÂåñÊäÄÂ∑ß
- ÂÑ™ÂåñÊèêÁ§∫Ë©ûÈï∑Â∫¶ÂíåÁµêÊßã
- ‰ΩøÁî®ÈÅ©Áï∂ÁöÑÊ∫´Â∫¶ÂèÉÊï∏
- ÂØ¶ÊñΩ‰∏¶ÁôºÊéßÂà∂ÈÅøÂÖçÈÄüÁéáÈôêÂà∂
- ÂÆöÊúüÁõ£ÊéßÂíåË™øÊï¥ÈÖçÁΩÆ

ÈÄöÈÅéÂêàÁêÜÁöÑLLMÈÖçÁΩÆÂíåÂÑ™ÂåñÔºåÂèØ‰ª•Âú®‰øùË≠âÂàÜÊûêË¥®ÈáèÁöÑÂêåÊôÇÊéßÂà∂ÊàêÊú¨‰∏¶ÊèêÈ´òÁ≥ªÁµ±ÊÄßËÉΩ„ÄÇ
