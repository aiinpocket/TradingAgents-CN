# TradingAgents æ•¸æ“šæµæ¶æ§‹

## æ¦‚è¿°

TradingAgents æ¡ç”¨å¤šå±¤æ¬¡æ•¸æ“šæµæ¶æ§‹ï¼Œæ”¯æŒä¸­åœ‹Aè‚¡ã€æ¸¯è‚¡å’Œç¾è‚¡çš„å…¨é¢æ•¸æ“šç²å–å’Œè™•ç†ã€‚ç³»çµ±é€šéçµ±ä¸€çš„æ•¸æ“šæ¥å£ã€æ™ºèƒ½çš„æ•¸æ“šæºç®¡ç†å’Œé«˜æ•ˆçš„ç·©å­˜æ©Ÿåˆ¶ï¼Œç‚ºæ™ºèƒ½é«”æä¾›é«˜è´¨é‡çš„é‡‘èæ•¸æ“šæœå‹™ã€‚

## ğŸ—ï¸ æ•¸æ“šæµæ¶æ§‹è¨­è¨ˆ

### æ¶æ§‹å±¤æ¬¡åœ–

```mermaid
graph TB
    subgraph "å¤–éƒ¨æ•¸æ“šæºå±¤ (External Data Sources)"
        subgraph "ä¸­åœ‹å¸‚å ´æ•¸æ“š"
            TUSHARE[Tushareå°ˆæ¥­æ•¸æ“š]
            AKSHARE[AKShareé–‹æºæ•¸æ“š]
            BAOSTOCK[BaoStockæ­·å²æ•¸æ“š]
            TDX[TDXé€šé”ä¿¡æ•¸æ“š - å·²å¼ƒç”¨]
        end
        
        subgraph "åœ‹é™…å¸‚å ´æ•¸æ“š"
            YFINANCE[Yahoo Finance]
            FINNHUB[FinnHub]
            SIMFIN[SimFin]
        end
        
        subgraph "æ–°èæƒ…ç»ªæ•¸æ“š"
            REDDIT[Redditç¤¾äº¤åª’é«”]
            GOOGLENEWS[Googleæ–°è]
            CHINESE_SOCIAL[ä¸­åœ‹ç¤¾äº¤åª’é«”]
        end
    end
    
    subgraph "æ•¸æ“šç²å–å±¤ (Data Acquisition Layer)"
        DSM[æ•¸æ“šæºç®¡ç†å™¨]
        ADAPTERS[æ•¸æ“šé©é…å™¨]
        API_MGR[APIç®¡ç†å™¨]
    end
    
    subgraph "æ•¸æ“šè™•ç†å±¤ (Data Processing Layer)"
        CLEANER[æ•¸æ“šæ¸…æ´—]
        TRANSFORMER[æ•¸æ“šè½‰æ›]
        VALIDATOR[æ•¸æ“šé©—è­‰]
        QUALITY[è´¨é‡æ§åˆ¶]
    end
    
    subgraph "æ•¸æ“šå­˜å‚¨å±¤ (Data Storage Layer)"
        CACHE[ç·©å­˜ç³»çµ±]
        FILES[æ–‡ä»¶å­˜å‚¨]
        CONFIG[é…ç½®ç®¡ç†]
    end
    
    subgraph "æ•¸æ“šåˆ†ç™¼å±¤ (Data Distribution Layer)"
        INTERFACE[çµ±ä¸€æ•¸æ“šæ¥å£]
        ROUTER[æ•¸æ“šè·¯ç”±å™¨]
        FORMATTER[æ ¼å¼åŒ–å™¨]
    end
    
    subgraph "å·¥å…·é›†æˆå±¤ (Tool Integration Layer)"
        TOOLKIT[Toolkitå·¥å…·åŒ…]
        UNIFIED_TOOLS[çµ±ä¸€å·¥å…·æ¥å£]
        STOCK_UTILS[è‚¡ç¥¨å·¥å…·]
    end
    
    subgraph "æ™ºèƒ½é«”æ¶ˆè²»å±¤ (Agent Consumption Layer)"
        ANALYSTS[åˆ†æå¸«æ™ºèƒ½é«”]
        RESEARCHERS[ç ”ç©¶å“¡æ™ºèƒ½é«”]
        TRADER[äº¤æ˜“å“¡æ™ºèƒ½é«”]
        MANAGERS[ç®¡ç†å±¤æ™ºèƒ½é«”]
    end
    
    %% æ•¸æ“šæµå‘
    TUSHARE --> DSM
    AKSHARE --> DSM
    BAOSTOCK --> DSM
    TDX --> DSM
    YFINANCE --> ADAPTERS
    FINNHUB --> ADAPTERS
    SIMFIN --> ADAPTERS
    REDDIT --> API_MGR
    GOOGLENEWS --> API_MGR
    CHINESE_SOCIAL --> API_MGR
    
    DSM --> CLEANER
    ADAPTERS --> CLEANER
    API_MGR --> CLEANER
    
    CLEANER --> TRANSFORMER
    TRANSFORMER --> VALIDATOR
    VALIDATOR --> QUALITY
    
    QUALITY --> CACHE
    QUALITY --> FILES
    QUALITY --> CONFIG
    
    CACHE --> INTERFACE
    FILES --> INTERFACE
    CONFIG --> INTERFACE
    
    INTERFACE --> ROUTER
    ROUTER --> FORMATTER
    
    FORMATTER --> TOOLKIT
    TOOLKIT --> UNIFIED_TOOLS
    UNIFIED_TOOLS --> STOCK_UTILS
    
    STOCK_UTILS --> ANALYSTS
    STOCK_UTILS --> RESEARCHERS
    STOCK_UTILS --> TRADER
    STOCK_UTILS --> MANAGERS
    
    %% æ¨£å¼å®šç¾©
    classDef sourceLayer fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef acquisitionLayer fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef processingLayer fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef storageLayer fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef distributionLayer fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef toolLayer fill:#e0f2f1,stroke:#00695c,stroke-width:2px
    classDef agentLayer fill:#f1f8e9,stroke:#558b2f,stroke-width:2px
    
    class TUSHARE,AKSHARE,BAOSTOCK,TDX,YFINANCE,FINNHUB,SIMFIN,REDDIT,GOOGLENEWS,CHINESE_SOCIAL sourceLayer
    class DSM,ADAPTERS,API_MGR acquisitionLayer
    class CLEANER,TRANSFORMER,VALIDATOR,QUALITY processingLayer
    class CACHE,FILES,CONFIG storageLayer
    class INTERFACE,ROUTER,FORMATTER distributionLayer
    class TOOLKIT,UNIFIED_TOOLS,STOCK_UTILS toolLayer
    class ANALYSTS,RESEARCHERS,TRADER,MANAGERS agentLayer
```

## ğŸ“Š å„å±¤æ¬¡è©³ç´°èªªæ˜

### 1. å¤–éƒ¨æ•¸æ“šæºå±¤ (External Data Sources)

#### ä¸­åœ‹å¸‚å ´æ•¸æ“šæº

##### Tushare å°ˆæ¥­æ•¸æ“šæº (æ¨è–¦)
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/tushare_utils.py`

```python
import tushare as ts
from tradingagents.utils.logging_manager import get_logger

class TushareProvider:
    """Tushareæ•¸æ“šæä¾›å•†"""
    
    def __init__(self):
        self.token = os.getenv('TUSHARE_TOKEN')
        if self.token:
            ts.set_token(self.token)
            self.pro = ts.pro_api()
        else:
            raise ValueError("TUSHARE_TOKENç’°å¢ƒè®Šé‡æœªè¨­ç½®")
    
    def get_stock_data(self, ts_code: str, start_date: str, end_date: str):
        """ç²å–è‚¡ç¥¨æ­·å²æ•¸æ“š"""
        try:
            df = self.pro.daily(
                ts_code=ts_code,
                start_date=start_date.replace('-', ''),
                end_date=end_date.replace('-', '')
            )
            return df
        except Exception as e:
            logger.error(f"Tushareæ•¸æ“šç²å–å¤±è´¥: {e}")
            return None
    
    def get_stock_basic(self, ts_code: str):
        """ç²å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯"""
        try:
            df = self.pro.stock_basic(
                ts_code=ts_code,
                fields='ts_code,symbol,name,area,industry,market,list_date'
            )
            return df
        except Exception as e:
            logger.error(f"TushareåŸºæœ¬ä¿¡æ¯ç²å–å¤±è´¥: {e}")
            return None
```

##### AKShare é–‹æºæ•¸æ“šæº (å¤‡ç”¨)
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/akshare_utils.py`

```python
import akshare as ak
import pandas as pd
from typing import Optional, Dict, Any

def get_akshare_provider():
    """ç²å–AKShareæ•¸æ“šæä¾›å•†å¯¦ä¾‹"""
    return AKShareProvider()

class AKShareProvider:
    """AKShareæ•¸æ“šæä¾›å•†"""
    
    def __init__(self):
        self.logger = get_logger('agents')
    
    def get_stock_zh_a_hist(self, symbol: str, period: str = "daily", 
                           start_date: str = None, end_date: str = None):
        """ç²å–Aè‚¡æ­·å²æ•¸æ“š"""
        try:
            df = ak.stock_zh_a_hist(
                symbol=symbol,
                period=period,
                start_date=start_date,
                end_date=end_date,
                adjust="qfq"  # å‰è¤‡æ¬Š
            )
            return df
        except Exception as e:
            self.logger.error(f"AKShare Aè‚¡æ•¸æ“šç²å–å¤±è´¥: {e}")
            return None
    
    def get_hk_stock_data_akshare(self, symbol: str, period: str = "daily"):
        """ç²å–æ¸¯è‚¡æ•¸æ“š"""
        try:
            # æ¸¯è‚¡ä»£ç¢¼æ ¼å¼è½‰æ›
            if not symbol.startswith('0') and len(symbol) <= 5:
                symbol = symbol.zfill(5)
            
            df = ak.stock_hk_hist(
                symbol=symbol,
                period=period,
                adjust="qfq"
            )
            return df
        except Exception as e:
            self.logger.error(f"AKShareæ¸¯è‚¡æ•¸æ“šç²å–å¤±è´¥: {e}")
            return None
    
    def get_hk_stock_info_akshare(self, symbol: str):
        """ç²å–æ¸¯è‚¡åŸºæœ¬ä¿¡æ¯"""
        try:
            df = ak.stock_hk_spot_em()
            if not df.empty:
                # æŸ¥æ‰¾åŒ¹é…çš„è‚¡ç¥¨
                matched = df[df['ä»£ç¢¼'].str.contains(symbol, na=False)]
                return matched
            return None
        except Exception as e:
            self.logger.error(f"AKShareæ¸¯è‚¡ä¿¡æ¯ç²å–å¤±è´¥: {e}")
            return None
```

##### BaoStock æ­·å²æ•¸æ“šæº (å¤‡ç”¨)
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/baostock_utils.py`

```python
import baostock as bs
import pandas as pd

class BaoStockProvider:
    """BaoStockæ•¸æ“šæä¾›å•†"""
    
    def __init__(self):
        self.logger = get_logger('agents')
        self.login_result = bs.login()
        if self.login_result.error_code != '0':
            self.logger.error(f"BaoStockç™»éŒ„å¤±è´¥: {self.login_result.error_msg}")
    
    def get_stock_data(self, code: str, start_date: str, end_date: str):
        """ç²å–è‚¡ç¥¨æ­·å²æ•¸æ“š"""
        try:
            rs = bs.query_history_k_data_plus(
                code,
                "date,code,open,high,low,close,preclose,volume,amount,adjustflag,turn,tradestatus,pctChg,isST",
                start_date=start_date,
                end_date=end_date,
                frequency="d",
                adjustflag="3"  # å‰è¤‡æ¬Š
            )
            
            data_list = []
            while (rs.error_code == '0') & rs.next():
                data_list.append(rs.get_row_data())
            
            df = pd.DataFrame(data_list, columns=rs.fields)
            return df
        except Exception as e:
            self.logger.error(f"BaoStockæ•¸æ“šç²å–å¤±è´¥: {e}")
            return None
    
    def __del__(self):
        """ææ§‹å‡½æ•¸ï¼Œç™»å‡ºBaoStock"""
        bs.logout()
```

#### åœ‹é™…å¸‚å ´æ•¸æ“šæº

##### Yahoo Finance
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/yfin_utils.py`

```python
import yfinance as yf
import pandas as pd
from typing import Optional

def get_yahoo_finance_data(ticker: str, period: str = "1y", 
                          start_date: str = None, end_date: str = None):
    """ç²å–Yahoo Financeæ•¸æ“š
    
    Args:
        ticker: è‚¡ç¥¨ä»£ç¢¼
        period: æ™‚é–“å‘¨æœŸ (1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max)
        start_date: é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: çµæŸæ—¥æœŸ (YYYY-MM-DD)
    
    Returns:
        DataFrame: è‚¡ç¥¨æ•¸æ“š
    """
    try:
        stock = yf.Ticker(ticker)
        
        if start_date and end_date:
            data = stock.history(start=start_date, end=end_date)
        else:
            data = stock.history(period=period)
        
        if data.empty:
            logger.warning(f"Yahoo Financeæœªæ‰¾åˆ°{ticker}çš„æ•¸æ“š")
            return None
        
        return data
    except Exception as e:
        logger.error(f"Yahoo Financeæ•¸æ“šç²å–å¤±è´¥: {e}")
        return None

def get_stock_info_yahoo(ticker: str):
    """ç²å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯"""
    try:
        stock = yf.Ticker(ticker)
        info = stock.info
        return info
    except Exception as e:
        logger.error(f"Yahoo Financeä¿¡æ¯ç²å–å¤±è´¥: {e}")
        return None
```

##### FinnHub æ–°èå’ŒåŸºæœ¬é¢æ•¸æ“š
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/finnhub_utils.py`

```python
from datetime import datetime, relativedelta
import json
import os

def get_data_in_range(ticker: str, start_date: str, end_date: str, 
                     data_type: str, data_dir: str):
    """å¾ç·©å­˜ä¸­ç²å–æŒ‡å®šæ™‚é–“ç¯„å›´çš„æ•¸æ“š
    
    Args:
        ticker: è‚¡ç¥¨ä»£ç¢¼
        start_date: é–‹å§‹æ—¥æœŸ
        end_date: çµæŸæ—¥æœŸ
        data_type: æ•¸æ“šé¡å‹ (news_data, insider_senti, insider_trans)
        data_dir: æ•¸æ“šç›®éŒ„
    
    Returns:
        dict: æ•¸æ“šå­—å…¸
    """
    try:
        file_path = os.path.join(data_dir, f"{ticker}_{data_type}.json")
        
        if not os.path.exists(file_path):
            logger.warning(f"æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return {}
        
        with open(file_path, 'r', encoding='utf-8') as f:
            all_data = json.load(f)
        
        # éæ¿¾æ™‚é–“ç¯„å›´å…§çš„æ•¸æ“š
        filtered_data = {}
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        
        for date_str, data in all_data.items():
            try:
                data_dt = datetime.strptime(date_str, "%Y-%m-%d")
                if start_dt <= data_dt <= end_dt:
                    filtered_data[date_str] = data
            except ValueError:
                continue
        
        return filtered_data
    except Exception as e:
        logger.error(f"æ•¸æ“šç²å–å¤±è´¥: {e}")
        return {}
```

#### æ–°èæƒ…ç»ªæ•¸æ“šæº

##### Reddit ç¤¾äº¤åª’é«”
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/reddit_utils.py`

```python
import praw
import os
from typing import List, Dict

def fetch_top_from_category(subreddit: str, category: str = "hot", 
                           limit: int = 10) -> List[Dict]:
    """å¾Redditç²å–ç†±é–€å¸–å­
    
    Args:
        subreddit: å­ç‰ˆå¡Šåç¨±
        category: åˆ†é¡ (hot, new, top)
        limit: ç²å–æ•¸é‡é™åˆ¶
    
    Returns:
        List[Dict]: å¸–å­åˆ—è¡¨
    """
    try:
        reddit = praw.Reddit(
            client_id=os.getenv('REDDIT_CLIENT_ID'),
            client_secret=os.getenv('REDDIT_CLIENT_SECRET'),
            user_agent='TradingAgents/1.0'
        )
        
        subreddit_obj = reddit.subreddit(subreddit)
        
        if category == "hot":
            posts = subreddit_obj.hot(limit=limit)
        elif category == "new":
            posts = subreddit_obj.new(limit=limit)
        elif category == "top":
            posts = subreddit_obj.top(limit=limit)
        else:
            posts = subreddit_obj.hot(limit=limit)
        
        results = []
        for post in posts:
            results.append({
                'title': post.title,
                'score': post.score,
                'url': post.url,
                'created_utc': post.created_utc,
                'num_comments': post.num_comments,
                'selftext': post.selftext[:500] if post.selftext else ''
            })
        
        return results
    except Exception as e:
        logger.error(f"Redditæ•¸æ“šç²å–å¤±è´¥: {e}")
        return []
```

##### ä¸­åœ‹ç¤¾äº¤åª’é«”æƒ…ç»ª
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/chinese_finance_utils.py`

```python
def get_chinese_social_sentiment(ticker: str, platform: str = "weibo"):
    """ç²å–ä¸­åœ‹ç¤¾äº¤åª’é«”æƒ…ç»ªæ•¸æ“š
    
    Args:
        ticker: è‚¡ç¥¨ä»£ç¢¼
        platform: å¹³å°åç¨± (weibo, xueqiu, eastmoney)
    
    Returns:
        str: æƒ…ç»ªåˆ†æå ±å‘Š
    """
    try:
        # é€™é‡Œå¯ä»¥é›†æˆå¾®åšã€é›ªçƒã€ä¸œæ–¹è²¡å¯Œç­‰å¹³å°çš„API
        # ç›®å‰è¿”å›æ¨¡æ“¬æ•¸æ“š
        sentiment_data = {
            'positive_ratio': 0.65,
            'negative_ratio': 0.25,
            'neutral_ratio': 0.10,
            'total_mentions': 1250,
            'trending_keywords': ['ä¸Šæ¶¨', 'åˆ©å¥½', 'æ¥­ç»©', 'å¢é•·']
        }
        
        report = f"""## {ticker} ä¸­åœ‹ç¤¾äº¤åª’é«”æƒ…ç»ªåˆ†æ
        
**å¹³å°**: {platform}
**æ€»æåŠæ•¸**: {sentiment_data['total_mentions']}
**æƒ…ç»ªåˆ†å¸ƒ**:
- ç©æ¥µ: {sentiment_data['positive_ratio']:.1%}
- æ¶ˆæ¥µ: {sentiment_data['negative_ratio']:.1%}
- ä¸­æ€§: {sentiment_data['neutral_ratio']:.1%}

**ç†±é–€é—œé”®è©**: {', '.join(sentiment_data['trending_keywords'])}
        """
        
        return report
    except Exception as e:
        logger.error(f"ä¸­åœ‹ç¤¾äº¤åª’é«”æƒ…ç»ªç²å–å¤±è´¥: {e}")
        return f"ä¸­åœ‹ç¤¾äº¤åª’é«”æƒ…ç»ªæ•¸æ“šç²å–å¤±è´¥: {str(e)}"
```

### 2. æ•¸æ“šç²å–å±¤ (Data Acquisition Layer)

#### æ•¸æ“šæºç®¡ç†å™¨
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/data_source_manager.py`

```python
from enum import Enum
from typing import List, Optional

class ChinaDataSource(Enum):
    """ä¸­åœ‹è‚¡ç¥¨æ•¸æ“šæºæšä¸¾"""
    TUSHARE = "tushare"
    AKSHARE = "akshare"
    BAOSTOCK = "baostock"
    TDX = "tdx"  # å·²å¼ƒç”¨

class DataSourceManager:
    """æ•¸æ“šæºç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ•¸æ“šæºç®¡ç†å™¨"""
        self.default_source = self._get_default_source()
        self.available_sources = self._check_available_sources()
        self.current_source = self.default_source
        
        logger.info(f"ğŸ“Š æ•¸æ“šæºç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   é»˜èªæ•¸æ“šæº: {self.default_source.value}")
        logger.info(f"   å¯ç”¨æ•¸æ“šæº: {[s.value for s in self.available_sources]}")
    
    def _get_default_source(self) -> ChinaDataSource:
        """ç²å–é»˜èªæ•¸æ“šæº"""
        default = os.getenv('DEFAULT_CHINA_DATA_SOURCE', 'tushare').lower()
        
        try:
            return ChinaDataSource(default)
        except ValueError:
            logger.warning(f"âš ï¸ ç„¡æ•ˆçš„é»˜èªæ•¸æ“šæº: {default}ï¼Œä½¿ç”¨Tushare")
            return ChinaDataSource.TUSHARE
    
    def _check_available_sources(self) -> List[ChinaDataSource]:
        """æª¢æŸ¥å¯ç”¨çš„æ•¸æ“šæº"""
        available = []
        
        # æª¢æŸ¥Tushare
        try:
            import tushare as ts
            token = os.getenv('TUSHARE_TOKEN')
            if token:
                available.append(ChinaDataSource.TUSHARE)
                logger.info("âœ… Tushareæ•¸æ“šæºå¯ç”¨")
            else:
                logger.warning("âš ï¸ Tushareæ•¸æ“šæºä¸å¯ç”¨: æœªè¨­ç½®TUSHARE_TOKEN")
        except ImportError:
            logger.warning("âš ï¸ Tushareæ•¸æ“šæºä¸å¯ç”¨: åº«æœªå®‰è£")
        
        # æª¢æŸ¥AKShare
        try:
            import akshare as ak
            available.append(ChinaDataSource.AKSHARE)
            logger.info("âœ… AKShareæ•¸æ“šæºå¯ç”¨")
        except ImportError:
            logger.warning("âš ï¸ AKShareæ•¸æ“šæºä¸å¯ç”¨: åº«æœªå®‰è£")
        
        # æª¢æŸ¥BaoStock
        try:
            import baostock as bs
            available.append(ChinaDataSource.BAOSTOCK)
            logger.info("âœ… BaoStockæ•¸æ“šæºå¯ç”¨")
        except ImportError:
            logger.warning("âš ï¸ BaoStockæ•¸æ“šæºä¸å¯ç”¨: åº«æœªå®‰è£")
        
        # æª¢æŸ¥TDX (å·²å¼ƒç”¨)
        try:
            import pytdx
            available.append(ChinaDataSource.TDX)
            logger.warning("âš ï¸ TDXæ•¸æ“šæºå¯ç”¨ä½†å·²å¼ƒç”¨ï¼Œå»ºè®®è¿ç§»åˆ°Tushare")
        except ImportError:
            logger.info("â„¹ï¸ TDXæ•¸æ“šæºä¸å¯ç”¨: åº«æœªå®‰è£")
        
        return available
    
    def switch_source(self, source_name: str) -> str:
        """åˆ‡æ›æ•¸æ“šæº
        
        Args:
            source_name: æ•¸æ“šæºåç¨±
        
        Returns:
            str: åˆ‡æ›çµæœæ¶ˆæ¯
        """
        try:
            new_source = ChinaDataSource(source_name.lower())
            
            if new_source in self.available_sources:
                self.current_source = new_source
                logger.info(f"âœ… æ•¸æ“šæºå·²åˆ‡æ›åˆ°: {new_source.value}")
                return f"âœ… æ•¸æ“šæºå·²æˆåŠŸåˆ‡æ›åˆ°: {new_source.value}"
            else:
                logger.warning(f"âš ï¸ æ•¸æ“šæº{new_source.value}ä¸å¯ç”¨")
                return f"âš ï¸ æ•¸æ“šæº{new_source.value}ä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥å®‰è£å’Œé…ç½®"
        except ValueError:
            logger.error(f"âŒ ç„¡æ•ˆçš„æ•¸æ“šæºåç¨±: {source_name}")
            return f"âŒ ç„¡æ•ˆçš„æ•¸æ“šæºåç¨±: {source_name}"
    
    def get_current_source(self) -> str:
        """ç²å–ç•¶å‰æ•¸æ“šæº"""
        return self.current_source.value
    
    def get_available_sources(self) -> List[str]:
        """ç²å–å¯ç”¨æ•¸æ“šæºåˆ—è¡¨"""
        return [s.value for s in self.available_sources]
```

### 3. æ•¸æ“šè™•ç†å±¤ (Data Processing Layer)

#### æ•¸æ“šé©—è­‰å’Œæ¸…æ´—
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/interface.py`

```python
def validate_and_clean_data(data, data_type: str):
    """æ•¸æ“šé©—è­‰å’Œæ¸…æ´—
    
    Args:
        data: åŸå§‹æ•¸æ“š
        data_type: æ•¸æ“šé¡å‹
    
    Returns:
        è™•ç†å¾Œçš„æ•¸æ“š
    """
    if data is None or (hasattr(data, 'empty') and data.empty):
        return None
    
    try:
        if data_type == "stock_data":
            # è‚¡ç¥¨æ•¸æ“šé©—è­‰
            required_columns = ['open', 'high', 'low', 'close', 'volume']
            if hasattr(data, 'columns'):
                missing_cols = [col for col in required_columns if col not in data.columns]
                if missing_cols:
                    logger.warning(f"âš ï¸ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
                
                # æ•¸æ“šæ¸…æ´—
                data = data.dropna()  # åˆªé™¤ç©ºå€¼
                data = data[data['volume'] > 0]  # åˆªé™¤ç„¡äº¤æ˜“é‡çš„æ•¸æ“š
        
        elif data_type == "news_data":
            # æ–°èæ•¸æ“šé©—è­‰
            if isinstance(data, str) and len(data.strip()) == 0:
                return None
        
        return data
    except Exception as e:
        logger.error(f"æ•¸æ“šé©—è­‰å¤±è´¥: {e}")
        return None
```

### 4. æ•¸æ“šå­˜å‚¨å±¤ (Data Storage Layer)

#### ç·©å­˜ç³»çµ±
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/config.py`

```python
import os
from typing import Dict, Any

# å…¨å±€é…ç½®
_config = None

def get_config() -> Dict[str, Any]:
    """ç²å–æ•¸æ“šæµé…ç½®"""
    global _config
    if _config is None:
        _config = {
            "data_dir": os.path.join(os.path.expanduser("~"), "Documents", "TradingAgents", "data"),
            "cache_dir": os.path.join(os.path.expanduser("~"), "Documents", "TradingAgents", "cache"),
            "cache_expiry": {
                "market_data": 300,      # 5åˆ†é˜
                "news_data": 3600,       # 1å°æ™‚
                "fundamentals": 86400,   # 24å°æ™‚
                "social_sentiment": 1800, # 30åˆ†é˜
            },
            "max_cache_size": 1000,  # æœ€å¤§ç·©å­˜æ¢ç›®æ•¸
            "enable_cache": True,
        }
    return _config

def set_config(config: Dict[str, Any]):
    """è¨­ç½®æ•¸æ“šæµé…ç½®"""
    global _config
    _config = config

# æ•¸æ“šç›®éŒ„
DATA_DIR = get_config()["data_dir"]
CACHE_DIR = get_config()["cache_dir"]

# ç¢ºä¿ç›®éŒ„å­˜åœ¨
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(CACHE_DIR, exist_ok=True)
```

### 5. æ•¸æ“šåˆ†ç™¼å±¤ (Data Distribution Layer)

#### çµ±ä¸€æ•¸æ“šæ¥å£
**æ–‡ä»¶ä½ç½®**: `tradingagents/dataflows/interface.py`

```python
# çµ±ä¸€æ•¸æ“šç²å–æ¥å£
def get_finnhub_news(
    ticker: Annotated[str, "å…¬å¸è‚¡ç¥¨ä»£ç¢¼ï¼Œå¦‚ 'AAPL', 'TSM' ç­‰"],
    curr_date: Annotated[str, "ç•¶å‰æ—¥æœŸï¼Œæ ¼å¼ç‚º yyyy-mm-dd"],
    look_back_days: Annotated[int, "å›çœ‹å¤©æ•¸"],
):
    """ç²å–æŒ‡å®šæ™‚é–“ç¯„å›´å…§çš„å…¬å¸æ–°è
    
    Args:
        ticker (str): ç›®æ¨™å…¬å¸çš„è‚¡ç¥¨ä»£ç¢¼
        curr_date (str): ç•¶å‰æ—¥æœŸï¼Œæ ¼å¼ç‚º yyyy-mm-dd
        look_back_days (int): å›çœ‹å¤©æ•¸
    
    Returns:
        str: åŒ…å«å…¬å¸æ–°èçš„æ•¸æ“šæ¡†
    """
    start_date = datetime.strptime(curr_date, "%Y-%m-%d")
    before = start_date - relativedelta(days=look_back_days)
    before = before.strftime("%Y-%m-%d")
    
    result = get_data_in_range(ticker, before, curr_date, "news_data", DATA_DIR)
    
    if len(result) == 0:
        error_msg = f"âš ï¸ ç„¡æ³•ç²å–{ticker}çš„æ–°èæ•¸æ“š ({before} åˆ° {curr_date})\n"
        error_msg += f"å¯èƒ½çš„åŸå› ï¼š\n"
        error_msg += f"1. æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨æˆ–è·¯å¾‘é…ç½®éŒ¯èª¤\n"
        error_msg += f"2. æŒ‡å®šæ—¥æœŸç¯„å›´å…§æ²¡æœ‰æ–°èæ•¸æ“š\n"
        error_msg += f"3. éœ€è¦å…ˆä¸‹è¼‰æˆ–æ›´æ–°Finnhubæ–°èæ•¸æ“š\n"
        error_msg += f"å»ºè®®ï¼šæª¢æŸ¥æ•¸æ“šç›®éŒ„é…ç½®æˆ–é‡æ–°ç²å–æ–°èæ•¸æ“š"
        logger.debug(f"ğŸ“° [DEBUG] {error_msg}")
        return error_msg
    
    combined_result = ""
    for day, data in result.items():
        if len(data) == 0:
            continue
        for entry in data:
            current_news = (
                "### " + entry["headline"] + f" ({day})" + "\n" + entry["summary"]
            )
            combined_result += current_news + "\n\n"
    
    return f"## {ticker} News, from {before} to {curr_date}:\n" + str(combined_result)

def get_finnhub_company_insider_sentiment(
    ticker: Annotated[str, "è‚¡ç¥¨ä»£ç¢¼"],
    curr_date: Annotated[str, "ç•¶å‰äº¤æ˜“æ—¥æœŸï¼Œyyyy-mm-ddæ ¼å¼"],
    look_back_days: Annotated[int, "å›çœ‹å¤©æ•¸"],
):
    """ç²å–å…¬å¸å…§éƒ¨äººå£«æƒ…ç»ªæ•¸æ“šï¼ˆä¾†è‡ªå…¬é–‹SECä¿¡æ¯ï¼‰
    
    Args:
        ticker (str): å…¬å¸è‚¡ç¥¨ä»£ç¢¼
        curr_date (str): ç•¶å‰äº¤æ˜“æ—¥æœŸï¼Œyyyy-mm-ddæ ¼å¼
        look_back_days (int): å›çœ‹å¤©æ•¸
    
    Returns:
        str: éå»æŒ‡å®šå¤©æ•¸çš„æƒ…ç»ªå ±å‘Š
    """
    date_obj = datetime.strptime(curr_date, "%Y-%m-%d")
    before = date_obj - relativedelta(days=look_back_days)
    before = before.strftime("%Y-%m-%d")
    
    data = get_data_in_range(ticker, before, curr_date, "insider_senti", DATA_DIR)
    
    if len(data) == 0:
        return ""
    
    result_str = ""
    seen_dicts = []
    for date, senti_list in data.items():
        for entry in senti_list:
            if entry not in seen_dicts:
                result_str += f"### {entry['year']}-{entry['month']}:\nChange: {entry['change']}\nMonthly Share Purchase Ratio: {entry['mspr']}\n\n"
                seen_dicts.append(entry)
    
    return (
        f"## {ticker} Insider Sentiment Data for {before} to {curr_date}:\n"
        + result_str
        + "The change field refers to the net buying/selling from all insiders' transactions. The mspr field refers to monthly share purchase ratio."
    )
```

### 6. å·¥å…·é›†æˆå±¤ (Tool Integration Layer)

#### Toolkit çµ±ä¸€å·¥å…·åŒ…
**æ–‡ä»¶ä½ç½®**: `tradingagents/agents/utils/agent_utils.py`

```python
class Toolkit:
    """çµ±ä¸€å·¥å…·åŒ…ï¼Œç‚ºæ‰€æœ‰æ™ºèƒ½é«”æä¾›æ•¸æ“šè¨ªå•æ¥å£"""
    
    def __init__(self, config):
        self.config = config
        self.logger = get_logger('agents')
    
    def get_stock_fundamentals_unified(self, ticker: str):
        """çµ±ä¸€åŸºæœ¬é¢åˆ†æå·¥å…·ï¼Œè‡ªå‹•è¯†åˆ¥è‚¡ç¥¨é¡å‹"""
        from tradingagents.utils.stock_utils import StockUtils
        
        try:
            market_info = StockUtils.get_market_info(ticker)
            
            if market_info['market_type'] == 'Aè‚¡':
                return self._get_china_stock_fundamentals(ticker)
            elif market_info['market_type'] == 'æ¸¯è‚¡':
                return self._get_hk_stock_fundamentals(ticker)
            else:
                return self._get_us_stock_fundamentals(ticker)
        except Exception as e:
            self.logger.error(f"åŸºæœ¬é¢æ•¸æ“šç²å–å¤±è´¥: {e}")
            return f"âŒ åŸºæœ¬é¢æ•¸æ“šç²å–å¤±è´¥: {str(e)}"
    
    def _get_china_stock_fundamentals(self, ticker: str):
        """ç²å–ä¸­åœ‹è‚¡ç¥¨åŸºæœ¬é¢æ•¸æ“š"""
        try:
            from tradingagents.dataflows.data_source_manager import DataSourceManager
            
            manager = DataSourceManager()
            current_source = manager.get_current_source()
            
            if current_source == 'tushare':
                return self._get_tushare_fundamentals(ticker)
            elif current_source == 'akshare':
                return self._get_akshare_fundamentals(ticker)
            else:
                # é™ç´šç­–ç•¥
                return self._get_akshare_fundamentals(ticker)
        except Exception as e:
            self.logger.error(f"ä¸­åœ‹è‚¡ç¥¨åŸºæœ¬é¢ç²å–å¤±è´¥: {e}")
            return f"âŒ ä¸­åœ‹è‚¡ç¥¨åŸºæœ¬é¢ç²å–å¤±è´¥: {str(e)}"
    
    def _get_tushare_fundamentals(self, ticker: str):
        """ä½¿ç”¨Tushareç²å–åŸºæœ¬é¢æ•¸æ“š"""
        try:
            from tradingagents.dataflows.tushare_utils import TushareProvider
            
            provider = TushareProvider()
            
            # ç²å–åŸºæœ¬ä¿¡æ¯
            basic_info = provider.get_stock_basic(ticker)
            
            # ç²å–è²¡å‹™æ•¸æ“š
            financial_data = provider.get_financial_data(ticker)
            
            # æ ¼å¼åŒ–è¼¸å‡º
            report = f"""## {ticker} åŸºæœ¬é¢åˆ†æå ±å‘Š (Tushareæ•¸æ“šæº)
            
**åŸºæœ¬ä¿¡æ¯**:
- è‚¡ç¥¨åç¨±: {basic_info.get('name', 'N/A')}
- æ‰€å±¬è¡Œæ¥­: {basic_info.get('industry', 'N/A')}
- ä¸Šå¸‚æ—¥æœŸ: {basic_info.get('list_date', 'N/A')}

**è²¡å‹™æŒ‡æ¨™**:
- æ€»å¸‚å€¼: {financial_data.get('total_mv', 'N/A')}
- å¸‚ç›ˆç‡: {financial_data.get('pe', 'N/A')}
- å¸‚å‡€ç‡: {financial_data.get('pb', 'N/A')}
- å‡€è³‡ç”¢æ”¶ç›Šç‡: {financial_data.get('roe', 'N/A')}
            """
            
            return report
        except Exception as e:
            self.logger.error(f"TushareåŸºæœ¬é¢ç²å–å¤±è´¥: {e}")
            return f"âŒ TushareåŸºæœ¬é¢ç²å–å¤±è´¥: {str(e)}"
```

#### è‚¡ç¥¨å·¥å…·
**æ–‡ä»¶ä½ç½®**: `tradingagents/utils/stock_utils.py`

```python
from enum import Enum
from typing import Dict, Any

class StockMarket(Enum):
    """è‚¡ç¥¨å¸‚å ´æšä¸¾"""
    CHINA_A = "china_a"      # ä¸­åœ‹Aè‚¡
    HONG_KONG = "hong_kong"  # æ¸¯è‚¡
    US = "us"                # ç¾è‚¡
    UNKNOWN = "unknown"      # æœªçŸ¥å¸‚å ´

class StockUtils:
    """è‚¡ç¥¨å·¥å…·é¡"""
    
    @staticmethod
    def identify_stock_market(ticker: str) -> StockMarket:
        """è¯†åˆ¥è‚¡ç¥¨æ‰€å±¬å¸‚å ´
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç¢¼
            
        Returns:
            StockMarket: è‚¡ç¥¨å¸‚å ´é¡å‹
        """
        ticker = ticker.upper().strip()
        
        # ä¸­åœ‹Aè‚¡åˆ¤æ–·
        if (ticker.isdigit() and len(ticker) == 6 and 
            (ticker.startswith('0') or ticker.startswith('3') or ticker.startswith('6'))):
            return StockMarket.CHINA_A
        
        # æ¸¯è‚¡åˆ¤æ–·
        if (ticker.isdigit() and len(ticker) <= 5) or ticker.endswith('.HK'):
            return StockMarket.HONG_KONG
        
        # ç¾è‚¡åˆ¤æ–·ï¼ˆå­—æ¯é–‹å¤´æˆ–åŒ…å«å­—æ¯ï¼‰
        if any(c.isalpha() for c in ticker) and not ticker.endswith('.HK'):
            return StockMarket.US
        
        return StockMarket.UNKNOWN
    
    @staticmethod
    def get_market_info(ticker: str) -> Dict[str, Any]:
        """ç²å–è‚¡ç¥¨å¸‚å ´ä¿¡æ¯
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç¢¼
            
        Returns:
            Dict: å¸‚å ´ä¿¡æ¯å­—å…¸
        """
        market = StockUtils.identify_stock_market(ticker)
        
        market_info = {
            StockMarket.CHINA_A: {
                'market_type': 'Aè‚¡',
                'market_name': 'ä¸­åœ‹Aè‚¡å¸‚å ´',
                'currency_name': 'äººæ°‘å¸',
                'currency_symbol': 'Â¥',
                'timezone': 'Asia/Shanghai',
                'trading_hours': '09:30-15:00'
            },
            StockMarket.HONG_KONG: {
                'market_type': 'æ¸¯è‚¡',
                'market_name': 'é¦™æ¸¯è‚¡ç¥¨å¸‚å ´',
                'currency_name': 'æ¸¯å¸',
                'currency_symbol': 'HK$',
                'timezone': 'Asia/Hong_Kong',
                'trading_hours': '09:30-16:00'
            },
            StockMarket.US: {
                'market_type': 'ç¾è‚¡',
                'market_name': 'ç¾åœ‹è‚¡ç¥¨å¸‚å ´',
                'currency_name': 'ç¾å…ƒ',
                'currency_symbol': '$',
                'timezone': 'America/New_York',
                'trading_hours': '09:30-16:00'
            },
            StockMarket.UNKNOWN: {
                'market_type': 'æœªçŸ¥',
                'market_name': 'æœªçŸ¥å¸‚å ´',
                'currency_name': 'æœªçŸ¥',
                'currency_symbol': '?',
                'timezone': 'UTC',
                'trading_hours': 'Unknown'
            }
        }
        
        return market_info.get(market, market_info[StockMarket.UNKNOWN])
    
    @staticmethod
    def get_data_source(ticker: str) -> str:
        """æ ¹æ“šè‚¡ç¥¨ä»£ç¢¼ç²å–æ¨è–¦çš„æ•¸æ“šæº
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç¢¼
            
        Returns:
            str: æ•¸æ“šæºåç¨±
        """
        market = StockUtils.identify_stock_market(ticker)
        
        if market == StockMarket.CHINA_A:
            return "china_unified"  # ä½¿ç”¨çµ±ä¸€çš„ä¸­åœ‹è‚¡ç¥¨æ•¸æ“šæº
        elif market == StockMarket.HONG_KONG:
            return "yahoo_finance"  # æ¸¯è‚¡ä½¿ç”¨Yahoo Finance
        elif market == StockMarket.US:
            return "yahoo_finance"  # ç¾è‚¡ä½¿ç”¨Yahoo Finance
        else:
            return "unknown"
```

## ğŸ”„ æ•¸æ“šæµè½‰éç¨‹

### å®Œæ•´æ•¸æ“šæµç¨‹åœ–

```mermaid
sequenceDiagram
    participant Agent as æ™ºèƒ½é«”
    participant Toolkit as å·¥å…·åŒ…
    participant Interface as æ•¸æ“šæ¥å£
    participant Manager as æ•¸æ“šæºç®¡ç†å™¨
    participant Cache as ç·©å­˜ç³»çµ±
    participant Source as æ•¸æ“šæº
    
    Agent->>Toolkit: è«‹æ±‚è‚¡ç¥¨æ•¸æ“š
    Toolkit->>Interface: èª¿ç”¨çµ±ä¸€æ¥å£
    Interface->>Cache: æª¢æŸ¥ç·©å­˜
    
    alt ç·©å­˜å‘½ä¸­
        Cache->>Interface: è¿”å›ç·©å­˜æ•¸æ“š
    else ç·©å­˜æœªå‘½ä¸­
        Interface->>Manager: ç²å–æ•¸æ“šæº
        Manager->>Source: èª¿ç”¨æ•¸æ“šæºAPI
        Source->>Manager: è¿”å›åŸå§‹æ•¸æ“š
        Manager->>Interface: è¿”å›è™•ç†å¾Œæ•¸æ“š
        Interface->>Cache: æ›´æ–°ç·©å­˜
    end
    
    Interface->>Toolkit: è¿”å›æ ¼å¼åŒ–æ•¸æ“š
    Toolkit->>Agent: è¿”å›åˆ†æå°±ç»ªæ•¸æ“š
```

### æ•¸æ“šè™•ç†æµæ°´ç·š

1. **æ•¸æ“šè«‹æ±‚**: æ™ºèƒ½é«”é€šéToolkitè«‹æ±‚æ•¸æ“š
2. **ç·©å­˜æª¢æŸ¥**: é¦–å…ˆæª¢æŸ¥æœ¬åœ°ç·©å­˜æ˜¯å¦æœ‰æ•ˆ
3. **æ•¸æ“šæºé¸æ“‡**: æ ¹æ“šè‚¡ç¥¨é¡å‹é¸æ“‡æœ€ä½³æ•¸æ“šæº
4. **æ•¸æ“šç²å–**: å¾å¤–éƒ¨APIç²å–åŸå§‹æ•¸æ“š
5. **æ•¸æ“šé©—è­‰**: é©—è­‰æ•¸æ“šå®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§
6. **æ•¸æ“šæ¸…æ´—**: æ¸…ç†ç•°å¸¸å€¼å’Œç¼ºå¤±æ•¸æ“š
7. **æ•¸æ“šæ¨™æº–åŒ–**: çµ±ä¸€æ•¸æ“šæ ¼å¼å’Œå­—æ®µå
8. **æ•¸æ“šç·©å­˜**: å°†è™•ç†å¾Œçš„æ•¸æ“šå­˜å…¥ç·©å­˜
9. **æ•¸æ“šè¿”å›**: è¿”å›æ ¼å¼åŒ–çš„åˆ†æå°±ç»ªæ•¸æ“š

## ğŸ“Š æ•¸æ“šè´¨é‡ç›£æ§

### æ•¸æ“šè´¨é‡æŒ‡æ¨™

```python
class DataQualityMonitor:
    """æ•¸æ“šè´¨é‡ç›£æ§å™¨"""
    
    def __init__(self):
        self.quality_metrics = {
            'completeness': 0.0,    # å®Œæ•´æ€§
            'accuracy': 0.0,        # æº–ç¢ºæ€§
            'timeliness': 0.0,      # åŠæ™‚æ€§
            'consistency': 0.0,     # ä¸€è‡´æ€§
        }
    
    def check_data_quality(self, data, data_type: str):
        """æª¢æŸ¥æ•¸æ“šè´¨é‡
        
        Args:
            data: å¾…æª¢æŸ¥çš„æ•¸æ“š
            data_type: æ•¸æ“šé¡å‹
        
        Returns:
            Dict: è´¨é‡è©•åˆ†
        """
        if data is None:
            return {'overall_score': 0.0, 'issues': ['æ•¸æ“šç‚ºç©º']}
        
        issues = []
        scores = {}
        
        # å®Œæ•´æ€§æª¢æŸ¥
        completeness = self._check_completeness(data, data_type)
        scores['completeness'] = completeness
        if completeness < 0.8:
            issues.append(f'æ•¸æ“šå®Œæ•´æ€§ä¸è¶³: {completeness:.1%}')
        
        # æº–ç¢ºæ€§æª¢æŸ¥
        accuracy = self._check_accuracy(data, data_type)
        scores['accuracy'] = accuracy
        if accuracy < 0.9:
            issues.append(f'æ•¸æ“šæº–ç¢ºæ€§ä¸è¶³: {accuracy:.1%}')
        
        # åŠæ™‚æ€§æª¢æŸ¥
        timeliness = self._check_timeliness(data, data_type)
        scores['timeliness'] = timeliness
        if timeliness < 0.7:
            issues.append(f'æ•¸æ“šåŠæ™‚æ€§ä¸è¶³: {timeliness:.1%}')
        
        # è¨ˆç®—æ€»åˆ†
        overall_score = sum(scores.values()) / len(scores)
        
        return {
            'overall_score': overall_score,
            'detailed_scores': scores,
            'issues': issues
        }
    
    def _check_completeness(self, data, data_type: str) -> float:
        """æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§"""
        if data_type == "stock_data":
            required_fields = ['open', 'high', 'low', 'close', 'volume']
            if hasattr(data, 'columns'):
                available_fields = len([f for f in required_fields if f in data.columns])
                return available_fields / len(required_fields)
        return 1.0
    
    def _check_accuracy(self, data, data_type: str) -> float:
        """æª¢æŸ¥æ•¸æ“šæº–ç¢ºæ€§"""
        if data_type == "stock_data" and hasattr(data, 'columns'):
            # æª¢æŸ¥åƒ¹æ ¼é€»è¾‘æ€§
            if all(col in data.columns for col in ['high', 'low', 'close']):
                valid_rows = (data['high'] >= data['low']).sum()
                total_rows = len(data)
                return valid_rows / total_rows if total_rows > 0 else 0.0
        return 1.0
    
    def _check_timeliness(self, data, data_type: str) -> float:
        """æª¢æŸ¥æ•¸æ“šåŠæ™‚æ€§"""
        # ç°¡åŒ–å¯¦ç¾ï¼Œå¯¦é™…æ‡‰æª¢æŸ¥æ•¸æ“šæ™‚é–“æˆ³
        return 1.0
```

## ğŸš€ æ€§èƒ½å„ªåŒ–

### ç·©å­˜ç­–ç•¥

```python
class CacheManager:
    """ç·©å­˜ç®¡ç†å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.cache_dir = config.get('cache_dir', './cache')
        self.cache_expiry = config.get('cache_expiry', {})
        self.max_cache_size = config.get('max_cache_size', 1000)
    
    def get_cache_key(self, ticker: str, data_type: str, params: dict = None) -> str:
        """ç”Ÿæˆç·©å­˜é”®"""
        import hashlib
        
        key_parts = [ticker, data_type]
        if params:
            key_parts.append(str(sorted(params.items())))
        
        key_string = '|'.join(key_parts)
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def is_cache_valid(self, cache_file: str, data_type: str) -> bool:
        """æª¢æŸ¥ç·©å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not os.path.exists(cache_file):
            return False
        
        # æª¢æŸ¥ç·©å­˜æ™‚é–“
        cache_time = os.path.getmtime(cache_file)
        current_time = time.time()
        expiry_seconds = self.cache_expiry.get(data_type, 3600)
        
        return (current_time - cache_time) < expiry_seconds
    
    def get_from_cache(self, cache_key: str, data_type: str):
        """å¾ç·©å­˜ç²å–æ•¸æ“š"""
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
        
        if self.is_cache_valid(cache_file, data_type):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"ç·©å­˜è®€å–å¤±è´¥: {e}")
        
        return None
    
    def save_to_cache(self, cache_key: str, data, data_type: str):
        """ä¿å­˜æ•¸æ“šåˆ°ç·©å­˜"""
        try:
            os.makedirs(self.cache_dir, exist_ok=True)
            cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
            
            # åºåˆ—åŒ–æ•¸æ“š
            if hasattr(data, 'to_dict'):
                serializable_data = data.to_dict()
            elif hasattr(data, 'to_json'):
                serializable_data = json.loads(data.to_json())
            else:
                serializable_data = data
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_data, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"æ•¸æ“šå·²ç·©å­˜: {cache_key}")
        except Exception as e:
            logger.warning(f"ç·©å­˜ä¿å­˜å¤±è´¥: {e}")
```

### ä¸¦è¡Œæ•¸æ“šç²å–

```python
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Callable

class ParallelDataFetcher:
    """ä¸¦è¡Œæ•¸æ“šç²å–å™¨"""
    
    def __init__(self, max_workers: int = 5):
        self.max_workers = max_workers
    
    def fetch_multiple_data(self, tasks: List[dict]) -> dict:
        """ä¸¦è¡Œç²å–å¤šå€‹æ•¸æ“šæºçš„æ•¸æ“š
        
        Args:
            tasks: ä»»å‹™åˆ—è¡¨ï¼Œæ¯å€‹ä»»å‹™åŒ…å« {'name': str, 'func': callable, 'args': tuple, 'kwargs': dict}
        
        Returns:
            dict: çµæœå­—å…¸ï¼Œé”®ç‚ºä»»å‹™åç¨±ï¼Œå€¼ç‚ºçµæœ
        """
        results = {}
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_name = {}
            for task in tasks:
                future = executor.submit(
                    task['func'], 
                    *task.get('args', ()), 
                    **task.get('kwargs', {})
                )
                future_to_name[future] = task['name']
            
            # æ”¶é›†çµæœ
            for future in as_completed(future_to_name):
                task_name = future_to_name[future]
                try:
                    result = future.result(timeout=30)  # 30ç§’è¶…æ™‚
                    results[task_name] = result
                    logger.debug(f"âœ… ä»»å‹™å®Œæˆ: {task_name}")
                except Exception as e:
                    logger.error(f"âŒ ä»»å‹™å¤±è´¥: {task_name}, éŒ¯èª¤: {e}")
                    results[task_name] = None
        
        return results
```

## ğŸ›¡ï¸ éŒ¯èª¤è™•ç†å’Œé™ç´šç­–ç•¥

### æ•¸æ“šæºé™ç´š

```python
class DataSourceFallback:
    """æ•¸æ“šæºé™ç´šè™•ç†å™¨"""
    
    def __init__(self, manager: DataSourceManager):
        self.manager = manager
        self.fallback_order = {
            'china_stock': ['tushare', 'akshare', 'baostock'],
            'us_stock': ['yahoo_finance', 'finnhub'],
            'hk_stock': ['yahoo_finance', 'akshare']
        }
    
    def get_data_with_fallback(self, ticker: str, data_type: str, 
                              get_data_func: Callable, *args, **kwargs):
        """ä½¿ç”¨é™ç´šç­–ç•¥ç²å–æ•¸æ“š
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç¢¼
            data_type: æ•¸æ“šé¡å‹
            get_data_func: æ•¸æ“šç²å–å‡½æ•¸
            *args, **kwargs: å‡½æ•¸åƒæ•¸
        
        Returns:
            æ•¸æ“šæˆ–éŒ¯èª¤ä¿¡æ¯
        """
        from tradingagents.utils.stock_utils import StockUtils
        
        market_info = StockUtils.get_market_info(ticker)
        market_type = market_info['market_type']
        
        # ç¢ºå®šé™ç´šé¡ºåº
        if market_type == 'Aè‚¡':
            sources = self.fallback_order['china_stock']
        elif market_type == 'ç¾è‚¡':
            sources = self.fallback_order['us_stock']
        elif market_type == 'æ¸¯è‚¡':
            sources = self.fallback_order['hk_stock']
        else:
            sources = ['yahoo_finance']  # é»˜èª
        
        last_error = None
        
        for source in sources:
            try:
                # åˆ‡æ›æ•¸æ“šæº
                if source in self.manager.get_available_sources():
                    self.manager.switch_source(source)
                    
                    # å˜—è©¦ç²å–æ•¸æ“š
                    data = get_data_func(*args, **kwargs)
                    
                    if data is not None and not (hasattr(data, 'empty') and data.empty):
                        logger.info(f"âœ… ä½¿ç”¨{source}æ•¸æ“šæºæˆåŠŸç²å–{ticker}çš„{data_type}æ•¸æ“š")
                        return data
                    else:
                        logger.warning(f"âš ï¸ {source}æ•¸æ“šæºè¿”å›ç©ºæ•¸æ“š")
                        
            except Exception as e:
                last_error = e
                logger.warning(f"âš ï¸ {source}æ•¸æ“šæºå¤±è´¥: {e}")
                continue
        
        # æ‰€æœ‰æ•¸æ“šæºéƒ½å¤±è´¥
        error_msg = f"âŒ æ‰€æœ‰æ•¸æ“šæºéƒ½ç„¡æ³•ç²å–{ticker}çš„{data_type}æ•¸æ“š"
        if last_error:
            error_msg += f"ï¼Œæœ€å¾ŒéŒ¯èª¤: {last_error}"
        
        logger.error(error_msg)
        return error_msg
```

## ğŸ“ˆ ç›£æ§å’Œè§€æ¸¬

### æ•¸æ“šæµç›£æ§

```python
class DataFlowMonitor:
    """æ•¸æ“šæµç›£æ§å™¨"""
    
    def __init__(self):
        self.metrics = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'average_response_time': 0.0,
            'data_source_usage': {},
        }
    
    def record_request(self, ticker: str, data_type: str, 
                      success: bool, response_time: float, 
                      data_source: str, from_cache: bool):
        """è¨˜éŒ„æ•¸æ“šè«‹æ±‚"""
        self.metrics['total_requests'] += 1
        
        if success:
            self.metrics['successful_requests'] += 1
        else:
            self.metrics['failed_requests'] += 1
        
        if from_cache:
            self.metrics['cache_hits'] += 1
        else:
            self.metrics['cache_misses'] += 1
        
        # æ›´æ–°å¹³å‡éŸ¿æ‡‰æ™‚é–“
        total_time = self.metrics['average_response_time'] * (self.metrics['total_requests'] - 1)
        self.metrics['average_response_time'] = (total_time + response_time) / self.metrics['total_requests']
        
        # è¨˜éŒ„æ•¸æ“šæºä½¿ç”¨æƒ…å†µ
        if data_source not in self.metrics['data_source_usage']:
            self.metrics['data_source_usage'][data_source] = 0
        self.metrics['data_source_usage'][data_source] += 1
        
        logger.info(f"ğŸ“Š æ•¸æ“šè«‹æ±‚è¨˜éŒ„: {ticker} {data_type} {'âœ…' if success else 'âŒ'} {response_time:.2f}s {data_source} {'(ç·©å­˜)' if from_cache else ''}")
    
    def get_metrics_report(self) -> str:
        """ç”Ÿæˆç›£æ§å ±å‘Š"""
        if self.metrics['total_requests'] == 0:
            return "ğŸ“Š æš‚ç„¡æ•¸æ“šè«‹æ±‚è¨˜éŒ„"
        
        success_rate = self.metrics['successful_requests'] / self.metrics['total_requests']
        cache_hit_rate = self.metrics['cache_hits'] / self.metrics['total_requests']
        
        report = f"""ğŸ“Š æ•¸æ“šæµç›£æ§å ±å‘Š
        
**è«‹æ±‚çµ±è¨ˆ**:
- æ€»è«‹æ±‚æ•¸: {self.metrics['total_requests']}
- æˆåŠŸè«‹æ±‚: {self.metrics['successful_requests']}
- å¤±è´¥è«‹æ±‚: {self.metrics['failed_requests']}
- æˆåŠŸç‡: {success_rate:.1%}

**ç·©å­˜çµ±è¨ˆ**:
- ç·©å­˜å‘½ä¸­: {self.metrics['cache_hits']}
- ç·©å­˜æœªå‘½ä¸­: {self.metrics['cache_misses']}
- ç·©å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1%}

**æ€§èƒ½çµ±è¨ˆ**:
- å¹³å‡éŸ¿æ‡‰æ™‚é–“: {self.metrics['average_response_time']:.2f}s

**æ•¸æ“šæºä½¿ç”¨æƒ…å†µ**:
"""
        
        for source, count in self.metrics['data_source_usage'].items():
            usage_rate = count / self.metrics['total_requests']
            report += f"- {source}: {count}æ¬¡ ({usage_rate:.1%})\n"
        
        return report

# å…¨å±€ç›£æ§å¯¦ä¾‹
data_flow_monitor = DataFlowMonitor()
```

## ğŸ”§ é…ç½®ç®¡ç†

### ç’°å¢ƒè®Šé‡é…ç½®

```bash
# .env æ–‡ä»¶ç¤ºä¾‹

# æ•¸æ“šæºé…ç½®
DEFAULT_CHINA_DATA_SOURCE=tushare
TUSHARE_TOKEN=your_tushare_token_here
FINNHUB_API_KEY=your_finnhub_api_key
REDDIT_CLIENT_ID=your_reddit_client_id
REDDIT_CLIENT_SECRET=your_reddit_client_secret

# æ•¸æ“šç›®éŒ„é…ç½®
DATA_DIR=./data
CACHE_DIR=./cache
RESULTS_DIR=./results

# ç·©å­˜é…ç½®
ENABLE_CACHE=true
CACHE_EXPIRY_MARKET_DATA=300
CACHE_EXPIRY_NEWS_DATA=3600
CACHE_EXPIRY_FUNDAMENTALS=86400
MAX_CACHE_SIZE=1000

# æ€§èƒ½é…ç½®
MAX_PARALLEL_WORKERS=5
REQUEST_TIMEOUT=30
RETRY_ATTEMPTS=3
RETRY_DELAY=1

# ç›£æ§é…ç½®
ENABLE_MONITORING=true
LOG_LEVEL=INFO
```

### å‹•æ…‹é…ç½®æ›´æ–°

```python
class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_file: str = None):
        self.config_file = config_file or '.env'
        self.config = self._load_config()
        self._setup_directories()
    
    def _load_config(self) -> dict:
        """åŠ è¼‰é…ç½®"""
        from dotenv import load_dotenv
        
        load_dotenv(self.config_file)
        
        return {
            # æ•¸æ“šæºé…ç½®
            'default_china_data_source': os.getenv('DEFAULT_CHINA_DATA_SOURCE', 'tushare'),
            'tushare_token': os.getenv('TUSHARE_TOKEN'),
            'finnhub_api_key': os.getenv('FINNHUB_API_KEY'),
            'reddit_client_id': os.getenv('REDDIT_CLIENT_ID'),
            'reddit_client_secret': os.getenv('REDDIT_CLIENT_SECRET'),
            
            # ç›®éŒ„é…ç½®
            'data_dir': os.getenv('DATA_DIR', './data'),
            'cache_dir': os.getenv('CACHE_DIR', './cache'),
            'results_dir': os.getenv('RESULTS_DIR', './results'),
            
            # ç·©å­˜é…ç½®
            'enable_cache': os.getenv('ENABLE_CACHE', 'true').lower() == 'true',
            'cache_expiry': {
                'market_data': int(os.getenv('CACHE_EXPIRY_MARKET_DATA', '300')),
                'news_data': int(os.getenv('CACHE_EXPIRY_NEWS_DATA', '3600')),
                'fundamentals': int(os.getenv('CACHE_EXPIRY_FUNDAMENTALS', '86400')),
            },
            'max_cache_size': int(os.getenv('MAX_CACHE_SIZE', '1000')),
            
            # æ€§èƒ½é…ç½®
            'max_parallel_workers': int(os.getenv('MAX_PARALLEL_WORKERS', '5')),
            'request_timeout': int(os.getenv('REQUEST_TIMEOUT', '30')),
            'retry_attempts': int(os.getenv('RETRY_ATTEMPTS', '3')),
            'retry_delay': float(os.getenv('RETRY_DELAY', '1.0')),
            
            # ç›£æ§é…ç½®
            'enable_monitoring': os.getenv('ENABLE_MONITORING', 'true').lower() == 'true',
            'log_level': os.getenv('LOG_LEVEL', 'INFO'),
        }
    
    def _setup_directories(self):
        """è¨­ç½®ç›®éŒ„"""
        for dir_key in ['data_dir', 'cache_dir', 'results_dir']:
            dir_path = self.config[dir_key]
            os.makedirs(dir_path, exist_ok=True)
            logger.info(f"ğŸ“ ç›®éŒ„å·²æº–å¤‡: {dir_key} = {dir_path}")
    
    def get(self, key: str, default=None):
        """ç²å–é…ç½®å€¼"""
        return self.config.get(key, default)
    
    def update(self, key: str, value):
        """æ›´æ–°é…ç½®å€¼"""
        self.config[key] = value
        logger.info(f"ğŸ”§ é…ç½®å·²æ›´æ–°: {key} = {value}")
    
    def reload(self):
        """é‡æ–°åŠ è¼‰é…ç½®"""
        self.config = self._load_config()
        self._setup_directories()
        logger.info("ğŸ”„ é…ç½®å·²é‡æ–°åŠ è¼‰")

# å…¨å±€é…ç½®å¯¦ä¾‹
config_manager = ConfigManager()
```

## ğŸš€ æœ€ä½³å¯¦è¸

### 1. æ•¸æ“šæºé¸æ“‡ç­–ç•¥

```python
# æ¨è–¦çš„æ•¸æ“šæºé…ç½®
RECOMMENDED_DATA_SOURCES = {
    'Aè‚¡': {
        'primary': 'tushare',      # ä¸»è¦æ•¸æ“šæºï¼šå°ˆæ¥­ã€ç©©å®š
        'fallback': ['akshare', 'baostock'],  # å¤‡ç”¨æ•¸æ“šæº
        'use_case': 'é©ç”¨æ–¼å°ˆæ¥­æŠ•è³‡åˆ†æï¼Œæ•¸æ“šè´¨é‡é«˜'
    },
    'æ¸¯è‚¡': {
        'primary': 'yahoo_finance',
        'fallback': ['akshare'],
        'use_case': 'åœ‹é™…åŒ–æ•¸æ“šæºï¼Œè¦†è“‹å…¨é¢'
    },
    'ç¾è‚¡': {
        'primary': 'yahoo_finance',
        'fallback': ['finnhub'],
        'use_case': 'å…è²»ä¸”ç©©å®šçš„ç¾è‚¡æ•¸æ“š'
    }
}
```

### 2. ç·©å­˜ç­–ç•¥å„ªåŒ–

```python
# ç·©å­˜éæœŸæ™‚é–“å»ºè®®
CACHE_EXPIRY_RECOMMENDATIONS = {
    'real_time_data': 60,        # å¯¦æ™‚æ•¸æ“šï¼š1åˆ†é˜
    'intraday_data': 300,        # æ—¥å…§æ•¸æ“šï¼š5åˆ†é˜
    'daily_data': 3600,          # æ—¥ç·šæ•¸æ“šï¼š1å°æ™‚
    'fundamental_data': 86400,   # åŸºæœ¬é¢æ•¸æ“šï¼š24å°æ™‚
    'news_data': 1800,           # æ–°èæ•¸æ“šï¼š30åˆ†é˜
    'social_sentiment': 900,     # ç¤¾äº¤æƒ…ç»ªï¼š15åˆ†é˜
}
```

### 3. éŒ¯èª¤è™•ç†æ¨¡å¼

```python
# éŒ¯èª¤è™•ç†æœ€ä½³å¯¦è¸
def robust_data_fetch(func):
    """æ•¸æ“šç²å–è£é¥°å™¨ï¼Œæä¾›çµ±ä¸€çš„éŒ¯èª¤è™•ç†"""
    def wrapper(*args, **kwargs):
        max_retries = 3
        retry_delay = 1.0
        
        for attempt in range(max_retries):
            try:
                result = func(*args, **kwargs)
                if result is not None:
                    return result
                else:
                    logger.warning(f"ç¬¬{attempt + 1}æ¬¡å˜—è©¦è¿”å›ç©ºæ•¸æ“š")
            except Exception as e:
                logger.warning(f"ç¬¬{attempt + 1}æ¬¡å˜—è©¦å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay * (2 ** attempt))  # æŒ‡æ•¸é€€é¿
                else:
                    logger.error(f"æ‰€æœ‰é‡è©¦éƒ½å¤±è´¥ï¼Œæœ€ç»ˆéŒ¯èª¤: {e}")
                    return None
        
        return None
    return wrapper
```

### 4. æ€§èƒ½ç›£æ§å»ºè®®

```python
# æ€§èƒ½ç›£æ§é—œé”®æŒ‡æ¨™
PERFORMANCE_THRESHOLDS = {
    'response_time': {
        'excellent': 1.0,    # 1ç§’ä»¥å…§
        'good': 3.0,         # 3ç§’ä»¥å…§
        'acceptable': 10.0,  # 10ç§’ä»¥å…§
    },
    'success_rate': {
        'excellent': 0.99,   # 99%ä»¥ä¸Š
        'good': 0.95,        # 95%ä»¥ä¸Š
        'acceptable': 0.90,  # 90%ä»¥ä¸Š
    },
    'cache_hit_rate': {
        'excellent': 0.80,   # 80%ä»¥ä¸Š
        'good': 0.60,        # 60%ä»¥ä¸Š
        'acceptable': 0.40,  # 40%ä»¥ä¸Š
    }
}
```

## ğŸ“‹ æ€»çµ

TradingAgents çš„æ•¸æ“šæµæ¶æ§‹å…·æœ‰ä»¥ä¸‹ç‰¹é»ï¼š

### âœ… å„ªåŠ¿

1. **çµ±ä¸€æ¥å£**: é€šéçµ±ä¸€çš„æ•¸æ“šæ¥å£å±è”½åº•å±¤æ•¸æ“šæºå·®ç•°
2. **æ™ºèƒ½é™ç´š**: è‡ªå‹•æ•¸æ“šæºåˆ‡æ›ï¼Œç¢ºä¿æ•¸æ“šç²å–çš„å¯é æ€§
3. **é«˜æ•ˆç·©å­˜**: å¤šå±¤ç·©å­˜ç­–ç•¥ï¼Œé¡¯è‘—æå‡éŸ¿æ‡‰é€Ÿåº¦
4. **è´¨é‡ç›£æ§**: å¯¦æ™‚æ•¸æ“šè´¨é‡æª¢æŸ¥å’Œæ€§èƒ½ç›£æ§
5. **çµæ´»æ“´å±•**: æ¨¡å¡ŠåŒ–è¨­è¨ˆï¼Œæ˜“æ–¼æ·»åŠ æ–°çš„æ•¸æ“šæº
6. **éŒ¯èª¤æ¢è¤‡**: å®Œå–„çš„éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶

### ğŸ¯ é©ç”¨å ´æ™¯

- **å¤šå¸‚å ´äº¤æ˜“**: æ”¯æŒAè‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡çš„çµ±ä¸€æ•¸æ“šè¨ªå•
- **å¯¦æ™‚åˆ†æ**: ä½å»¶è¿Ÿçš„æ•¸æ“šç²å–å’Œè™•ç†
- **å¤§è¦æ¨¡éƒ¨ç½²**: æ”¯æŒé«˜ä¸¦ç™¼å’Œå¤§æ•¸æ“šé‡è™•ç†
- **ç ”ç©¶é–‹ç™¼**: çµæ´»çš„æ•¸æ“šæºé…ç½®å’Œæ“´å±•èƒ½åŠ›

### ğŸ”® æœªä¾†ç™¼å±•

1. **å¯¦æ™‚æ•¸æ“šæµ**: é›†æˆWebSocketå¯¦æ™‚æ•¸æ“šæ¨é€
2. **æ©Ÿå™¨å­¸ä¹ **: æ•¸æ“šè´¨é‡æ™ºèƒ½è©•ä¼°å’Œé æ¸¬
3. **äº‘åŸç”Ÿ**: æ”¯æŒäº‘ç«¯æ•¸æ“šæºå’Œåˆ†å¸ƒå¼ç·©å­˜
4. **åœ‹é™…åŒ–**: æ“´å±•æ›´å¤šåœ‹é™…å¸‚å ´æ•¸æ“šæº

é€šéé€™å€‹æ•¸æ“šæµæ¶æ§‹ï¼ŒTradingAgents èƒ½å¤ ç‚ºæ™ºèƒ½é«”æä¾›é«˜è´¨é‡ã€é«˜å¯ç”¨çš„é‡‘èæ•¸æ“šæœå‹™ï¼Œæ”¯æ’‘è¤‡é›œçš„æŠ•è³‡æ±ºç­–åˆ†æã€‚