# TradingAgents è³‡æ–™æµæ¶æ§‹

## æ¦‚è¿°

TradingAgents æ¡ç”¨å¤šå±¤æ¬¡è³‡æ–™æµæ¶æ§‹ï¼Œæ”¯æ´å…¨çƒè‚¡ç¥¨å¸‚å ´çš„å…¨é¢è³‡æ–™å–å¾—å’Œè™•ç†ã€‚ç³»çµ±é€éçµ±ä¸€çš„è³‡æ–™ä»‹é¢ã€æ™ºæ…§çš„è³‡æ–™ä¾†æºç®¡ç†å’Œé«˜æ•ˆçš„å¿«å–æ©Ÿåˆ¶ï¼Œç‚ºæ™ºæ…§é«”æä¾›é«˜å“è³ªçš„é‡‘èè³‡æ–™æœå‹™ã€‚

## ğŸ—ï¸ è³‡æ–™æµæ¶æ§‹è¨­è¨ˆ

### æ¶æ§‹å±¤æ¬¡åœ–

```mermaid
graph TB
    subgraph "å¤–éƒ¨è³‡æ–™ä¾†æºå±¤ (External Data Sources)"
        subgraph "åœ‹éš›å¸‚å ´è³‡æ–™"
            YFINANCE[Yahoo Finance]
            FINNHUB[FinnHub]
            SIMFIN[SimFin]
        end

        subgraph "æ–°èæƒ…ç·’è³‡æ–™"
            REDDIT[Redditç¤¾ç¾¤åª’é«”]
            GOOGLENEWS[Googleæ–°è]
        end
    end

    subgraph "è³‡æ–™å–å¾—å±¤ (Data Acquisition Layer)"
        DSM[è³‡æ–™ä¾†æºç®¡ç†å™¨]
        ADAPTERS[è³‡æ–™é©é…å™¨]
        API_MGR[APIç®¡ç†å™¨]
    end

    subgraph "è³‡æ–™è™•ç†å±¤ (Data Processing Layer)"
        CLEANER[è³‡æ–™æ¸…ç†]
        TRANSFORMER[è³‡æ–™è½‰æ›]
        VALIDATOR[è³‡æ–™é©—è­‰]
        QUALITY[å“è³ªæ§åˆ¶]
    end

    subgraph "è³‡æ–™å„²å­˜å±¤ (Data Storage Layer)"
        CACHE[å¿«å–ç³»çµ±]
        FILES[æª”æ¡ˆå„²å­˜]
        CONFIG[è¨­å®šç®¡ç†]
    end

    subgraph "è³‡æ–™åˆ†ç™¼å±¤ (Data Distribution Layer)"
        INTERFACE[çµ±ä¸€è³‡æ–™ä»‹é¢]
        ROUTER[è³‡æ–™è·¯ç”±å™¨]
        FORMATTER[æ ¼å¼åŒ–å™¨]
    end

    subgraph "å·¥å…·æ•´åˆå±¤ (Tool Integration Layer)"
        TOOLKIT[Toolkitå·¥å…·åŒ…]
        UNIFIED_TOOLS[çµ±ä¸€å·¥å…·ä»‹é¢]
        STOCK_UTILS[è‚¡ç¥¨å·¥å…·]
    end

    subgraph "æ™ºæ…§é«”æ¶ˆè²»å±¤ (Agent Consumption Layer)"
        ANALYSTS[åˆ†æå¸«æ™ºæ…§é«”]
        RESEARCHERS[ç ”ç©¶å“¡æ™ºæ…§é«”]
        TRADER[äº¤æ˜“å“¡æ™ºæ…§é«”]
        MANAGERS[ç®¡ç†å±¤æ™ºæ…§é«”]
    end

    %% è³‡æ–™æµå‘
    YFINANCE --> ADAPTERS
    FINNHUB --> ADAPTERS
    SIMFIN --> ADAPTERS
    REDDIT --> API_MGR
    GOOGLENEWS --> API_MGR

    ADAPTERS --> CLEANER
    API_MGR --> CLEANER

    CLEANER --> TRANSFORMER
    TRANSFORMER --> VALIDATOR
    VALIDATOR --> QUALITY

    QUALITY --> CACHE
    QUALITY --> FILES
    QUALITY --> CONFIG

    CACHE --> INTERFACE
    FILES --> INTERFACE
    CONFIG --> INTERFACE

    INTERFACE --> ROUTER
    ROUTER --> FORMATTER

    FORMATTER --> TOOLKIT
    TOOLKIT --> UNIFIED_TOOLS
    UNIFIED_TOOLS --> STOCK_UTILS

    STOCK_UTILS --> ANALYSTS
    STOCK_UTILS --> RESEARCHERS
    STOCK_UTILS --> TRADER
    STOCK_UTILS --> MANAGERS
```

## ğŸ“Š å„å±¤æ¬¡è©³ç´°èªªæ˜

### 1. å¤–éƒ¨è³‡æ–™ä¾†æºå±¤ (External Data Sources)

#### Yahoo Finance
**æª”æ¡ˆä½ç½®**: `tradingagents/dataflows/yfin_utils.py`

```python
import yfinance as yf
import pandas as pd
from typing import Optional

def get_yahoo_finance_data(ticker: str, period: str = "1y",
                          start_date: str = None, end_date: str = None):
    """å–å¾—Yahoo Financeè³‡æ–™

    Args:
        ticker: è‚¡ç¥¨ä»£è™Ÿ
        period: æ™‚é–“é€±æœŸ (1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max)
        start_date: é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: çµæŸæ—¥æœŸ (YYYY-MM-DD)

    Returns:
        DataFrame: è‚¡ç¥¨è³‡æ–™
    """
    try:
        stock = yf.Ticker(ticker)

        if start_date and end_date:
            data = stock.history(start=start_date, end=end_date)
        else:
            data = stock.history(period=period)

        if data.empty:
            logger.warning(f"Yahoo Financeæœªæ‰¾åˆ°{ticker}çš„è³‡æ–™")
            return None

        return data
    except Exception as e:
        logger.error(f"Yahoo Financeè³‡æ–™å–å¾—å¤±æ•—: {e}")
        return None
```

#### FinnHub æ–°èå’ŒåŸºæœ¬é¢è³‡æ–™
**æª”æ¡ˆä½ç½®**: `tradingagents/dataflows/finnhub_utils.py`

```python
from datetime import datetime, relativedelta
import json
import os

def get_data_in_range(ticker: str, start_date: str, end_date: str,
                     data_type: str, data_dir: str):
    """å¾å¿«å–ä¸­å–å¾—æŒ‡å®šæ™‚é–“ç¯„åœçš„è³‡æ–™

    Args:
        ticker: è‚¡ç¥¨ä»£è™Ÿ
        start_date: é–‹å§‹æ—¥æœŸ
        end_date: çµæŸæ—¥æœŸ
        data_type: è³‡æ–™é¡å‹ (news_data, insider_senti, insider_trans)
        data_dir: è³‡æ–™ç›®éŒ„

    Returns:
        dict: è³‡æ–™å­—å…¸
    """
    try:
        file_path = os.path.join(data_dir, f"{ticker}_{data_type}.json")

        if not os.path.exists(file_path):
            logger.warning(f"è³‡æ–™æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
            return {}

        with open(file_path, 'r', encoding='utf-8') as f:
            all_data = json.load(f)

        # éæ¿¾æ™‚é–“ç¯„åœå…§çš„è³‡æ–™
        filtered_data = {}
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")

        for date_str, data in all_data.items():
            try:
                data_dt = datetime.strptime(date_str, "%Y-%m-%d")
                if start_dt <= data_dt <= end_dt:
                    filtered_data[date_str] = data
            except ValueError:
                continue

        return filtered_data
    except Exception as e:
        logger.error(f"è³‡æ–™å–å¾—å¤±æ•—: {e}")
        return {}
```

### 2. è³‡æ–™è™•ç†å±¤ (Data Processing Layer)

#### è³‡æ–™é©—è­‰å’Œæ¸…ç†

```python
def validate_and_clean_data(data, data_type: str):
    """è³‡æ–™é©—è­‰å’Œæ¸…ç†

    Args:
        data: åŸå§‹è³‡æ–™
        data_type: è³‡æ–™é¡å‹

    Returns:
        è™•ç†å¾Œçš„è³‡æ–™
    """
    if data is None or (hasattr(data, 'empty') and data.empty):
        return None

    try:
        if data_type == "stock_data":
            required_columns = ['open', 'high', 'low', 'close', 'volume']
            if hasattr(data, 'columns'):
                missing_cols = [col for col in required_columns if col not in data.columns]
                if missing_cols:
                    logger.warning(f"âš ï¸ ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_cols}")

                # è³‡æ–™æ¸…ç†
                data = data.dropna()
                data = data[data['volume'] > 0]

        elif data_type == "news_data":
            if isinstance(data, str) and len(data.strip()) == 0:
                return None

        return data
    except Exception as e:
        logger.error(f"è³‡æ–™é©—è­‰å¤±æ•—: {e}")
        return None
```

### 3. å·¥å…·æ•´åˆå±¤ (Tool Integration Layer)

#### Toolkit çµ±ä¸€å·¥å…·åŒ…

```python
class Toolkit:
    """çµ±ä¸€å·¥å…·åŒ…ï¼Œç‚ºæ‰€æœ‰æ™ºæ…§é«”æä¾›è³‡æ–™å­˜å–ä»‹é¢"""

    def __init__(self, config):
        self.config = config
        self.logger = get_logger('agents')

    def get_stock_fundamentals_unified(self, ticker: str):
        """çµ±ä¸€åŸºæœ¬é¢åˆ†æå·¥å…·"""
        try:
            return self._get_us_stock_fundamentals(ticker)
        except Exception as e:
            self.logger.error(f"åŸºæœ¬é¢è³‡æ–™å–å¾—å¤±æ•—: {e}")
            return f"âŒ åŸºæœ¬é¢è³‡æ–™å–å¾—å¤±æ•—: {str(e)}"

    def get_market_data(self, ticker: str, period: str = "1y"):
        """å–å¾—å¸‚å ´è³‡æ–™"""
        return get_yahoo_finance_data(ticker, period)

    def get_news_data(self, ticker: str, days: int = 7):
        """å–å¾—æ–°èè³‡æ–™"""
        return get_finnhub_news(ticker, datetime.now().strftime("%Y-%m-%d"), days)
```

## ğŸ”„ è³‡æ–™æµè½‰éç¨‹

### å®Œæ•´è³‡æ–™æµç¨‹åœ–

```mermaid
sequenceDiagram
    participant Agent as æ™ºæ…§é«”
    participant Toolkit as å·¥å…·åŒ…
    participant Interface as è³‡æ–™ä»‹é¢
    participant Cache as å¿«å–ç³»çµ±
    participant Source as è³‡æ–™ä¾†æº

    Agent->>Toolkit: è«‹æ±‚è‚¡ç¥¨è³‡æ–™
    Toolkit->>Interface: å‘¼å«çµ±ä¸€ä»‹é¢
    Interface->>Cache: æª¢æŸ¥å¿«å–

    alt å¿«å–å‘½ä¸­
        Cache->>Interface: å›å‚³å¿«å–è³‡æ–™
    else å¿«å–æœªå‘½ä¸­
        Interface->>Source: å‘¼å«è³‡æ–™ä¾†æºAPI
        Source->>Interface: å›å‚³åŸå§‹è³‡æ–™
        Interface->>Cache: æ›´æ–°å¿«å–
    end

    Interface->>Toolkit: å›å‚³æ ¼å¼åŒ–è³‡æ–™
    Toolkit->>Agent: å›å‚³åˆ†æå°±ç·’è³‡æ–™
```

## ğŸ“Š æ•ˆèƒ½å„ªåŒ–

### å¿«å–ç­–ç•¥

```python
class CacheManager:
    """å¿«å–ç®¡ç†å™¨"""

    def __init__(self, config):
        self.config = config
        self.cache_dir = config.get('cache_dir', './cache')
        self.cache_expiry = config.get('cache_expiry', {})
        self.max_cache_size = config.get('max_cache_size', 1000)

    def get_cache_key(self, ticker: str, data_type: str, params: dict = None) -> str:
        """ç”¢ç”Ÿå¿«å–éµ"""
        import hashlib

        key_parts = [ticker, data_type]
        if params:
            key_parts.append(str(sorted(params.items())))

        key_string = '|'.join(key_parts)
        return hashlib.md5(key_string.encode()).hexdigest()
```

## ğŸ”§ è¨­å®šç®¡ç†

### ç’°å¢ƒè®Šæ•¸è¨­å®š

```bash
# .env æª”æ¡ˆç¯„ä¾‹

# è³‡æ–™ä¾†æºè¨­å®š
FINNHUB_API_KEY=your_finnhub_api_key
REDDIT_CLIENT_ID=your_reddit_client_id
REDDIT_CLIENT_SECRET=your_reddit_client_secret

# è³‡æ–™ç›®éŒ„è¨­å®š
DATA_DIR=./data
CACHE_DIR=./cache
RESULTS_DIR=./results

# å¿«å–è¨­å®š
ENABLE_CACHE=true
CACHE_EXPIRY_MARKET_DATA=300
CACHE_EXPIRY_NEWS_DATA=3600
CACHE_EXPIRY_FUNDAMENTALS=86400
MAX_CACHE_SIZE=1000

# æ•ˆèƒ½è¨­å®š
MAX_PARALLEL_WORKERS=5
REQUEST_TIMEOUT=30
RETRY_ATTEMPTS=3
RETRY_DELAY=1

# ç›£æ§è¨­å®š
ENABLE_MONITORING=true
LOG_LEVEL=INFO
```

## ğŸ“‹ ç¸½çµ

TradingAgents çš„è³‡æ–™æµæ¶æ§‹å…·æœ‰ä»¥ä¸‹ç‰¹é»ï¼š

### âœ… å„ªå‹¢

1. **çµ±ä¸€ä»‹é¢**: é€éçµ±ä¸€çš„è³‡æ–™ä»‹é¢é®è”½åº•å±¤è³‡æ–™ä¾†æºå·®ç•°
2. **æ™ºæ…§é™ç´š**: è‡ªå‹•è³‡æ–™ä¾†æºåˆ‡æ›ï¼Œç¢ºä¿è³‡æ–™å–å¾—çš„å¯é æ€§
3. **é«˜æ•ˆå¿«å–**: å¤šå±¤å¿«å–ç­–ç•¥ï¼Œé¡¯è‘—æå‡å›æ‡‰é€Ÿåº¦
4. **å“è³ªç›£æ§**: å³æ™‚è³‡æ–™å“è³ªæª¢æŸ¥å’Œæ•ˆèƒ½ç›£æ§
5. **å½ˆæ€§æ“´å±•**: æ¨¡çµ„åŒ–è¨­è¨ˆï¼Œæ˜“æ–¼æ–°å¢æ–°çš„è³‡æ–™ä¾†æº
6. **éŒ¯èª¤æ¢å¾©**: å®Œå–„çš„éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶

### ğŸ¯ é©ç”¨å ´æ™¯

- **å¤šå¸‚å ´äº¤æ˜“**: æ”¯æ´å…¨çƒè‚¡ç¥¨å¸‚å ´çš„çµ±ä¸€è³‡æ–™å­˜å–
- **å³æ™‚åˆ†æ**: ä½å»¶é²çš„è³‡æ–™å–å¾—å’Œè™•ç†
- **å¤§è¦æ¨¡éƒ¨ç½²**: æ”¯æ´é«˜ä¸¦è¡Œå’Œå¤§è³‡æ–™é‡è™•ç†
- **ç ”ç©¶é–‹ç™¼**: å½ˆæ€§çš„è³‡æ–™ä¾†æºè¨­å®šå’Œæ“´å±•èƒ½åŠ›

é€éé€™å€‹è³‡æ–™æµæ¶æ§‹ï¼ŒTradingAgents èƒ½å¤ ç‚ºæ™ºæ…§é«”æä¾›é«˜å“è³ªã€é«˜å¯ç”¨çš„é‡‘èè³‡æ–™æœå‹™ï¼Œæ”¯æ’è¤‡é›œçš„æŠ•è³‡æ±ºç­–åˆ†æã€‚
