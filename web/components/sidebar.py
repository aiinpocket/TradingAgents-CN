"""
ä¾§é‚Šæ çµ„ä»¶
"""

import streamlit as st
import os
import logging
import sys
from pathlib import Path

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from web.utils.persistence import load_model_selection, save_model_selection
from web.utils.auth_manager import auth_manager

logger = logging.getLogger(__name__)

def get_version():
    """å¾VERSIONæ–‡ä»¶è®€å–é …ç›®ç‰ˆæœ¬è™Ÿ"""
    try:
        version_file = project_root / "VERSION"
        if version_file.exists():
            return version_file.read_text().strip()
        else:
            return "unknown"
    except Exception as e:
        logger.warning(f"ç„¡æ³•è®€å–ç‰ˆæœ¬æ–‡ä»¶: {e}")
        return "unknown"

def render_sidebar():
    """æ¸²æŸ“ä¾§é‚Šæ é…ç½®"""

    # æ·»åŠ localStorageæ”¯æŒçš„JavaScript
    st.markdown("""
    <script>
    // ä¿å­˜åˆ°localStorage
    function saveToLocalStorage(key, value) {
        localStorage.setItem('tradingagents_' + key, value);
        console.log('Saved to localStorage:', key, value);
    }

    // å¾localStorageè®€å–
    function loadFromLocalStorage(key, defaultValue) {
        const value = localStorage.getItem('tradingagents_' + key);
        console.log('Loaded from localStorage:', key, value || defaultValue);
        return value || defaultValue;
    }

    // é¡µé¢åŠ è¼‰æ™‚æ¢è¤‡è¨­ç½®
    window.addEventListener('load', function() {
        console.log('Page loaded, restoring settings...');
    });
    </script>
    """, unsafe_allow_html=True)

    # ä¾§é‚Šæ ç‰¹å®šæ¨£å¼ï¼ˆå…¨å±€æ¨£å¼åœ¨global_sidebar.cssä¸­ï¼‰
    st.markdown("""
    <style>
    /* ä¾§é‚Šæ å®½åº¦å’ŒåŸºç¡€æ¨£å¼å·²åœ¨global_sidebar.cssä¸­å®šç¾© */

    /* ä¾§é‚Šæ ç‰¹å®šçš„å…§é‚Šè·å’Œçµ„ä»¶æ¨£å¼ */
    section[data-testid="stSidebar"] .block-container,
    section[data-testid="stSidebar"] > div > div,
    .css-1d391kg,
    .css-1lcbmhc,
    .css-1cypcdb {
        padding-top: 0.2rem !important;
        padding-left: 0.5rem !important;
        padding-right: 0.5rem !important;
        padding-bottom: 0.75rem !important;
    }

    /* å„ªåŒ–selectboxå®¹å™¨ */
    section[data-testid="stSidebar"] .stSelectbox {
        margin-bottom: 0.4rem !important;
        width: 100% !important;
    }

    /* å„ªåŒ–ä¸‹æ‹‰æ¡†é¸é …æ–‡æœ¬ */
    section[data-testid="stSidebar"] .stSelectbox label {
        font-size: 0.85rem !important;
        font-weight: 600 !important;
        margin-bottom: 0.2rem !important;
    }

    /* å„ªåŒ–æ–‡æœ¬è¼¸å…¥æ¡† */
    section[data-testid="stSidebar"] .stTextInput > div > div > input {
        font-size: 0.8rem !important;
        padding: 0.3rem 0.5rem !important;
        width: 100% !important;
    }

    /* å„ªåŒ–æŒ‰é’®æ¨£å¼ */
    section[data-testid="stSidebar"] .stButton > button {
        width: 100% !important;
        font-size: 0.8rem !important;
        padding: 0.3rem 0.5rem !important;
        margin: 0.1rem 0 !important;
        border-radius: 0.3rem !important;
    }

    /* å„ªåŒ–æ¨™é¡Œæ¨£å¼ */
    section[data-testid="stSidebar"] h3 {
        font-size: 1rem !important;
        margin-bottom: 0.5rem !important;
        margin-top: 0rem !important;
        padding: 0 !important;
    }

    /* å„ªåŒ–infoæ¡†æ¨£å¼ */
    section[data-testid="stSidebar"] .stAlert {
        padding: 0.4rem !important;
        margin: 0.3rem 0 !important;
        font-size: 0.75rem !important;
    }

    /* å„ªåŒ–markdownæ–‡æœ¬ */
    section[data-testid="stSidebar"] .stMarkdown {
        margin-bottom: 0.3rem !important;
        padding: 0 !important;
    }

    /* å„ªåŒ–åˆ†éš”ç·š */
    section[data-testid="stSidebar"] hr {
        margin: 0.75rem 0 !important;
    }

    /* ç¢ºä¿ä¸‹æ‹‰æ¡†é¸é …å®Œå…¨å¯è§ - èª¿æ•´ç‚ºé©åˆ320px */
    .stSelectbox [data-baseweb="select"] {
        min-width: 260px !important;
        max-width: 280px !important;
    }

    /* å„ªåŒ–ä¸‹æ‹‰æ¡†é¸é …åˆ—è¡¨ */
    .stSelectbox [role="listbox"] {
        min-width: 260px !important;
        max-width: 290px !important;
    }

    /* é¡å¤–çš„é‚Šè·æ§åˆ¶ - ç¢ºä¿å·¦å³é‚Šè·å‡å° */
    .sidebar .element-container {
        padding: 0 !important;
        margin: 0.2rem 0 !important;
    }

    /* å¼·åˆ¶è¦†è“‹é»˜èªæ¨£å¼ */
    .css-1d391kg .element-container {
        padding-left: 0.5rem !important;
        padding-right: 0.5rem !important;
    }

    /* å‡å°‘ä¾§é‚Šæ é¡¶éƒ¨ç©ºç™½ */
    section[data-testid="stSidebar"] > div:first-child {
        padding-top: 0 !important;
        margin-top: 0 !important;
    }

    /* å‡å°‘ç¬¬ä¸€å€‹å…ƒç´ çš„é¡¶éƒ¨é‚Šè· */
    section[data-testid="stSidebar"] .element-container:first-child {
        margin-top: 0 !important;
        padding-top: 0 !important;
    }

    /* å‡å°‘æ¨™é¡Œçš„é¡¶éƒ¨é‚Šè· */
    section[data-testid="stSidebar"] h1,
    section[data-testid="stSidebar"] h2,
    section[data-testid="stSidebar"] h3 {
        margin-top: 0 !important;
        padding-top: 0 !important;
    }
    </style>
    """, unsafe_allow_html=True)

    with st.sidebar:
        # ä½¿ç”¨çµ„ä»¶ä¾†å¾localStorageè®€å–ä¸¦åˆå§‹åŒ–session state
        st.markdown("""
        <div id="localStorage-reader" style="display: none;">
            <script>
            // å¾localStorageè®€å–è¨­ç½®ä¸¦ç™¼é€çµ¦Streamlit
            const provider = loadFromLocalStorage('llm_provider', 'dashscope');
            const category = loadFromLocalStorage('model_category', 'openai');
            const model = loadFromLocalStorage('llm_model', '');

            // é€šéè‡ªå®šç¾©äº‹ä»¶ç™¼é€æ•¸æ“š
            window.parent.postMessage({
                type: 'localStorage_data',
                provider: provider,
                category: category,
                model: model
            }, '*');
            </script>
        </div>
        """, unsafe_allow_html=True)

        # å¾æŒä¹…åŒ–å­˜å‚¨åŠ è¼‰é…ç½®
        saved_config = load_model_selection()

        # åˆå§‹åŒ–session stateï¼Œå„ªå…ˆä½¿ç”¨ä¿å­˜çš„é…ç½®
        if 'llm_provider' not in st.session_state:
            st.session_state.llm_provider = saved_config['provider']
            logger.debug(f"ğŸ”§ [Persistence] æ¢è¤‡ llm_provider: {st.session_state.llm_provider}")
        if 'model_category' not in st.session_state:
            st.session_state.model_category = saved_config['category']
            logger.debug(f"ğŸ”§ [Persistence] æ¢è¤‡ model_category: {st.session_state.model_category}")
        if 'llm_model' not in st.session_state:
            st.session_state.llm_model = saved_config['model']
            logger.debug(f"ğŸ”§ [Persistence] æ¢è¤‡ llm_model: {st.session_state.llm_model}")

        # é¡¯ç¤ºç•¶å‰session stateç‹€æ…‹ï¼ˆèª¿è©¦ç”¨ï¼‰
        logger.debug(f"ğŸ” [Session State] ç•¶å‰ç‹€æ…‹ - provider: {st.session_state.llm_provider}, category: {st.session_state.model_category}, model: {st.session_state.llm_model}")

        # AIæ¨¡å‹é…ç½®
        st.markdown("### ğŸ§  AIæ¨¡å‹é…ç½®")

        # LLMæä¾›å•†é¸æ“‡
        llm_provider = st.selectbox(
            "LLMæä¾›å•†",
            options=["google", "openai", "openrouter", "custom_openai", "anthropic", "ollama"],
            index=["google", "openai", "openrouter", "custom_openai", "anthropic", "ollama"].index(st.session_state.llm_provider) if st.session_state.llm_provider in ["google", "openai", "openrouter", "custom_openai", "anthropic", "ollama"] else 0,
            format_func=lambda x: {
                "google": "ğŸŒŸ Google AI",
                "openai": "ğŸ¤– OpenAI",
                "openrouter": "ğŸŒ OpenRouter",
                "custom_openai": "ğŸ”§ è‡ªå®šç¾©OpenAIç«¯é»",
                "anthropic": "ğŸ¤– Anthropic (Claude)",
                "ollama": "ğŸ’» Ollama (æœ¬åœ°)"
            }[x],
            help="é¸æ“‡AIæ¨¡å‹æä¾›å•†",
            key="llm_provider_select"
        )

        # æ›´æ–°session stateå’ŒæŒä¹…åŒ–å­˜å‚¨
        if st.session_state.llm_provider != llm_provider:
            logger.info(f"ğŸ”„ [Persistence] æä¾›å•†è®Šæ›´: {st.session_state.llm_provider} â†’ {llm_provider}")
            st.session_state.llm_provider = llm_provider
            # æä¾›å•†è®Šæ›´æ™‚æ¸…ç©ºæ¨¡å‹é¸æ“‡
            st.session_state.llm_model = ""
            st.session_state.model_category = "openai"  # é‡ç½®ç‚ºé»˜èªé¡åˆ¥
            logger.info(f"ğŸ”„ [Persistence] æ¸…ç©ºæ¨¡å‹é¸æ“‡")

            # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
            save_model_selection(llm_provider, st.session_state.model_category, "")
        else:
            st.session_state.llm_provider = llm_provider

        # æ ¹æ“šæä¾›å•†é¡¯ç¤ºä¸åŒçš„æ¨¡å‹é¸é …
        if llm_provider == "google":
            google_options = [
                "gemini-2.5-pro",
                "gemini-2.5-flash",
                "gemini-2.5-flash-lite",
                "gemini-2.5-pro-002",
                "gemini-2.5-flash-002",
                "gemini-2.5-flash-preview-05-20",
                "gemini-2.5-flash-lite-preview-06-17",
                "gemini-2.0-flash",
                "gemini-2.0-flash-lite",
                "gemini-2.0-pro-experimental",
                "gemini-1.5-pro",
                "gemini-1.5-flash"
            ]

            # ç²å–ç•¶å‰é¸æ“‡çš„ç´¢å¼•
            current_index = 0
            if st.session_state.llm_model in google_options:
                current_index = google_options.index(st.session_state.llm_model)

            llm_model = st.selectbox(
                "é¸æ“‡Googleæ¨¡å‹",
                options=google_options,
                index=current_index,
                format_func=lambda x: {
                    "gemini-2.5-pro": "ğŸš€ Gemini 2.5 Pro - æœ€æ–°æ——è‰¦æ¨¡å‹ï¼ˆè‡ªé©æ‡‰æ€ç¶­ï¼‰",
                    "gemini-2.5-flash": "âš¡ Gemini 2.5 Flash - æœ€æ–°å¿«é€Ÿæ¨¡å‹ï¼ˆSWE-Bench 54%ï¼‰",
                    "gemini-2.5-flash-lite": "ğŸ’¡ Gemini 2.5 Flash Lite - è¼•é‡å¿«é€Ÿ",
                    "gemini-2.5-pro-002": "ğŸ”§ Gemini 2.5 Pro-002 - å„ªåŒ–ç‰ˆæœ¬",
                    "gemini-2.5-flash-002": "âš¡ Gemini 2.5 Flash-002 - å„ªåŒ–å¿«é€Ÿç‰ˆ",
                    "gemini-2.5-flash-preview-05-20": "ğŸ§ª Gemini 2.5 Flash Preview - é è¦½ç‰ˆï¼ˆæ¨ç†å¼·åŒ–ï¼‰",
                    "gemini-2.5-flash-lite-preview-06-17": "âš¡ Gemini 2.5 Flash Lite Preview - è¶…å¿«éŸ¿æ‡‰",
                    "gemini-2.0-flash": "ğŸš€ Gemini 2.0 Flash - æ¨è–¦ä½¿ç”¨",
                    "gemini-2.0-flash-lite": "ğŸ’¡ Gemini 2.0 Flash Lite - è¼•é‡ç‰ˆ",
                    "gemini-2.0-pro-experimental": "ğŸ§ª Gemini 2.0 Pro Experimental - å¯¦é©—ç‰ˆæœ¬",
                    "gemini-1.5-pro": "Gemini 1.5 Pro - å¼·å¤§æ€§èƒ½",
                    "gemini-1.5-flash": "Gemini 1.5 Flash - å¿«é€ŸéŸ¿æ‡‰"
                }[x],
                help="é¸æ“‡ç”¨æ–¼åˆ†æçš„Google Geminiæ¨¡å‹ï¼ˆåŒ…å«2025å¹´æœ€æ–°çš„2.5å’Œ2.0ç³»åˆ—ï¼‰",
                key="google_model_select"
            )

            # æ›´æ–°session stateå’ŒæŒä¹…åŒ–å­˜å‚¨
            if st.session_state.llm_model != llm_model:
                logger.debug(f"ğŸ”„ [Persistence] Googleæ¨¡å‹è®Šæ›´: {st.session_state.llm_model} â†’ {llm_model}")
            st.session_state.llm_model = llm_model
            logger.debug(f"ğŸ’¾ [Persistence] Googleæ¨¡å‹å·²ä¿å­˜: {llm_model}")

            # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
            save_model_selection(st.session_state.llm_provider, st.session_state.model_category, llm_model)
        elif llm_provider == "openai":
             openai_options = [
                 "gpt-5",
                 "gpt-5-mini",
                 "gpt-5-nano",
                 "o1",
                 "o1-mini",
                 "o1-preview",
                 "gpt-4o",
                 "gpt-4o-mini",
                 "gpt-4-turbo",
                 "gpt-4",
                 "gpt-3.5-turbo"
             ]

             # ç²å–ç•¶å‰é¸æ“‡çš„ç´¢å¼•
             current_index = 6  # é»˜èªé¸æ“‡ gpt-4o
             if st.session_state.llm_model in openai_options:
                 current_index = openai_options.index(st.session_state.llm_model)

             llm_model = st.selectbox(
                 "é¸æ“‡OpenAIæ¨¡å‹",
                 options=openai_options,
                 index=current_index,
                 format_func=lambda x: {
                     "gpt-5": "ğŸš€ GPT-5 - 2025æœ€æ–°æ——è‰¦æ¨¡å‹",
                     "gpt-5-mini": "âš¡ GPT-5 Mini - è¼•é‡ç‰ˆGPT-5",
                     "gpt-5-nano": "ğŸ’¡ GPT-5 Nano - è¶…è¼•é‡ç‰ˆ",
                     "o1": "ğŸ§  o1 - æœ€æ–°æ¨ç†æ¨¡å‹",
                     "o1-mini": "ğŸ§  o1-mini - è¼•é‡æ¨ç†æ¨¡å‹",
                     "o1-preview": "ğŸ§ª o1-preview - æ¨ç†æ¨¡å‹é è¦½ç‰ˆ",
                     "gpt-4o": "GPT-4o - æ——è‰¦æ¨¡å‹",
                     "gpt-4o-mini": "GPT-4o Mini - è¼•é‡æ——è‰¦",
                     "gpt-4-turbo": "GPT-4 Turbo - å¼·åŒ–ç‰ˆ",
                     "gpt-4": "GPT-4 - ç¶“å…¸ç‰ˆ",
                     "gpt-3.5-turbo": "GPT-3.5 Turbo - ç¶“æ¿Ÿç‰ˆ"
                 }[x],
                 help="é¸æ“‡ç”¨æ–¼åˆ†æçš„OpenAIæ¨¡å‹ï¼ˆåŒ…å«2025å¹´8æœˆç™¼å¸ƒçš„GPT-5ç³»åˆ—ï¼‰",
                 key="openai_model_select"
             )

             # å¿«é€Ÿé¸æ“‡æŒ‰é’®
             st.markdown("**å¿«é€Ÿé¸æ“‡:**")
             
             col1, col2 = st.columns(2)
             with col1:
                 if st.button("ğŸš€ GPT-4o", key="quick_gpt4o", use_container_width=True):
                     model_id = "gpt-4o"
                     st.session_state.llm_model = model_id
                     save_model_selection(st.session_state.llm_provider, st.session_state.model_category, model_id)
                     logger.debug(f"ğŸ’¾ [Persistence] å¿«é€Ÿé¸æ“‡GPT-4o: {model_id}")
                     st.rerun()
             
             with col2:
                 if st.button("âš¡ GPT-4o Mini", key="quick_gpt4o_mini", use_container_width=True):
                     model_id = "gpt-4o-mini"
                     st.session_state.llm_model = model_id
                     save_model_selection(st.session_state.llm_provider, st.session_state.model_category, model_id)
                     logger.debug(f"ğŸ’¾ [Persistence] å¿«é€Ÿé¸æ“‡GPT-4o Mini: {model_id}")
                     st.rerun()

             # æ›´æ–°session stateå’ŒæŒä¹…åŒ–å­˜å‚¨
             if st.session_state.llm_model != llm_model:
                 logger.debug(f"ğŸ”„ [Persistence] OpenAIæ¨¡å‹è®Šæ›´: {st.session_state.llm_model} â†’ {llm_model}")
             st.session_state.llm_model = llm_model
             logger.debug(f"ğŸ’¾ [Persistence] OpenAIæ¨¡å‹å·²ä¿å­˜: {llm_model}")

             # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
             save_model_selection(st.session_state.llm_provider, st.session_state.model_category, llm_model)

             # OpenAIç‰¹æ®Šæç¤º
             st.info("ğŸ’¡ **OpenAIé…ç½®**: åœ¨.envæ–‡ä»¶ä¸­è¨­ç½®OPENAI_API_KEY")
        elif llm_provider == "custom_openai":
            st.markdown("### ğŸ”§ è‡ªå®šç¾©OpenAIç«¯é»é…ç½®")
            
            # åˆå§‹åŒ–session state
            if 'custom_openai_base_url' not in st.session_state:
                st.session_state.custom_openai_base_url = "https://api.openai.com/v1"
            if 'custom_openai_api_key' not in st.session_state:
                st.session_state.custom_openai_api_key = ""
            
            # APIç«¯é»URLé…ç½®
            base_url = st.text_input(
                "APIç«¯é»URL",
                value=st.session_state.custom_openai_base_url,
                placeholder="https://api.openai.com/v1",
                help="è¼¸å…¥OpenAIå…¼å®¹çš„APIç«¯é»URLï¼Œä¾‹å¦‚ä¸­è½‰æœå‹™æˆ–æœ¬åœ°éƒ¨ç½²çš„API",
                key="custom_openai_base_url_input"
            )
            
            # æ›´æ–°session state
            st.session_state.custom_openai_base_url = base_url
            
            # APIå¯†é‘°é…ç½®
            api_key = st.text_input(
                "APIå¯†é‘°",
                value=st.session_state.custom_openai_api_key,
                type="password",
                placeholder="sk-...",
                help="è¼¸å…¥APIå¯†é‘°ï¼Œä¹Ÿå¯ä»¥åœ¨.envæ–‡ä»¶ä¸­è¨­ç½®CUSTOM_OPENAI_API_KEY",
                key="custom_openai_api_key_input"
            )
            
            # æ›´æ–°session state
            st.session_state.custom_openai_api_key = api_key
            
            # æ¨¡å‹é¸æ“‡
            custom_openai_options = [
                "gpt-4o",
                "gpt-4o-mini", 
                "gpt-4-turbo",
                "gpt-4",
                "gpt-3.5-turbo",
                "claude-3.5-sonnet",
                "claude-3-opus",
                "claude-3-sonnet",
                "claude-3-haiku",
                "gemini-pro",
                "gemini-1.5-pro",
                "llama-3.1-8b",
                "llama-3.1-70b",
                "llama-3.1-405b",
                "custom-model"
            ]
            
            # ç²å–ç•¶å‰é¸æ“‡çš„ç´¢å¼•
            current_index = 0
            if st.session_state.llm_model in custom_openai_options:
                current_index = custom_openai_options.index(st.session_state.llm_model)
            
            llm_model = st.selectbox(
                "é¸æ“‡æ¨¡å‹",
                options=custom_openai_options,
                index=current_index,
                format_func=lambda x: {
                    "gpt-4o": "GPT-4o - OpenAIæœ€æ–°æ——èˆ°",
                    "gpt-4o-mini": "GPT-4o Mini - è½»é‡æ——èˆ°",
                    "gpt-4-turbo": "GPT-4 Turbo - å¼ºåŒ–ç‰ˆ",
                    "gpt-4": "GPT-4 - ç¶“å…¸ç‰ˆ",
                    "gpt-3.5-turbo": "GPT-3.5 Turbo - ç¶“æ¿Ÿç‰ˆ",
                    "claude-3.5-sonnet": "Claude 3.5 Sonnet - Anthropicæ——èˆ°",
                    "claude-3-opus": "Claude 3 Opus - å¼·å¤§æ€§èƒ½",
                    "claude-3-sonnet": "Claude 3 Sonnet - å¹³è¡¡ç‰ˆ",
                    "claude-3-haiku": "Claude 3 Haiku - å¿«é€Ÿç‰ˆ",
                    "gemini-pro": "Gemini Pro - Google AI",
                    "gemini-1.5-pro": "Gemini 1.5 Pro - å¢å¼ºç‰ˆ",
                    "llama-3.1-8b": "Llama 3.1 8B - Metaé–‹æº",
                    "llama-3.1-70b": "Llama 3.1 70B - å¤§å‹é–‹æº",
                    "llama-3.1-405b": "Llama 3.1 405B - è¶…å¤§é–‹æº",
                    "custom-model": "è‡ªå®šç¾©æ¨¡å‹åç¨±"
                }[x],
                help="é¸æ“‡è¦ä½¿ç”¨çš„æ¨¡å‹ï¼Œæ”¯æŒå„ç¨®OpenAIå…¼å®¹çš„æ¨¡å‹",
                key="custom_openai_model_select"
            )
            
            # å¦‚æœé¸æ“‡äº†è‡ªå®šç¾©æ¨¡å‹ï¼Œé¡¯ç¤ºè¼¸å…¥æ¡†
            if llm_model == "custom-model":
                custom_model_name = st.text_input(
                    "è‡ªå®šç¾©æ¨¡å‹åç¨±",
                    value="",
                    placeholder="ä¾‹å¦‚: gpt-4-custom, claude-3.5-sonnet-custom",
                    help="è¼¸å…¥è‡ªå®šç¾©çš„æ¨¡å‹åç¨±",
                    key="custom_model_name_input"
                )
                if custom_model_name:
                    llm_model = custom_model_name
            
            # æ›´æ–°session stateå’ŒæŒä¹…åŒ–å­˜å‚¨
            if st.session_state.llm_model != llm_model:
                logger.debug(f"ğŸ”„ [Persistence] è‡ªå®šç¾©OpenAIæ¨¡å‹è®Šæ›´: {st.session_state.llm_model} â†’ {llm_model}")
            st.session_state.llm_model = llm_model
            logger.debug(f"ğŸ’¾ [Persistence] è‡ªå®šç¾©OpenAIæ¨¡å‹å·²ä¿å­˜: {llm_model}")
            
            # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
            save_model_selection(st.session_state.llm_provider, st.session_state.model_category, llm_model)
            
            # å¸¸ç”¨ç«¯é»å¿«é€Ÿé…ç½®
            st.markdown("**ğŸš€ å¸¸ç”¨ç«¯é»å¿«é€Ÿé…ç½®:**")
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("ğŸŒ OpenAIå®˜æ–¹", key="quick_openai_official", use_container_width=True):
                    st.session_state.custom_openai_base_url = "https://api.openai.com/v1"
                    st.rerun()
                
                if st.button("ğŸ‡¨ğŸ‡³ OpenAIä¸­è½‰1", key="quick_openai_relay1", use_container_width=True):
                    st.session_state.custom_openai_base_url = "https://api.openai-proxy.com/v1"
                    st.rerun()
            
            with col2:
                if st.button("ğŸ  æœ¬åœ°éƒ¨ç½²", key="quick_local_deploy", use_container_width=True):
                    st.session_state.custom_openai_base_url = "http://localhost:8000/v1"
                    st.rerun()
                
                if st.button("ğŸ‡¨ğŸ‡³ OpenAIä¸­è½‰2", key="quick_openai_relay2", use_container_width=True):
                    st.session_state.custom_openai_base_url = "https://api.openai-sb.com/v1"
                    st.rerun()
            
            # é…ç½®é©—è­‰
            if base_url and api_key:
                st.success(f"âœ… é…ç½®å®Œæˆ")
                st.info(f"**ç«¯é»**: `{base_url}`")
                st.info(f"**æ¨¡å‹**: `{llm_model}`")
            elif base_url:
                st.warning("âš ï¸ è«‹è¼¸å…¥APIå¯†é‘°")
            else:
                st.warning("âš ï¸ è«‹é…ç½®APIç«¯é»URLå’Œå¯†é‘°")
            
            # é…ç½®èªªæ˜
            st.markdown("""
            **ğŸ“– é…ç½®èªªæ˜:**
            - **APIç«¯é»URL**: OpenAIå…¼å®¹çš„APIæœå‹™åœ°å€
            - **APIå¯†é‘°**: å°æ‡‰æœå‹™çš„APIå¯†é‘°
            - **æ¨¡å‹**: é¸æ“‡æˆ–è‡ªå®šç¾©æ¨¡å‹åç¨±
            
            **ğŸ”§ æ”¯æŒçš„æœå‹™é¡å‹:**
            - OpenAIå®˜æ–¹API
            - OpenAIä¸­è½‰æœå‹™
            - æœ¬åœ°éƒ¨ç½²çš„OpenAIå…¼å®¹æœå‹™
            - å…¶ä»–å…¼å®¹OpenAIæ ¼å¼çš„APIæœå‹™
            """)
        else:  # openrouter
            # OpenRouteræ¨¡å‹åˆ†é¡é¸æ“‡
            model_category = st.selectbox(
                "æ¨¡å‹é¡åˆ¥",
                options=["openai", "anthropic", "meta", "google", "custom"],
                index=["openai", "anthropic", "meta", "google", "custom"].index(st.session_state.model_category) if st.session_state.model_category in ["openai", "anthropic", "meta", "google", "custom"] else 0,
                format_func=lambda x: {
                    "openai": "ğŸ¤– OpenAI (GPTç³»åˆ—)",
                    "anthropic": "ğŸ§  Anthropic (Claudeç³»åˆ—)",
                    "meta": "ğŸ¦™ Meta (Llamaç³»åˆ—)",
                    "google": "ğŸŒŸ Google (Geminiç³»åˆ—)",
                    "custom": "âœï¸ è‡ªå®šç¾©æ¨¡å‹"
                }[x],
                help="é¸æ“‡æ¨¡å‹å» å•†é¡åˆ¥æˆ–è‡ªå®šç¾©è¼¸å…¥",
                key="model_category_select"
            )

            # æ›´æ–°session stateå’ŒæŒä¹…åŒ–å­˜å‚¨
            if st.session_state.model_category != model_category:
                logger.debug(f"ğŸ”„ [Persistence] æ¨¡å‹é¡åˆ¥è®Šæ›´: {st.session_state.model_category} â†’ {model_category}")
                st.session_state.llm_model = ""  # é¡åˆ¥è®Šæ›´æ™‚æ¸…ç©ºæ¨¡å‹é¸æ“‡
            st.session_state.model_category = model_category

            # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
            save_model_selection(st.session_state.llm_provider, model_category, st.session_state.llm_model)

            # æ ¹æ“šå» å•†é¡¯ç¤ºä¸åŒçš„æ¨¡å‹
            if model_category == "openai":
                openai_options = [
                    "openai/o4-mini-high",
                    "openai/o3-pro",
                    "openai/o3-mini-high",
                    "openai/o3-mini",
                    "openai/o1-pro",
                    "openai/o1-mini",
                    "openai/gpt-4o-2024-11-20",
                    "openai/gpt-4o-mini",
                    "openai/gpt-4-turbo",
                    "openai/gpt-3.5-turbo"
                ]

                # ç²å–ç•¶å‰é¸æ“‡çš„ç´¢å¼•
                current_index = 0
                if st.session_state.llm_model in openai_options:
                    current_index = openai_options.index(st.session_state.llm_model)

                llm_model = st.selectbox(
                    "é¸æ“‡OpenAIæ¨¡å‹",
                    options=openai_options,
                    index=current_index,
                    format_func=lambda x: {
                        "openai/o4-mini-high": "ğŸš€ o4 Mini High - æœ€æ–°o4ç³»åˆ—",
                        "openai/o3-pro": "ğŸš€ o3 Pro - æœ€æ–°æ¨ç†å°ˆæ¥­ç‰ˆ",
                        "openai/o3-mini-high": "o3 Mini High - é«˜æ€§èƒ½æ¨ç†",
                        "openai/o3-mini": "o3 Mini - æ¨ç†æ¨¡å‹",
                        "openai/o1-pro": "o1 Pro - å°ˆæ¥­æ¨ç†",
                        "openai/o1-mini": "o1 Mini - è½»é‡æ¨ç†",
                        "openai/gpt-4o-2024-11-20": "GPT-4o (2024-11-20) - æœ€æ–°ç‰ˆ",
                        "openai/gpt-4o-mini": "GPT-4o Mini - è½»é‡æ——èˆ°",
                        "openai/gpt-4-turbo": "GPT-4 Turbo - ç¶“å…¸å¼ºåŒ–",
                        "openai/gpt-3.5-turbo": "GPT-3.5 Turbo - ç¶“æ¿Ÿå¯¦ç”¨"
                    }[x],
                    help="OpenAIå…¬å¸çš„GPTå’Œoç³»åˆ—æ¨¡å‹ï¼ŒåŒ…å«æœ€æ–°o4",
                    key="openai_model_select"
                )

                # æ›´æ–°session stateå’ŒæŒä¹…åŒ–å­˜å‚¨
                if st.session_state.llm_model != llm_model:
                    logger.debug(f"ğŸ”„ [Persistence] OpenAIæ¨¡å‹è®Šæ›´: {st.session_state.llm_model} â†’ {llm_model}")
                st.session_state.llm_model = llm_model
                logger.debug(f"ğŸ’¾ [Persistence] OpenAIæ¨¡å‹å·²ä¿å­˜: {llm_model}")

                # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
                save_model_selection(st.session_state.llm_provider, st.session_state.model_category, llm_model)
            elif model_category == "anthropic":
                anthropic_options = [
                    "anthropic/claude-opus-4.1",
                    "anthropic/claude-sonnet-4.5",
                    "anthropic/claude-haiku-4.5",
                    "anthropic/claude-opus-4",
                    "anthropic/claude-sonnet-4",
                    "anthropic/claude-haiku-4",
                    "anthropic/claude-3.5-sonnet",
                    "anthropic/claude-3.5-haiku",
                    "anthropic/claude-3.5-sonnet-20241022",
                    "anthropic/claude-3.5-haiku-20241022",
                    "anthropic/claude-3-opus",
                    "anthropic/claude-3-sonnet",
                    "anthropic/claude-3-haiku"
                ]

                # ç²å–ç•¶å‰é¸æ“‡çš„ç´¢å¼•
                current_index = 0
                if st.session_state.llm_model in anthropic_options:
                    current_index = anthropic_options.index(st.session_state.llm_model)

                llm_model = st.selectbox(
                    "é¸æ“‡Anthropicæ¨¡å‹",
                    options=anthropic_options,
                    index=current_index,
                    format_func=lambda x: {
                        "anthropic/claude-opus-4.1": "ğŸš€ Claude Opus 4.1 - æœ€å¼·æ¨¡å‹ï¼ˆ2025-08ï¼‰",
                        "anthropic/claude-sonnet-4.5": "ğŸ’» Claude Sonnet 4.5 - ä¸–ç•Œæœ€å¼·ç·¨ç¢¼æ¨¡å‹ï¼ˆ2025-09ï¼‰",
                        "anthropic/claude-haiku-4.5": "âš¡ Claude Haiku 4.5 - é«˜æ€§åƒ¹æ¯”ï¼ˆ2025-10ï¼‰",
                        "anthropic/claude-opus-4": "ğŸš€ Claude Opus 4 - é ‚ç´šæ¨¡å‹",
                        "anthropic/claude-sonnet-4": "ğŸš€ Claude Sonnet 4 - å¹³è¡¡æ¨¡å‹",
                        "anthropic/claude-haiku-4": "ğŸš€ Claude Haiku 4 - å¿«é€Ÿæ¨¡å‹",
                        "anthropic/claude-3.5-sonnet": "Claude 3.5 Sonnet - ç•¶å‰æ——è‰¦",
                        "anthropic/claude-3.5-haiku": "Claude 3.5 Haiku - å¿«é€ŸéŸ¿æ‡‰",
                        "anthropic/claude-3.5-sonnet-20241022": "Claude 3.5 Sonnet (2024-10-22)",
                        "anthropic/claude-3.5-haiku-20241022": "Claude 3.5 Haiku (2024-10-22)",
                        "anthropic/claude-3-opus": "Claude 3 Opus - å¼·å¤§æ€§èƒ½",
                        "anthropic/claude-3-sonnet": "Claude 3 Sonnet - å¹³è¡¡ç‰ˆ",
                        "anthropic/claude-3-haiku": "Claude 3 Haiku - ç¶“æ¿Ÿç‰ˆ"
                    }[x],
                    help="Anthropicå…¬å¸çš„Claudeç³»åˆ—æ¨¡å‹ï¼ŒåŒ…å«2025å¹´æœ€æ–°Claude 4.5ç³»åˆ—",
                    key="anthropic_model_select"
                )

                # æ›´æ–°session stateå’ŒæŒä¹…åŒ–å­˜å‚¨
                if st.session_state.llm_model != llm_model:
                    logger.debug(f"ğŸ”„ [Persistence] Anthropicæ¨¡å‹è®Šæ›´: {st.session_state.llm_model} â†’ {llm_model}")
                st.session_state.llm_model = llm_model
                logger.debug(f"ğŸ’¾ [Persistence] Anthropicæ¨¡å‹å·²ä¿å­˜: {llm_model}")

                # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
                save_model_selection(st.session_state.llm_provider, st.session_state.model_category, llm_model)
            elif model_category == "meta":
                meta_options = [
                    "meta-llama/llama-4-maverick",
                    "meta-llama/llama-4-scout",
                    "meta-llama/llama-3.3-70b-instruct",
                    "meta-llama/llama-3.2-90b-vision-instruct",
                    "meta-llama/llama-3.1-405b-instruct",
                    "meta-llama/llama-3.1-70b-instruct",
                    "meta-llama/llama-3.2-11b-vision-instruct",
                    "meta-llama/llama-3.1-8b-instruct",
                    "meta-llama/llama-3.2-3b-instruct",
                    "meta-llama/llama-3.2-1b-instruct"
                ]

                # ç²å–ç•¶å‰é¸æ“‡çš„ç´¢å¼•
                current_index = 0
                if st.session_state.llm_model in meta_options:
                    current_index = meta_options.index(st.session_state.llm_model)

                llm_model = st.selectbox(
                    "é¸æ“‡Metaæ¨¡å‹",
                    options=meta_options,
                    index=current_index,
                    format_func=lambda x: {
                        "meta-llama/llama-4-maverick": "ğŸš€ Llama 4 Maverick - æœ€æ–°æ——èˆ°",
                        "meta-llama/llama-4-scout": "ğŸš€ Llama 4 Scout - æœ€æ–°é è¦½",
                        "meta-llama/llama-3.3-70b-instruct": "Llama 3.3 70B - å¼·å¤§æ€§èƒ½",
                        "meta-llama/llama-3.2-90b-vision-instruct": "Llama 3.2 90B Vision - å¤šæ¨¡æ…‹",
                        "meta-llama/llama-3.1-405b-instruct": "Llama 3.1 405B - è¶…å¤§æ¨¡å‹",
                        "meta-llama/llama-3.1-70b-instruct": "Llama 3.1 70B - å¹³è¡¡æ€§èƒ½",
                        "meta-llama/llama-3.2-11b-vision-instruct": "Llama 3.2 11B Vision - è½»é‡å¤šæ¨¡æ…‹",
                        "meta-llama/llama-3.1-8b-instruct": "Llama 3.1 8B - é«˜æ•ˆæ¨¡å‹",
                        "meta-llama/llama-3.2-3b-instruct": "Llama 3.2 3B - è½»é‡ç´š",
                        "meta-llama/llama-3.2-1b-instruct": "Llama 3.2 1B - è¶…è½»é‡"
                    }[x],
                    help="Metaå…¬å¸çš„Llamaç³»åˆ—æ¨¡å‹ï¼ŒåŒ…å«æœ€æ–°Llama 4",
                    key="meta_model_select"
                )

                # æ›´æ–°session stateå’ŒæŒä¹…åŒ–å­˜å‚¨
                if st.session_state.llm_model != llm_model:
                    logger.debug(f"ğŸ”„ [Persistence] Metaæ¨¡å‹è®Šæ›´: {st.session_state.llm_model} â†’ {llm_model}")
                st.session_state.llm_model = llm_model
                logger.debug(f"ğŸ’¾ [Persistence] Metaæ¨¡å‹å·²ä¿å­˜: {llm_model}")

                # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
                save_model_selection(st.session_state.llm_provider, st.session_state.model_category, llm_model)
            elif model_category == "google":
                google_openrouter_options = [
                    "google/gemini-2.5-pro",
                    "google/gemini-2.5-flash",
                    "google/gemini-2.5-flash-lite",
                    "google/gemini-2.5-pro-002",
                    "google/gemini-2.5-flash-002",
                    "google/gemini-2.0-flash-001",
                    "google/gemini-2.0-flash-lite-001",
                    "google/gemini-1.5-pro",
                    "google/gemini-1.5-flash",
                    "google/gemma-3-27b-it",
                    "google/gemma-3-12b-it",
                    "google/gemma-2-27b-it"
                ]

                # ç²å–ç•¶å‰é¸æ“‡çš„ç´¢å¼•
                current_index = 0
                if st.session_state.llm_model in google_openrouter_options:
                    current_index = google_openrouter_options.index(st.session_state.llm_model)

                llm_model = st.selectbox(
                    "é¸æ“‡Googleæ¨¡å‹",
                    options=google_openrouter_options,
                    index=current_index,
                    format_func=lambda x: {
                        "google/gemini-2.5-pro": "ğŸš€ Gemini 2.5 Pro - æœ€æ–°æ——èˆ°",
                        "google/gemini-2.5-flash": "âš¡ Gemini 2.5 Flash - æœ€æ–°å¿«é€Ÿ",
                        "google/gemini-2.5-flash-lite": "ğŸ’¡ Gemini 2.5 Flash Lite - è½»é‡ç‰ˆ",
                        "google/gemini-2.5-pro-002": "ğŸ”§ Gemini 2.5 Pro-002 - å„ªåŒ–ç‰ˆ",
                        "google/gemini-2.5-flash-002": "âš¡ Gemini 2.5 Flash-002 - å„ªåŒ–å¿«é€Ÿç‰ˆ",
                        "google/gemini-2.0-flash-001": "Gemini 2.0 Flash - ç©©å®šç‰ˆ",
                        "google/gemini-2.0-flash-lite-001": "Gemini 2.0 Flash Lite",
                        "google/gemini-1.5-pro": "Gemini 1.5 Pro - å°ˆæ¥­ç‰ˆ",
                        "google/gemini-1.5-flash": "Gemini 1.5 Flash - å¿«é€Ÿç‰ˆ",
                        "google/gemma-3-27b-it": "Gemma 3 27B - æœ€æ–°é–‹æºå¤§æ¨¡å‹",
                        "google/gemma-3-12b-it": "Gemma 3 12B - é–‹æºä¸­å‹æ¨¡å‹",
                        "google/gemma-2-27b-it": "Gemma 2 27B - é–‹æºç¶“å…¸ç‰ˆ"
                    }[x],
                    help="Googleå…¬å¸çš„Gemini/Gemmaç³»åˆ—æ¨¡å‹ï¼ŒåŒ…å«æœ€æ–°Gemini 2.5",
                    key="google_openrouter_model_select"
                )

                # æ›´æ–°session stateå’ŒæŒä¹…åŒ–å­˜å‚¨
                if st.session_state.llm_model != llm_model:
                    logger.debug(f"ğŸ”„ [Persistence] Google OpenRouteræ¨¡å‹è®Šæ›´: {st.session_state.llm_model} â†’ {llm_model}")
                st.session_state.llm_model = llm_model
                logger.debug(f"ğŸ’¾ [Persistence] Google OpenRouteræ¨¡å‹å·²ä¿å­˜: {llm_model}")

                # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
                save_model_selection(st.session_state.llm_provider, st.session_state.model_category, llm_model)

            else:  # custom
                st.markdown("### âœï¸ è‡ªå®šç¾©æ¨¡å‹")

                # åˆå§‹åŒ–è‡ªå®šç¾©æ¨¡å‹session state
                if 'custom_model' not in st.session_state:
                    st.session_state.custom_model = ""

                # è‡ªå®šç¾©æ¨¡å‹è¼¸å…¥ - ä½¿ç”¨session stateä½œç‚ºé»˜èªå€¼
                default_value = st.session_state.custom_model if st.session_state.custom_model else "anthropic/claude-3.7-sonnet"

                llm_model = st.text_input(
                    "è¼¸å…¥æ¨¡å‹ID",
                    value=default_value,
                    placeholder="ä¾‹å¦‚: anthropic/claude-3.7-sonnet",
                    help="è¼¸å…¥OpenRouteræ”¯æŒçš„ä»»ä½•æ¨¡å‹ID",
                    key="custom_model_input"
                )

                # å¸¸ç”¨æ¨¡å‹å¿«é€Ÿé¸æ“‡
                st.markdown("**å¿«é€Ÿé¸æ“‡å¸¸ç”¨æ¨¡å‹:**")

                # é•·æ¢å½¢æŒ‰é’®ï¼Œæ¯å€‹å ä¸€è¡Œ
                if st.button("ğŸ§  Claude 3.7 Sonnet - æœ€æ–°å°è©±æ¨¡å‹", key="claude37", use_container_width=True):
                    model_id = "anthropic/claude-3.7-sonnet"
                    st.session_state.custom_model = model_id
                    st.session_state.llm_model = model_id
                    save_model_selection(st.session_state.llm_provider, st.session_state.model_category, model_id)
                    logger.debug(f"ğŸ’¾ [Persistence] å¿«é€Ÿé¸æ“‡Claude 3.7 Sonnet: {model_id}")
                    st.rerun()

                if st.button("ğŸ’ Claude 4 Opus - é¡¶ç´šæ€§èƒ½æ¨¡å‹", key="claude4opus", use_container_width=True):
                    model_id = "anthropic/claude-opus-4"
                    st.session_state.custom_model = model_id
                    st.session_state.llm_model = model_id
                    save_model_selection(st.session_state.llm_provider, st.session_state.model_category, model_id)
                    logger.debug(f"ğŸ’¾ [Persistence] å¿«é€Ÿé¸æ“‡Claude 4 Opus: {model_id}")
                    st.rerun()

                if st.button("ğŸ¤– GPT-4o - OpenAIæ——èˆ°æ¨¡å‹", key="gpt4o", use_container_width=True):
                    model_id = "openai/gpt-4o"
                    st.session_state.custom_model = model_id
                    st.session_state.llm_model = model_id
                    save_model_selection(st.session_state.llm_provider, st.session_state.model_category, model_id)
                    logger.debug(f"ğŸ’¾ [Persistence] å¿«é€Ÿé¸æ“‡GPT-4o: {model_id}")
                    st.rerun()

                if st.button("ğŸ¦™ Llama 4 Scout - Metaæœ€æ–°æ¨¡å‹", key="llama4", use_container_width=True):
                    model_id = "meta-llama/llama-4-scout"
                    st.session_state.custom_model = model_id
                    st.session_state.llm_model = model_id
                    save_model_selection(st.session_state.llm_provider, st.session_state.model_category, model_id)
                    logger.debug(f"ğŸ’¾ [Persistence] å¿«é€Ÿé¸æ“‡Llama 4 Scout: {model_id}")
                    st.rerun()

                if st.button("ğŸŒŸ Gemini 2.5 Pro - Googleå¤šæ¨¡æ…‹", key="gemini25", use_container_width=True):
                    model_id = "google/gemini-2.5-pro"
                    st.session_state.custom_model = model_id
                    st.session_state.llm_model = model_id
                    save_model_selection(st.session_state.llm_provider, st.session_state.model_category, model_id)
                    logger.debug(f"ğŸ’¾ [Persistence] å¿«é€Ÿé¸æ“‡Gemini 2.5 Pro: {model_id}")
                    st.rerun()

                # æ›´æ–°session stateå’ŒæŒä¹…åŒ–å­˜å‚¨
                if st.session_state.llm_model != llm_model:
                    logger.debug(f"ğŸ”„ [Persistence] è‡ªå®šç¾©æ¨¡å‹è®Šæ›´: {st.session_state.llm_model} â†’ {llm_model}")
                st.session_state.custom_model = llm_model
                st.session_state.llm_model = llm_model
                logger.debug(f"ğŸ’¾ [Persistence] è‡ªå®šç¾©æ¨¡å‹å·²ä¿å­˜: {llm_model}")

                # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
                save_model_selection(st.session_state.llm_provider, st.session_state.model_category, llm_model)

                # æ¨¡å‹é©—è­‰æç¤º
                if llm_model:
                    st.success(f"âœ… ç•¶å‰æ¨¡å‹: `{llm_model}`")

                    # æä¾›æ¨¡å‹æŸ¥æ‰¾éˆæ¥
                    st.markdown("""
                    **ğŸ“š æŸ¥æ‰¾æ›´å¤šæ¨¡å‹:**
                    - [OpenRouteræ¨¡å‹åˆ—è¡¨](https://openrouter.ai/models)
                    - [Anthropicæ¨¡å‹æ–‡æª”](https://docs.anthropic.com/claude/docs/models-overview)
                    - [OpenAIæ¨¡å‹æ–‡æª”](https://platform.openai.com/docs/models)
                    """)
                else:
                    st.warning("âš ï¸ è«‹è¼¸å…¥æœ‰æ•ˆçš„æ¨¡å‹ID")

            # OpenRouterç‰¹æ®Šæç¤º
            st.info("ğŸ’¡ **OpenRouteré…ç½®**: åœ¨.envæ–‡ä»¶ä¸­è¨­ç½®OPENROUTER_API_KEYï¼Œæˆ–è€…å¦‚æœåªç”¨OpenRouterå¯ä»¥è¨­ç½®OPENAI_API_KEY")
        
        # é«˜ç´šè¨­ç½®
        with st.expander("âš™ï¸ é«˜ç´šè¨­ç½®"):
            enable_memory = st.checkbox(
                "å•Ÿç”¨è¨˜å¿†åŠŸèƒ½",
                value=False,
                help="å•Ÿç”¨æ™ºèƒ½é«”è¨˜å¿†åŠŸèƒ½ï¼ˆå¯èƒ½å½±éŸ¿æ€§èƒ½ï¼‰"
            )
            
            enable_debug = st.checkbox(
                "èª¿è©¦æ¨¡å¼",
                value=False,
                help="å•Ÿç”¨è©³ç´°çš„èª¿è©¦ä¿¡æ¯è¼¸å‡º"
            )
            
            max_tokens = st.slider(
                "æœ€å¤§è¼¸å‡ºé•·åº¦",
                min_value=1000,
                max_value=8000,
                value=4000,
                step=500,
                help="AIæ¨¡å‹çš„æœ€å¤§è¼¸å‡ºtokenæ•¸é‡"
            )
        
        st.markdown("---")

        # ç³»çµ±é…ç½®
        st.markdown("**ğŸ”§ ç³»çµ±é…ç½®**")

        # APIå¯†é‘°ç‹€æ…‹
        st.markdown("**ğŸ”‘ APIå¯†é‘°ç‹€æ…‹**")

        def validate_api_key(key, expected_format):
            """é©—è­‰APIå¯†é‘°æ ¼å¼"""
            if not key:
                return "æœªé…ç½®", "error"

            if expected_format == "dashscope" and key.startswith("sk-") and len(key) >= 32:
                return f"{key[:8]}...", "success"
            elif expected_format == "deepseek" and key.startswith("sk-") and len(key) >= 32:
                return f"{key[:8]}...", "success"
            elif expected_format == "finnhub" and len(key) >= 20:
                return f"{key[:8]}...", "success"
            elif expected_format == "tushare" and len(key) >= 32:
                return f"{key[:8]}...", "success"
            elif expected_format == "google" and key.startswith("AIza") and len(key) >= 32:
                return f"{key[:8]}...", "success"
            elif expected_format == "openai" and key.startswith("sk-") and len(key) >= 40:
                return f"{key[:8]}...", "success"
            elif expected_format == "anthropic" and key.startswith("sk-") and len(key) >= 40:
                return f"{key[:8]}...", "success"
            elif expected_format == "reddit" and len(key) >= 10:
                return f"{key[:8]}...", "success"
            else:
                return f"{key[:8]}... (æ ¼å¼ç•°å¸¸)", "warning"

        # å¿…éœ€çš„APIå¯†é‘°
        st.markdown("*å¿…éœ€é…ç½®:*")

        # FinnHub
        finnhub_key = os.getenv("FINNHUB_API_KEY")
        status, level = validate_api_key(finnhub_key, "finnhub")
        if level == "success":
            st.success(f"âœ… FinnHub: {status}")
        elif level == "warning":
            st.warning(f"âš ï¸ FinnHub: {status}")
        else:
            st.error("âŒ FinnHub: æœªé…ç½®")

        # å¯é¸çš„APIå¯†é‘°
        st.markdown("*å¯é¸é…ç½®:*")

        # Google AI
        google_key = os.getenv("GOOGLE_API_KEY")
        status, level = validate_api_key(google_key, "google")
        if level == "success":
            st.success(f"âœ… Google AI: {status}")
        elif level == "warning":
            st.warning(f"âš ï¸ Google AI: {status}")
        else:
            st.info("â„¹ï¸ Google AI: æœªé…ç½®")

        # OpenAI (å¦‚æœé…ç½®äº†ä¸”ä¸æ˜¯é»˜èªå€¼)
        openai_key = os.getenv("OPENAI_API_KEY")
        if openai_key and openai_key != "your_openai_api_key_here":
            status, level = validate_api_key(openai_key, "openai")
            if level == "success":
                st.success(f"âœ… OpenAI: {status}")
            elif level == "warning":
                st.warning(f"âš ï¸ OpenAI: {status}")

        # Anthropic (å¦‚æœé…ç½®äº†ä¸”ä¸æ˜¯é»˜èªå€¼)
        anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        if anthropic_key and anthropic_key != "your_anthropic_api_key_here":
            status, level = validate_api_key(anthropic_key, "anthropic")
            if level == "success":
                st.success(f"âœ… Anthropic: {status}")
            elif level == "warning":
                st.warning(f"âš ï¸ Anthropic: {status}")

        st.markdown("---")

        # ç³»çµ±ä¿¡æ¯
        st.markdown("**â„¹ï¸ ç³»çµ±ä¿¡æ¯**")
        
        st.info(f"""
        **ç‰ˆæœ¬**: {get_version()}
        **æ¡†æ¶**: Streamlit + LangGraph
        **AIæ¨¡å‹**: {st.session_state.llm_provider.upper()} - {st.session_state.llm_model}
        **æ•¸æ“šæº**: Tushare + FinnHub API
        """)
        
        # ç®¡ç†å“¡åŠŸèƒ½
        if auth_manager and auth_manager.check_permission("admin"):
            st.markdown("---")
            st.markdown("### ğŸ”§ ç®¡ç†åŠŸèƒ½")
            
            if st.button("ğŸ“Š ç”¨æˆ¶æ´»å‹•è¨˜éŒ„", key="user_activity_btn", use_container_width=True):
                st.session_state.page = "user_activity"
            
            if st.button("âš™ï¸ ç³»çµ±è¨­ç½®", key="system_settings_btn", use_container_width=True):
                st.session_state.page = "system_settings"
        
        # å¹«åŠ©éˆæ¥
        st.markdown("**ğŸ“š å¹«åŠ©è³‡æº**")
        
        st.markdown("""
        - [ğŸ“– ä½¿ç”¨æ–‡æª”](https://github.com/TauricResearch/TradingAgents)
        - [ğŸ› å•é¡Œåé¥‹](https://github.com/TauricResearch/TradingAgents/issues)
        - [ğŸ’¬ è¨è«–ç¤¾å€](https://github.com/TauricResearch/TradingAgents/discussions)
        - [ğŸ”§ APIå¯†é‘°é…ç½®](../docs/security/api_keys_security.md)
        """)
    
    # ç¢ºä¿è¿”å›session stateä¸­çš„å€¼ï¼Œè€Œä¸æ˜¯å±€éƒ¨è®Šé‡
    final_provider = st.session_state.llm_provider
    final_model = st.session_state.llm_model

    logger.debug(f"ğŸ”„ [Session State] è¿”å›é…ç½® - provider: {final_provider}, model: {final_model}")

    return {
        'llm_provider': final_provider,
        'llm_model': final_model,
        'enable_memory': enable_memory,
        'enable_debug': enable_debug,
        'max_tokens': max_tokens
    }
