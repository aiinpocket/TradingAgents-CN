import chromadb
from chromadb.config import Settings
from openai import OpenAI
import os
import threading
import hashlib
from typing import Dict, Optional

# å°å…¥çµ±ä¸€æ—¥èªŒç³»çµ±
from tradingagents.utils.logging_init import get_logger
logger = get_logger("agents.utils.memory")


class ChromaDBManager:
    """å–®ä¾‹ChromaDBç®¡ç†å™¨ï¼Œé¿å…ä¸¦ç™¼å‰µå»ºé›†åˆçš„å†²çª"""

    _instance = None
    _lock = threading.Lock()
    _collections: Dict[str, any] = {}
    _client = None

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(ChromaDBManager, cls).__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if not self._initialized:
            try:
                # è‡ªå‹•æª¢æ¸¬æ“ä½œç³»çµ±ç‰ˆæœ¬ä¸¦ä½¿ç”¨æœ€å„ªé…ç½®
                import platform
                system = platform.system()
                
                if system == "Windows":
                    # ä½¿ç”¨æ”¹é€²çš„Windows 11æª¢æ¸¬
                    from .chromadb_win11_config import is_windows_11
                    if is_windows_11():
                        # Windows 11 æˆ–æ›´æ–°ç‰ˆæœ¬ï¼Œä½¿ç”¨å„ªåŒ–é…ç½®
                        from .chromadb_win11_config import get_win11_chromadb_client
                        self._client = get_win11_chromadb_client()
                        logger.info(f"ğŸ“š [ChromaDB] Windows 11å„ªåŒ–é…ç½®åˆå§‹åŒ–å®Œæˆ (æ§‹å»ºè™Ÿ: {platform.version()})")
                    else:
                        # Windows 10 æˆ–æ›´è€ç‰ˆæœ¬ï¼Œä½¿ç”¨å…¼å®¹é…ç½®
                        from .chromadb_win10_config import get_win10_chromadb_client
                        self._client = get_win10_chromadb_client()
                        logger.info(f"ğŸ“š [ChromaDB] Windows 10å…¼å®¹é…ç½®åˆå§‹åŒ–å®Œæˆ")
                else:
                    # éWindowsç³»çµ±ï¼Œä½¿ç”¨æ¨™æº–é…ç½®
                    settings = Settings(
                        allow_reset=True,
                        anonymized_telemetry=False,
                        is_persistent=False
                    )
                    self._client = chromadb.Client(settings)
                    logger.info(f"ğŸ“š [ChromaDB] {system}æ¨™æº–é…ç½®åˆå§‹åŒ–å®Œæˆ")
                
                self._initialized = True
            except Exception as e:
                logger.error(f"âŒ [ChromaDB] åˆå§‹åŒ–å¤±è´¥: {e}")
                # ä½¿ç”¨æœ€ç°¡å–®çš„é…ç½®ä½œç‚ºå¤‡ç”¨
                try:
                    settings = Settings(
                        allow_reset=True,
                        anonymized_telemetry=False,  # é—œé”®ï¼šç¦ç”¨é¥æ¸¬
                        is_persistent=False
                    )
                    self._client = chromadb.Client(settings)
                    logger.info(f"ğŸ“š [ChromaDB] ä½¿ç”¨å¤‡ç”¨é…ç½®åˆå§‹åŒ–å®Œæˆ")
                except Exception as backup_error:
                    # æœ€å¾Œçš„å¤‡ç”¨æ–¹æ¡ˆ
                    self._client = chromadb.Client()
                    logger.warning(f"âš ï¸ [ChromaDB] ä½¿ç”¨æœ€ç°¡é…ç½®åˆå§‹åŒ–: {backup_error}")
                self._initialized = True

    def get_or_create_collection(self, name: str):
        """ç·šç¨‹å®‰å…¨åœ°ç²å–æˆ–å‰µå»ºé›†åˆ"""
        with self._lock:
            if name in self._collections:
                logger.info(f"ğŸ“š [ChromaDB] ä½¿ç”¨ç·©å­˜é›†åˆ: {name}")
                return self._collections[name]

            try:
                # å˜—è©¦ç²å–ç¾æœ‰é›†åˆ
                collection = self._client.get_collection(name=name)
                logger.info(f"ğŸ“š [ChromaDB] ç²å–ç¾æœ‰é›†åˆ: {name}")
            except Exception:
                try:
                    # å‰µå»ºæ–°é›†åˆ
                    collection = self._client.create_collection(name=name)
                    logger.info(f"ğŸ“š [ChromaDB] å‰µå»ºæ–°é›†åˆ: {name}")
                except Exception as e:
                    # å¯èƒ½æ˜¯ä¸¦ç™¼å‰µå»ºï¼Œå†æ¬¡å˜—è©¦ç²å–
                    try:
                        collection = self._client.get_collection(name=name)
                        logger.info(f"ğŸ“š [ChromaDB] ä¸¦ç™¼å‰µå»ºå¾Œç²å–é›†åˆ: {name}")
                    except Exception as final_error:
                        logger.error(f"âŒ [ChromaDB] é›†åˆæ“ä½œå¤±è´¥: {name}, éŒ¯èª¤: {final_error}")
                        raise final_error

            # ç·©å­˜é›†åˆ
            self._collections[name] = collection
            return collection


class FinancialSituationMemory:
    def __init__(self, name, config):
        self.config = config
        self.llm_provider = config.get("llm_provider", "openai").lower()

        # é…ç½®å‘é‡ç·©å­˜çš„é•·åº¦é™åˆ¶ï¼ˆå‘é‡ç·©å­˜é»˜èªå•Ÿç”¨é•·åº¦æª¢æŸ¥ï¼‰
        self.max_embedding_length = int(os.getenv('MAX_EMBEDDING_CONTENT_LENGTH', '50000'))  # é»˜èª50Kå­—ç¬¦
        self.enable_embedding_length_check = os.getenv('ENABLE_EMBEDDING_LENGTH_CHECK', 'true').lower() == 'true'  # å‘é‡ç·©å­˜é»˜èªå•Ÿç”¨
        
        # æ ¹æ“šLLMæä¾›å•†é¸æ“‡åµŒå…¥æ¨¡å‹å’Œå®¢æˆ¶ç«¯
        # åˆå§‹åŒ–é™ç´šé¸é …æ¨™èªŒ
        self.fallback_available = False
        
        if self.llm_provider == "dashscope" or self.llm_provider == "alibaba":
            self.embedding = "text-embedding-v3"
            self.client = None  # DashScopeä¸éœ€è¦OpenAIå®¢æˆ¶ç«¯

            # è¨­ç½®DashScope APIå¯†é‘°
            dashscope_key = os.getenv('DASHSCOPE_API_KEY')
            if dashscope_key:
                try:
                    # å˜—è©¦å°å…¥å’Œåˆå§‹åŒ–DashScope
                    import dashscope
                    from dashscope import TextEmbedding

                    dashscope.api_key = dashscope_key
                    logger.info(f"âœ… DashScope APIå¯†é‘°å·²é…ç½®ï¼Œå•Ÿç”¨è¨˜å¿†åŠŸèƒ½")

                    # å¯é¸ï¼šæ¸¬è©¦APIé€£æ¥ï¼ˆç°¡å–®é©—è­‰ï¼‰
                    # é€™é‡Œä¸åšå¯¦é™…èª¿ç”¨ï¼Œåªé©—è­‰å°å…¥å’Œå¯†é‘°è¨­ç½®

                except ImportError as e:
                    # DashScopeåŒ…æœªå®‰è£
                    logger.error(f"âŒ DashScopeåŒ…æœªå®‰è£: {e}")
                    self.client = "DISABLED"
                    logger.warning(f"âš ï¸ è¨˜å¿†åŠŸèƒ½å·²ç¦ç”¨")

                except Exception as e:
                    # å…¶ä»–åˆå§‹åŒ–éŒ¯èª¤
                    logger.error(f"âŒ DashScopeåˆå§‹åŒ–å¤±è´¥: {e}")
                    self.client = "DISABLED"
                    logger.warning(f"âš ï¸ è¨˜å¿†åŠŸèƒ½å·²ç¦ç”¨")
            else:
                # æ²¡æœ‰DashScopeå¯†é‘°ï¼Œç¦ç”¨è¨˜å¿†åŠŸèƒ½
                self.client = "DISABLED"
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°DASHSCOPE_API_KEYï¼Œè¨˜å¿†åŠŸèƒ½å·²ç¦ç”¨")
                logger.info(f"ğŸ’¡ ç³»çµ±å°†ç¹¼ç»­é‹è¡Œï¼Œä½†ä¸æœƒä¿å­˜æˆ–æª¢ç´¢æ­·å²è¨˜å¿†")
        elif self.llm_provider == "qianfan":
            # åƒå¸†ï¼ˆæ–‡å¿ƒä¸€è¨€ï¼‰embeddingé…ç½®
            # åƒå¸†ç›®å‰æ²¡æœ‰ç¨ç«‹çš„embedding APIï¼Œä½¿ç”¨é˜¿é‡Œç™¾ç‚¼ä½œç‚ºé™ç´šé¸é …
            dashscope_key = os.getenv('DASHSCOPE_API_KEY')
            if dashscope_key:
                try:
                    # ä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥æœå‹™ä½œç‚ºåƒå¸†çš„embeddingè§£æ±ºæ–¹æ¡ˆ
                    import dashscope
                    from dashscope import TextEmbedding

                    dashscope.api_key = dashscope_key
                    self.embedding = "text-embedding-v3"
                    self.client = None
                    logger.info(f"ğŸ’¡ åƒå¸†ä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥æœå‹™")
                except ImportError as e:
                    logger.error(f"âŒ DashScopeåŒ…æœªå®‰è£: {e}")
                    self.client = "DISABLED"
                    logger.warning(f"âš ï¸ åƒå¸†è¨˜å¿†åŠŸèƒ½å·²ç¦ç”¨")
                except Exception as e:
                    logger.error(f"âŒ åƒå¸†åµŒå…¥åˆå§‹åŒ–å¤±è´¥: {e}")
                    self.client = "DISABLED"
                    logger.warning(f"âš ï¸ åƒå¸†è¨˜å¿†åŠŸèƒ½å·²ç¦ç”¨")
            else:
                # æ²¡æœ‰DashScopeå¯†é‘°ï¼Œç¦ç”¨è¨˜å¿†åŠŸèƒ½
                self.client = "DISABLED"
                logger.warning(f"âš ï¸ åƒå¸†æœªæ‰¾åˆ°DASHSCOPE_API_KEYï¼Œè¨˜å¿†åŠŸèƒ½å·²ç¦ç”¨")
                logger.info(f"ğŸ’¡ ç³»çµ±å°†ç¹¼ç»­é‹è¡Œï¼Œä½†ä¸æœƒä¿å­˜æˆ–æª¢ç´¢æ­·å²è¨˜å¿†")
        elif self.llm_provider == "deepseek":
            # æª¢æŸ¥æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨OpenAIåµŒå…¥
            force_openai = os.getenv('FORCE_OPENAI_EMBEDDING', 'false').lower() == 'true'

            if not force_openai:
                # å˜—è©¦ä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥
                dashscope_key = os.getenv('DASHSCOPE_API_KEY')
                if dashscope_key:
                    try:
                        # æ¸¬è©¦é˜¿é‡Œç™¾ç‚¼æ˜¯å¦å¯ç”¨
                        import dashscope
                        from dashscope import TextEmbedding

                        dashscope.api_key = dashscope_key
                        # é©—è­‰TextEmbeddingå¯ç”¨æ€§ï¼ˆä¸éœ€è¦å¯¦é™…èª¿ç”¨ï¼‰
                        self.embedding = "text-embedding-v3"
                        self.client = None
                        logger.info(f"ğŸ’¡ DeepSeekä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥æœå‹™")
                    except ImportError as e:
                        logger.error(f"âš ï¸ DashScopeåŒ…æœªå®‰è£: {e}")
                        dashscope_key = None  # å¼ºåˆ¶é™ç´š
                    except Exception as e:
                        logger.error(f"âš ï¸ é˜¿é‡Œç™¾ç‚¼åµŒå…¥åˆå§‹åŒ–å¤±è´¥: {e}")
                        dashscope_key = None  # å¼ºåˆ¶é™ç´š
            else:
                dashscope_key = None  # è·³éé˜¿é‡Œç™¾ç‚¼

            if not dashscope_key or force_openai:
                # é™ç´šåˆ°OpenAIåµŒå…¥
                self.embedding = "text-embedding-3-small"
                openai_key = os.getenv('OPENAI_API_KEY')
                if openai_key:
                    self.client = OpenAI(
                        api_key=openai_key,
                        base_url=config.get("backend_url", "https://api.openai.com/v1")
                    )
                    logger.warning(f"âš ï¸ DeepSeekå›é€€åˆ°OpenAIåµŒå…¥æœå‹™")
                else:
                    # æœ€å¾Œå˜—è©¦DeepSeekè‡ªå·±çš„åµŒå…¥
                    deepseek_key = os.getenv('DEEPSEEK_API_KEY')
                    if deepseek_key:
                        try:
                            self.client = OpenAI(
                                api_key=deepseek_key,
                                base_url="https://api.deepseek.com"
                            )
                            logger.info(f"ğŸ’¡ DeepSeekä½¿ç”¨è‡ªå·±çš„åµŒå…¥æœå‹™")
                        except Exception as e:
                            logger.error(f"âŒ DeepSeekåµŒå…¥æœå‹™ä¸å¯ç”¨: {e}")
                            # ç¦ç”¨å…§å­˜åŠŸèƒ½
                            self.client = "DISABLED"
                            logger.info(f"ğŸš¨ å…§å­˜åŠŸèƒ½å·²ç¦ç”¨ï¼Œç³»çµ±å°†ç¹¼ç»­é‹è¡Œä½†ä¸ä¿å­˜æ­·å²è¨˜å¿†")
                    else:
                        # ç¦ç”¨å…§å­˜åŠŸèƒ½è€Œä¸æ˜¯æŠ›å‡ºç•°å¸¸
                        self.client = "DISABLED"
                        logger.info(f"ğŸš¨ æœªæ‰¾åˆ°å¯ç”¨çš„åµŒå…¥æœå‹™ï¼Œå…§å­˜åŠŸèƒ½å·²ç¦ç”¨")
        elif self.llm_provider == "google":
            # Google AIä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œå¦å‰‡ç¦ç”¨è¨˜å¿†åŠŸèƒ½
            dashscope_key = os.getenv('DASHSCOPE_API_KEY')
            openai_key = os.getenv('OPENAI_API_KEY')
            
            if dashscope_key:
                try:
                    # å˜—è©¦åˆå§‹åŒ–DashScope
                    import dashscope
                    from dashscope import TextEmbedding

                    self.embedding = "text-embedding-v3"
                    self.client = None
                    dashscope.api_key = dashscope_key
                    
                    # æª¢æŸ¥æ˜¯å¦æœ‰OpenAIå¯†é‘°ä½œç‚ºé™ç´šé¸é …
                    if openai_key:
                        logger.info(f"ğŸ’¡ Google AIä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥æœå‹™ï¼ˆOpenAIä½œç‚ºé™ç´šé¸é …ï¼‰")
                        self.fallback_available = True
                        self.fallback_client = OpenAI(api_key=openai_key, base_url=config["backend_url"])
                        self.fallback_embedding = "text-embedding-3-small"
                    else:
                        logger.info(f"ğŸ’¡ Google AIä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥æœå‹™ï¼ˆç„¡é™ç´šé¸é …ï¼‰")
                        self.fallback_available = False
                        
                except ImportError as e:
                    logger.error(f"âŒ DashScopeåŒ…æœªå®‰è£: {e}")
                    self.client = "DISABLED"
                    logger.warning(f"âš ï¸ Google AIè¨˜å¿†åŠŸèƒ½å·²ç¦ç”¨")
                except Exception as e:
                    logger.error(f"âŒ DashScopeåˆå§‹åŒ–å¤±è´¥: {e}")
                    self.client = "DISABLED"
                    logger.warning(f"âš ï¸ Google AIè¨˜å¿†åŠŸèƒ½å·²ç¦ç”¨")
            else:
                # æ²¡æœ‰DashScopeå¯†é‘°ï¼Œç¦ç”¨è¨˜å¿†åŠŸèƒ½
                self.client = "DISABLED"
                self.fallback_available = False
                logger.warning(f"âš ï¸ Google AIæœªæ‰¾åˆ°DASHSCOPE_API_KEYï¼Œè¨˜å¿†åŠŸèƒ½å·²ç¦ç”¨")
                logger.info(f"ğŸ’¡ ç³»çµ±å°†ç¹¼ç»­é‹è¡Œï¼Œä½†ä¸æœƒä¿å­˜æˆ–æª¢ç´¢æ­·å²è¨˜å¿†")
        elif self.llm_provider == "openrouter":
            # OpenRouteræ”¯æŒï¼šå„ªå…ˆä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥ï¼Œå¦å‰‡ç¦ç”¨è¨˜å¿†åŠŸèƒ½
            dashscope_key = os.getenv('DASHSCOPE_API_KEY')
            if dashscope_key:
                try:
                    # å˜—è©¦ä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥
                    import dashscope
                    from dashscope import TextEmbedding

                    self.embedding = "text-embedding-v3"
                    self.client = None
                    dashscope.api_key = dashscope_key
                    logger.info(f"ğŸ’¡ OpenRouterä½¿ç”¨é˜¿é‡Œç™¾ç‚¼åµŒå…¥æœå‹™")
                except ImportError as e:
                    logger.error(f"âŒ DashScopeåŒ…æœªå®‰è£: {e}")
                    self.client = "DISABLED"
                    logger.warning(f"âš ï¸ OpenRouterè¨˜å¿†åŠŸèƒ½å·²ç¦ç”¨")
                except Exception as e:
                    logger.error(f"âŒ DashScopeåˆå§‹åŒ–å¤±è´¥: {e}")
                    self.client = "DISABLED"
                    logger.warning(f"âš ï¸ OpenRouterè¨˜å¿†åŠŸèƒ½å·²ç¦ç”¨")
            else:
                # æ²¡æœ‰DashScopeå¯†é‘°ï¼Œç¦ç”¨è¨˜å¿†åŠŸèƒ½
                self.client = "DISABLED"
                logger.warning(f"âš ï¸ OpenRouteræœªæ‰¾åˆ°DASHSCOPE_API_KEYï¼Œè¨˜å¿†åŠŸèƒ½å·²ç¦ç”¨")
                logger.info(f"ğŸ’¡ ç³»çµ±å°†ç¹¼ç»­é‹è¡Œï¼Œä½†ä¸æœƒä¿å­˜æˆ–æª¢ç´¢æ­·å²è¨˜å¿†")
        elif config["backend_url"] == "http://localhost:11434/v1":
            self.embedding = "nomic-embed-text"
            self.client = OpenAI(base_url=config["backend_url"])
        else:
            self.embedding = "text-embedding-3-small"
            openai_key = os.getenv('OPENAI_API_KEY')
            if openai_key:
                self.client = OpenAI(
                    api_key=openai_key,
                    base_url=config["backend_url"]
                )
            else:
                self.client = "DISABLED"
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°OPENAI_API_KEYï¼Œè¨˜å¿†åŠŸèƒ½å·²ç¦ç”¨")

        # ä½¿ç”¨å–®ä¾‹ChromaDBç®¡ç†å™¨
        self.chroma_manager = ChromaDBManager()
        self.situation_collection = self.chroma_manager.get_or_create_collection(name)

    def _smart_text_truncation(self, text, max_length=8192):
        """æ™ºèƒ½æ–‡æœ¬æˆªæ–·ï¼Œä¿æŒèªç¾©å®Œæ•´æ€§å’Œç·©å­˜å…¼å®¹æ€§"""
        if len(text) <= max_length:
            return text, False  # è¿”å›åŸæ–‡æœ¬å’Œæ˜¯å¦æˆªæ–·çš„æ¨™èªŒ
        
        # å˜—è©¦åœ¨å¥å­é‚Šç•Œæˆªæ–·
        sentences = text.split('ã€‚')
        if len(sentences) > 1:
            truncated = ""
            for sentence in sentences:
                if len(truncated + sentence + 'ã€‚') <= max_length - 50:  # ç•™50å­—ç¬¦ä½™é‡
                    truncated += sentence + 'ã€‚'
                else:
                    break
            if len(truncated) > max_length // 2:  # è‡³å°‘ä¿ç•™ä¸€åŠå…§å®¹
                logger.info(f"ğŸ“ æ™ºèƒ½æˆªæ–·ï¼šåœ¨å¥å­é‚Šç•Œæˆªæ–·ï¼Œä¿ç•™{len(truncated)}/{len(text)}å­—ç¬¦")
                return truncated, True
        
        # å˜—è©¦åœ¨æ®µè½é‚Šç•Œæˆªæ–·
        paragraphs = text.split('\n')
        if len(paragraphs) > 1:
            truncated = ""
            for paragraph in paragraphs:
                if len(truncated + paragraph + '\n') <= max_length - 50:
                    truncated += paragraph + '\n'
                else:
                    break
            if len(truncated) > max_length // 2:
                logger.info(f"ğŸ“ æ™ºèƒ½æˆªæ–·ï¼šåœ¨æ®µè½é‚Šç•Œæˆªæ–·ï¼Œä¿ç•™{len(truncated)}/{len(text)}å­—ç¬¦")
                return truncated, True
        
        # æœ€å¾Œé¸æ“‡ï¼šä¿ç•™å‰åŠéƒ¨åˆ†å’Œå¾ŒåŠéƒ¨åˆ†çš„é—œé”®ä¿¡æ¯
        front_part = text[:max_length//2]
        back_part = text[-(max_length//2-100):]  # ç•™100å­—ç¬¦çµ¦é€£æ¥ç¬¦
        truncated = front_part + "\n...[å…§å®¹æˆªæ–·]...\n" + back_part
        logger.warning(f"âš ï¸ å¼ºåˆ¶æˆªæ–·ï¼šä¿ç•™é¦–å°¾é—œé”®ä¿¡æ¯ï¼Œ{len(text)}å­—ç¬¦æˆªæ–·ç‚º{len(truncated)}å­—ç¬¦")
        return truncated, True

    def get_embedding(self, text):
        """Get embedding for a text using the configured provider"""

        # æª¢æŸ¥è¨˜å¿†åŠŸèƒ½æ˜¯å¦è¢«ç¦ç”¨
        if self.client == "DISABLED":
            # å…§å­˜åŠŸèƒ½å·²ç¦ç”¨ï¼Œè¿”å›ç©ºå‘é‡
            logger.debug(f"âš ï¸ è¨˜å¿†åŠŸèƒ½å·²ç¦ç”¨ï¼Œè¿”å›ç©ºå‘é‡")
            return [0.0] * 1024  # è¿”å›1024ç¶­çš„é›¶å‘é‡

        # é©—è­‰è¼¸å…¥æ–‡æœ¬
        if not text or not isinstance(text, str):
            logger.warning(f"âš ï¸ è¼¸å…¥æ–‡æœ¬ç‚ºç©ºæˆ–ç„¡æ•ˆï¼Œè¿”å›ç©ºå‘é‡")
            return [0.0] * 1024

        text_length = len(text)
        if text_length == 0:
            logger.warning(f"âš ï¸ è¼¸å…¥æ–‡æœ¬é•·åº¦ç‚º0ï¼Œè¿”å›ç©ºå‘é‡")
            return [0.0] * 1024
        
        # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨é•·åº¦é™åˆ¶
        if self.enable_embedding_length_check and text_length > self.max_embedding_length:
            logger.warning(f"âš ï¸ æ–‡æœ¬éé•·({text_length:,}å­—ç¬¦ > {self.max_embedding_length:,}å­—ç¬¦)ï¼Œè·³éå‘é‡åŒ–")
            # å­˜å‚¨è·³éä¿¡æ¯
            self._last_text_info = {
                'original_length': text_length,
                'processed_length': 0,
                'was_truncated': False,
                'was_skipped': True,
                'provider': self.llm_provider,
                'strategy': 'length_limit_skip',
                'max_length': self.max_embedding_length
            }
            return [0.0] * 1024
        
        # è¨˜éŒ„æ–‡æœ¬ä¿¡æ¯ï¼ˆä¸é€²è¡Œä»»ä½•æˆªæ–·ï¼‰
        if text_length > 8192:
            logger.info(f"ğŸ“ è™•ç†é•·æ–‡æœ¬: {text_length}å­—ç¬¦ï¼Œæä¾›å•†: {self.llm_provider}")
        
        # å­˜å‚¨æ–‡æœ¬è™•ç†ä¿¡æ¯
        self._last_text_info = {
            'original_length': text_length,
            'processed_length': text_length,  # ä¸æˆªæ–·ï¼Œä¿æŒåŸé•·åº¦
            'was_truncated': False,  # æ°¸ä¸æˆªæ–·
            'was_skipped': False,
            'provider': self.llm_provider,
            'strategy': 'no_truncation_with_fallback'  # æ¨™è¨˜ç­–ç•¥
        }

        if (self.llm_provider == "dashscope" or
            self.llm_provider == "alibaba" or
            self.llm_provider == "qianfan" or
            (self.llm_provider == "google" and self.client is None) or
            (self.llm_provider == "deepseek" and self.client is None) or
            (self.llm_provider == "openrouter" and self.client is None)):
            # ä½¿ç”¨é˜¿é‡Œç™¾ç‚¼çš„åµŒå…¥æ¨¡å‹
            try:
                # å°å…¥DashScopeæ¨¡å¡Š
                import dashscope
                from dashscope import TextEmbedding

                # æª¢æŸ¥DashScope APIå¯†é‘°æ˜¯å¦å¯ç”¨
                if not hasattr(dashscope, 'api_key') or not dashscope.api_key:
                    logger.warning(f"âš ï¸ DashScope APIå¯†é‘°æœªè¨­ç½®ï¼Œè¨˜å¿†åŠŸèƒ½é™ç´š")
                    return [0.0] * 1024  # è¿”å›ç©ºå‘é‡

                # å˜—è©¦èª¿ç”¨DashScope API
                response = TextEmbedding.call(
                    model=self.embedding,
                    input=text
                )

                # æª¢æŸ¥éŸ¿æ‡‰ç‹€æ…‹
                if response.status_code == 200:
                    # æˆåŠŸç²å–embedding
                    embedding = response.output['embeddings'][0]['embedding']
                    logger.debug(f"âœ… DashScope embeddingæˆåŠŸï¼Œç¶­åº¦: {len(embedding)}")
                    return embedding
                else:
                    # APIè¿”å›éŒ¯èª¤ç‹€æ…‹ç¢¼
                    error_msg = f"{response.code} - {response.message}"
                    
                    # æª¢æŸ¥æ˜¯å¦ç‚ºé•·åº¦é™åˆ¶éŒ¯èª¤
                    if any(keyword in error_msg.lower() for keyword in ['length', 'token', 'limit', 'exceed']):
                        logger.warning(f"âš ï¸ DashScopeé•·åº¦é™åˆ¶: {error_msg}")
                        
                        # æª¢æŸ¥æ˜¯å¦æœ‰é™ç´šé¸é …
                        if hasattr(self, 'fallback_available') and self.fallback_available:
                            logger.info(f"ğŸ’¡ å˜—è©¦ä½¿ç”¨OpenAIé™ç´šè™•ç†é•·æ–‡æœ¬")
                            try:
                                response = self.fallback_client.embeddings.create(
                                    model=self.fallback_embedding,
                                    input=text
                                )
                                embedding = response.data[0].embedding
                                logger.info(f"âœ… OpenAIé™ç´šæˆåŠŸï¼Œç¶­åº¦: {len(embedding)}")
                                return embedding
                            except Exception as fallback_error:
                                logger.error(f"âŒ OpenAIé™ç´šå¤±è´¥: {str(fallback_error)}")
                                logger.info(f"ğŸ’¡ æ‰€æœ‰é™ç´šé¸é …å¤±è´¥ï¼Œè¨˜å¿†åŠŸèƒ½é™ç´š")
                                return [0.0] * 1024
                        else:
                            logger.info(f"ğŸ’¡ ç„¡å¯ç”¨é™ç´šé¸é …ï¼Œè¨˜å¿†åŠŸèƒ½é™ç´š")
                            return [0.0] * 1024
                    else:
                        logger.error(f"âŒ DashScope APIéŒ¯èª¤: {error_msg}")
                        return [0.0] * 1024  # è¿”å›ç©ºå‘é‡è€Œä¸æ˜¯æŠ›å‡ºç•°å¸¸

            except Exception as e:
                error_str = str(e).lower()
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºé•·åº¦é™åˆ¶éŒ¯èª¤
                if any(keyword in error_str for keyword in ['length', 'token', 'limit', 'exceed', 'too long']):
                    logger.warning(f"âš ï¸ DashScopeé•·åº¦é™åˆ¶ç•°å¸¸: {str(e)}")
                    
                    # æª¢æŸ¥æ˜¯å¦æœ‰é™ç´šé¸é …
                    if hasattr(self, 'fallback_available') and self.fallback_available:
                        logger.info(f"ğŸ’¡ å˜—è©¦ä½¿ç”¨OpenAIé™ç´šè™•ç†é•·æ–‡æœ¬")
                        try:
                            response = self.fallback_client.embeddings.create(
                                model=self.fallback_embedding,
                                input=text
                            )
                            embedding = response.data[0].embedding
                            logger.info(f"âœ… OpenAIé™ç´šæˆåŠŸï¼Œç¶­åº¦: {len(embedding)}")
                            return embedding
                        except Exception as fallback_error:
                            logger.error(f"âŒ OpenAIé™ç´šå¤±è´¥: {str(fallback_error)}")
                            logger.info(f"ğŸ’¡ æ‰€æœ‰é™ç´šé¸é …å¤±è´¥ï¼Œè¨˜å¿†åŠŸèƒ½é™ç´š")
                            return [0.0] * 1024
                    else:
                        logger.info(f"ğŸ’¡ ç„¡å¯ç”¨é™ç´šé¸é …ï¼Œè¨˜å¿†åŠŸèƒ½é™ç´š")
                        return [0.0] * 1024
                elif 'import' in error_str:
                    logger.error(f"âŒ DashScopeåŒ…æœªå®‰è£: {str(e)}")
                elif 'connection' in error_str:
                    logger.error(f"âŒ DashScopeç¶²çµ¡é€£æ¥éŒ¯èª¤: {str(e)}")
                elif 'timeout' in error_str:
                    logger.error(f"âŒ DashScopeè«‹æ±‚è¶…æ™‚: {str(e)}")
                else:
                    logger.error(f"âŒ DashScope embeddingç•°å¸¸: {str(e)}")
                
                logger.warning(f"âš ï¸ è¨˜å¿†åŠŸèƒ½é™ç´šï¼Œè¿”å›ç©ºå‘é‡")
                return [0.0] * 1024
        else:
            # ä½¿ç”¨OpenAIå…¼å®¹çš„åµŒå…¥æ¨¡å‹
            if self.client is None:
                logger.warning(f"âš ï¸ åµŒå…¥å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºå‘é‡")
                return [0.0] * 1024  # è¿”å›ç©ºå‘é‡
            elif self.client == "DISABLED":
                # å…§å­˜åŠŸèƒ½å·²ç¦ç”¨ï¼Œè¿”å›ç©ºå‘é‡
                logger.debug(f"âš ï¸ å…§å­˜åŠŸèƒ½å·²ç¦ç”¨ï¼Œè¿”å›ç©ºå‘é‡")
                return [0.0] * 1024  # è¿”å›1024ç¶­çš„é›¶å‘é‡

            # å˜—è©¦èª¿ç”¨OpenAIå…¼å®¹çš„embedding API
            try:
                response = self.client.embeddings.create(
                    model=self.embedding,
                    input=text
                )
                embedding = response.data[0].embedding
                logger.debug(f"âœ… {self.llm_provider} embeddingæˆåŠŸï¼Œç¶­åº¦: {len(embedding)}")
                return embedding

            except Exception as e:
                error_str = str(e).lower()
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºé•·åº¦é™åˆ¶éŒ¯èª¤
                length_error_keywords = [
                    'token', 'length', 'too long', 'exceed', 'maximum', 'limit',
                    'context', 'input too large', 'request too large'
                ]
                
                is_length_error = any(keyword in error_str for keyword in length_error_keywords)
                
                if is_length_error:
                    # é•·åº¦é™åˆ¶éŒ¯èª¤ï¼šç›´æ¥é™ç´šï¼Œä¸æˆªæ–·é‡è©¦
                    logger.warning(f"âš ï¸ {self.llm_provider}é•·åº¦é™åˆ¶: {str(e)}")
                    logger.info(f"ğŸ’¡ ç‚ºä¿è­‰åˆ†ææº–ç¢ºæ€§ï¼Œä¸æˆªæ–·æ–‡æœ¬ï¼Œè¨˜å¿†åŠŸèƒ½é™ç´š")
                else:
                    # å…¶ä»–é¡å‹çš„éŒ¯èª¤
                    if 'attributeerror' in error_str:
                        logger.error(f"âŒ {self.llm_provider} APIèª¿ç”¨éŒ¯èª¤: {str(e)}")
                    elif 'connectionerror' in error_str or 'connection' in error_str:
                        logger.error(f"âŒ {self.llm_provider}ç¶²çµ¡é€£æ¥éŒ¯èª¤: {str(e)}")
                    elif 'timeout' in error_str:
                        logger.error(f"âŒ {self.llm_provider}è«‹æ±‚è¶…æ™‚: {str(e)}")
                    elif 'keyerror' in error_str:
                        logger.error(f"âŒ {self.llm_provider}éŸ¿æ‡‰æ ¼å¼éŒ¯èª¤: {str(e)}")
                    else:
                        logger.error(f"âŒ {self.llm_provider} embeddingç•°å¸¸: {str(e)}")
                
                logger.warning(f"âš ï¸ è¨˜å¿†åŠŸèƒ½é™ç´šï¼Œè¿”å›ç©ºå‘é‡")
                return [0.0] * 1024

    def get_embedding_config_status(self):
        """ç²å–å‘é‡ç·©å­˜é…ç½®ç‹€æ…‹"""
        return {
            'enabled': self.enable_embedding_length_check,
            'max_embedding_length': self.max_embedding_length,
            'max_embedding_length_formatted': f"{self.max_embedding_length:,}å­—ç¬¦",
            'provider': self.llm_provider,
            'client_status': 'DISABLED' if self.client == "DISABLED" else 'ENABLED'
        }

    def get_last_text_info(self):
        """ç²å–æœ€å¾Œè™•ç†çš„æ–‡æœ¬ä¿¡æ¯"""
        return getattr(self, '_last_text_info', None)

    def add_situations(self, situations_and_advice):
        """Add financial situations and their corresponding advice. Parameter is a list of tuples (situation, rec)"""

        situations = []
        advice = []
        ids = []
        embeddings = []

        offset = self.situation_collection.count()

        for i, (situation, recommendation) in enumerate(situations_and_advice):
            situations.append(situation)
            advice.append(recommendation)
            ids.append(str(offset + i))
            embeddings.append(self.get_embedding(situation))

        self.situation_collection.add(
            documents=situations,
            metadatas=[{"recommendation": rec} for rec in advice],
            embeddings=embeddings,
            ids=ids,
        )

    def get_memories(self, current_situation, n_matches=1):
        """Find matching recommendations using embeddings with smart truncation handling"""
        
        # ç²å–ç•¶å‰æƒ…å†µçš„embedding
        query_embedding = self.get_embedding(current_situation)
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºç©ºå‘é‡ï¼ˆè¨˜å¿†åŠŸèƒ½è¢«ç¦ç”¨æˆ–å‡ºéŒ¯ï¼‰
        if all(x == 0.0 for x in query_embedding):
            logger.debug(f"âš ï¸ æŸ¥è©¢embeddingç‚ºç©ºå‘é‡ï¼Œè¿”å›ç©ºçµæœ")
            return []
        
        # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ çš„æ•¸æ“šé€²è¡ŒæŸ¥è©¢
        collection_count = self.situation_collection.count()
        if collection_count == 0:
            logger.debug(f"ğŸ“­ è¨˜å¿†åº«ç‚ºç©ºï¼Œè¿”å›ç©ºçµæœ")
            return []
        
        # èª¿æ•´æŸ¥è©¢æ•¸é‡ï¼Œä¸èƒ½è¶…éé›†åˆä¸­çš„æ–‡æ¡£æ•¸é‡
        actual_n_matches = min(n_matches, collection_count)
        
        try:
            # åŸ·è¡Œç›¸ä¼¼åº¦æŸ¥è©¢
            results = self.situation_collection.query(
                query_embeddings=[query_embedding],
                n_results=actual_n_matches
            )
            
            # è™•ç†æŸ¥è©¢çµæœ
            memories = []
            if results and 'documents' in results and results['documents']:
                documents = results['documents'][0]
                metadatas = results.get('metadatas', [[]])[0]
                distances = results.get('distances', [[]])[0]
                
                for i, doc in enumerate(documents):
                    metadata = metadatas[i] if i < len(metadatas) else {}
                    distance = distances[i] if i < len(distances) else 1.0
                    
                    memory_item = {
                        'situation': doc,
                        'recommendation': metadata.get('recommendation', ''),
                        'similarity': 1.0 - distance,  # è½‰æ›ç‚ºç›¸ä¼¼åº¦åˆ†æ•¸
                        'distance': distance
                    }
                    memories.append(memory_item)
                
                # è¨˜éŒ„æŸ¥è©¢ä¿¡æ¯
                if hasattr(self, '_last_text_info') and self._last_text_info.get('was_truncated'):
                    logger.info(f"ğŸ” æˆªæ–·æ–‡æœ¬æŸ¥è©¢å®Œæˆï¼Œæ‰¾åˆ°{len(memories)}å€‹ç›¸é—œè¨˜å¿†")
                    logger.debug(f"ğŸ“Š åŸæ–‡é•·åº¦: {self._last_text_info['original_length']}, "
                               f"è™•ç†å¾Œé•·åº¦: {self._last_text_info['processed_length']}")
                else:
                    logger.debug(f"ğŸ” è¨˜å¿†æŸ¥è©¢å®Œæˆï¼Œæ‰¾åˆ°{len(memories)}å€‹ç›¸é—œè¨˜å¿†")
            
            return memories
            
        except Exception as e:
            logger.error(f"âŒ è¨˜å¿†æŸ¥è©¢å¤±è´¥: {str(e)}")
            return []

    def get_cache_info(self):
        """ç²å–ç·©å­˜ç›¸é—œä¿¡æ¯ï¼Œç”¨æ–¼èª¿è©¦å’Œç›£æ§"""
        info = {
            'collection_count': self.situation_collection.count(),
            'client_status': 'enabled' if self.client != "DISABLED" else 'disabled',
            'embedding_model': self.embedding,
            'provider': self.llm_provider
        }
        
        # æ·»åŠ æœ€å¾Œä¸€æ¬¡æ–‡æœ¬è™•ç†ä¿¡æ¯
        if hasattr(self, '_last_text_info'):
            info['last_text_processing'] = self._last_text_info
            
        return info


if __name__ == "__main__":
    # Example usage
    matcher = FinancialSituationMemory()

    # Example data
    example_data = [
        (
            "High inflation rate with rising interest rates and declining consumer spending",
            "Consider defensive sectors like consumer staples and utilities. Review fixed-income portfolio duration.",
        ),
        (
            "Tech sector showing high volatility with increasing institutional selling pressure",
            "Reduce exposure to high-growth tech stocks. Look for value opportunities in established tech companies with strong cash flows.",
        ),
        (
            "Strong dollar affecting emerging markets with increasing forex volatility",
            "Hedge currency exposure in international positions. Consider reducing allocation to emerging market debt.",
        ),
        (
            "Market showing signs of sector rotation with rising yields",
            "Rebalance portfolio to maintain target allocations. Consider increasing exposure to sectors benefiting from higher rates.",
        ),
    ]

    # Add the example situations and recommendations
    matcher.add_situations(example_data)

    # Example query
    current_situation = """
    Market showing increased volatility in tech sector, with institutional investors 
    reducing positions and rising interest rates affecting growth stock valuations
    """

    try:
        recommendations = matcher.get_memories(current_situation, n_matches=2)

        for i, rec in enumerate(recommendations, 1):
            logger.info(f"\nMatch {i}:")
            logger.info(f"Similarity Score: {rec.get('similarity', 0):.2f}")
            logger.info(f"Matched Situation: {rec.get('situation', '')}")
            logger.info(f"Recommendation: {rec.get('recommendation', '')}")

    except Exception as e:
        logger.error(f"Error during recommendation: {str(e)}")
