#!/usr/bin/env python3
"""
çµ±ä¸€æ–°èåˆ†æå·¥å…·
æ•´åˆç¾è‚¡å¸‚å ´çš„æ–°èç²å–é‚è¼¯åˆ°ä¸€å€‹å·¥å…·å‡½æ•¸ä¸­
è®“å¤§æ¨¡å‹åªéœ€è¦èª¿ç”¨ä¸€å€‹å·¥å…·å°±èƒ½ç²å–ç¾è‚¡çš„æ–°èæ•¸æ“š
"""

import logging
from datetime import datetime, timedelta
import re
import hashlib
import os

logger = logging.getLogger(__name__)

class UnifiedNewsAnalyzer:
    """çµ±ä¸€æ–°èåˆ†æå™¨ï¼Œæ•´åˆæ‰€æœ‰æ–°èç²å–é‚è¼¯"""

    def __init__(self, toolkit):
        """åˆå§‹åŒ–çµ±ä¸€æ–°èåˆ†æå™¨

        Args:
            toolkit: åŒ…å«å„ç¨®æ–°èç²å–å·¥å…·çš„å·¥å…·åŒ…
        """
        self.toolkit = toolkit
        self.news_cache = {}
        self.cache_hours = int(os.getenv('NEWS_CACHE_HOURS', '4'))
        self.cache_enabled = os.getenv('NEWS_CACHE_ENABLED', 'true').lower() == 'true'
        logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] å¿«å–å·²{'å•Ÿç”¨' if self.cache_enabled else 'ç¦ç”¨'}ï¼Œå¿«å–æ™‚é–“: {self.cache_hours}å°æ™‚")
        
    def get_stock_news_unified(self, stock_code: str, max_news: int = 10, model_info: str = "") -> str:
        """
        çµ±ä¸€æ–°èç²å–æ¥å£
        æ ¹æ“šè‚¡ç¥¨ä»£ç¢¼è‡ªå‹•è­˜åˆ¥è‚¡ç¥¨é¡å‹ä¸¦ç²å–ç›¸æ‡‰æ–°è

        Args:
            stock_code: è‚¡ç¥¨ä»£ç¢¼
            max_news: æœ€å¤§æ–°èæ•¸é‡
            model_info: ç•¶å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ï¼Œç”¨æ–¼ç‰¹æ®Šè™•ç†

        Returns:
            str: æ ¼å¼åŒ–çš„æ–°èå…§å®¹
        """
        logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] é–‹å§‹ç²å– {stock_code} çš„æ–°èï¼Œæ¨¡å‹: {model_info}")
        logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] ğŸ¤– ç•¶å‰æ¨¡å‹ä¿¡æ¯: {model_info}")

        if self.cache_enabled:
            cache_key = self._generate_cache_key(stock_code, max_news)
            cached_result = self._get_from_cache(cache_key)
            if cached_result:
                logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] âœ… å¾å¿«å–è¿”å›çµæœï¼Œè‚¡ç¥¨: {stock_code}")
                return cached_result

        stock_type = self._identify_stock_type(stock_code)
        logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] è‚¡ç¥¨é¡å‹: {stock_type}")
        
        # çµ±ä¸€ä½¿ç”¨ç¾è‚¡æ–°èç²å–æ–¹æ³•
        result = self._get_us_share_news(stock_code, max_news, model_info)
        
        logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] ğŸ“Š æ–°èç²å–å®Œæˆï¼Œçµæœé•·åº¦: {len(result)} å­—ç¬¦")
        logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] ğŸ“‹ è¿”å›çµæœé è¦½ (å‰1000å­—ç¬¦): {result[:1000]}")

        if not result or len(result.strip()) < 50:
            logger.warning(f"[çµ±ä¸€æ–°èå·¥å…·] âš ï¸ è¿”å›çµæœç•°å¸¸çŸ­æˆ–ç‚ºç©ºï¼")
            logger.warning(f"[çµ±ä¸€æ–°èå·¥å…·] ğŸ“ å®Œæ•´çµæœå…§å®¹: '{result}'")

        if self.cache_enabled and result and len(result.strip()) >= 50:
            cache_key = self._generate_cache_key(stock_code, max_news)
            self._save_to_cache(cache_key, result)
            logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] ğŸ’¾ çµæœå·²ä¿å­˜åˆ°å¿«å–ï¼Œè‚¡ç¥¨: {stock_code}")

        return result

    def _generate_cache_key(self, stock_code: str, max_news: int) -> str:
        """ç”¢ç”Ÿå¿«å–éµå€¼"""
        today = datetime.now().strftime("%Y-%m-%d")
        key_str = f"{stock_code}_{max_news}_{today}"
        return hashlib.md5(key_str.encode()).hexdigest()

    def _get_from_cache(self, cache_key: str):
        """å¾å¿«å–ç²å–è³‡æ–™"""
        if cache_key in self.news_cache:
            cached_data, cached_time = self.news_cache[cache_key]
            if datetime.now() - cached_time < timedelta(hours=self.cache_hours):
                return cached_data
            else:
                del self.news_cache[cache_key]
        return None

    def _save_to_cache(self, cache_key: str, data: str):
        """å°‡è³‡æ–™ä¿å­˜åˆ°å¿«å–"""
        self.news_cache[cache_key] = (data, datetime.now())
    
    def _identify_stock_type(self, stock_code: str) -> str:
        """è­˜åˆ¥è‚¡ç¥¨é¡å‹ - çµ±ä¸€è¦–ç‚ºç¾è‚¡"""
        return "ç¾è‚¡"
    
    def _get_us_share_news(self, stock_code: str, max_news: int, model_info: str = "") -> str:
        """ç²å–ç¾è‚¡æ–°è"""
        logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] ç²å–ç¾è‚¡ {stock_code} æ–°è")
        
        # ç²å–ç•¶å‰æ—¥æœŸ
        curr_date = datetime.now().strftime("%Y-%m-%d")
        
        # å„ªå…ˆç´š1: OpenAIå…¨çƒæ–°è
        try:
            if hasattr(self.toolkit, 'get_global_news_openai'):
                logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] å˜—è©¦OpenAIç¾è‚¡æ–°è...")
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¢ºèª¿ç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸åƒæ•¸
                result = self.toolkit.get_global_news_openai.invoke({"curr_date": curr_date})
                if result and len(result.strip()) > 50:
                    logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] âœ… OpenAIç¾è‚¡æ–°èç²å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "OpenAIç¾è‚¡æ–°è", model_info)
        except Exception as e:
            logger.warning(f"[çµ±ä¸€æ–°èå·¥å…·] OpenAIç¾è‚¡æ–°èç²å–å¤±æ•—: {e}")
        
        # å„ªå…ˆç´š2: Googleæ–°èï¼ˆè‹±æ–‡æœç´¢ï¼‰
        try:
            if hasattr(self.toolkit, 'get_google_news'):
                logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] å˜—è©¦Googleç¾è‚¡æ–°è...")
                query = f"{stock_code} stock news earnings financial"
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¢ºèª¿ç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸åƒæ•¸
                result = self.toolkit.get_google_news.invoke({"query": query, "curr_date": curr_date})
                if result and len(result.strip()) > 50:
                    logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] âœ… Googleç¾è‚¡æ–°èç²å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "Googleç¾è‚¡æ–°è", model_info)
        except Exception as e:
            logger.warning(f"[çµ±ä¸€æ–°èå·¥å…·] Googleç¾è‚¡æ–°èç²å–å¤±æ•—: {e}")
        
        # å„ªå…ˆç´š3: FinnHubæ–°èï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            if hasattr(self.toolkit, 'get_finnhub_news'):
                logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] å˜—è©¦FinnHubç¾è‚¡æ–°è...")
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¢ºèª¿ç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸åƒæ•¸
                result = self.toolkit.get_finnhub_news.invoke({"symbol": stock_code, "max_results": min(max_news, 50)})
                if result and len(result.strip()) > 50:
                    logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] âœ… FinnHubç¾è‚¡æ–°èç²å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "FinnHubç¾è‚¡æ–°è", model_info)
        except Exception as e:
            logger.warning(f"[çµ±ä¸€æ–°èå·¥å…·] FinnHubç¾è‚¡æ–°èç²å–å¤±æ•—: {e}")
        
        return "âŒ ç„¡æ³•ç²å–ç¾è‚¡æ–°èæ•¸æ“šï¼Œæ‰€æœ‰æ–°èæºå‡ä¸å¯ç”¨"
    
    def _format_news_result(self, news_content: str, source: str, model_info: str = "") -> str:
        """æ ¼å¼åŒ–æ–°èçµæœ"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # ğŸ” æ·»åŠ èª¿è©¦æ—¥èªŒï¼šæ‰“å°åŸå§‹æ–°èå…§å®¹
        logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] ğŸ“‹ åŸå§‹æ–°èå…§å®¹é è¦½ (å‰500å­—ç¬¦): {news_content[:500]}")
        logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] ğŸ“Š åŸå§‹å…§å®¹é•·åº¦: {len(news_content)} å­—ç¬¦")
        
        # æª¢æ¸¬æ˜¯å¦ç‚ºGoogle/Geminiæ¨¡å‹
        is_google_model = any(keyword in model_info.lower() for keyword in ['google', 'gemini', 'gemma'])
        original_length = len(news_content)
        google_control_applied = False
        
        # ğŸ” æ·»åŠ Googleæ¨¡å‹æª¢æ¸¬æ—¥èªŒ
        if is_google_model:
            logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] ğŸ¤– æª¢æ¸¬åˆ°Googleæ¨¡å‹ï¼Œå•Ÿç”¨ç‰¹æ®Šè™•ç†")
        
        # å°Googleæ¨¡å‹é€²è¡Œç‰¹æ®Šçš„é•·åº¦æ§åˆ¶
        if is_google_model and len(news_content) > 5000:  # é™ä½é–¾å€¼åˆ°5000å­—ç¬¦
            logger.warning(f"[çµ±ä¸€æ–°èå·¥å…·] ğŸ”§ æª¢æ¸¬åˆ°Googleæ¨¡å‹ï¼Œæ–°èå…§å®¹éé•·({len(news_content)}å­—ç¬¦)ï¼Œé€²è¡Œé•·åº¦æ§åˆ¶...")
            
            # æ›´åš´æ ¼çš„é•·åº¦æ§åˆ¶ç­–ç•¥
            lines = news_content.split('\n')
            important_lines = []
            char_count = 0
            target_length = 3000  # ç›®æ¨™é•·åº¦è¨­ç‚º3000å­—ç¬¦
            
            # ç¬¬ä¸€è¼ªï¼šå„ªå…ˆä¿ç•™åŒ…å«é—œéµè©çš„é‡è¦è¡Œ
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # æª¢æŸ¥æ˜¯å¦åŒ…å«é‡è¦é—œéµè©
                important_keywords = ['è‚¡ç¥¨', 'å…¬å¸', 'è²¡å ±', 'æ¥­ç¸¾', 'æ¼²è·Œ', 'åƒ¹æ ¼', 'å¸‚å€¼', 'ç‡Ÿæ”¶', 'åˆ©æ½¤',
                                    'å¢é•·', 'ä¸‹è·Œ', 'ä¸Šæ¼²', 'ç›ˆåˆ©', 'è™§æ', 'æŠ•è³‡', 'åˆ†æ', 'é æœŸ', 'å…¬å‘Š']
                
                is_important = any(keyword in line for keyword in important_keywords)
                
                if is_important and char_count + len(line) < target_length:
                    important_lines.append(line)
                    char_count += len(line)
                elif not is_important and char_count + len(line) < target_length * 0.7:  # éé‡è¦å…§å®¹æ›´åš´æ ¼é™åˆ¶
                    important_lines.append(line)
                    char_count += len(line)
                
                # å¦‚æœå·²é”åˆ°ç›®æ¨™é•·åº¦ï¼Œåœæ­¢æ·»åŠ 
                if char_count >= target_length:
                    break
            
            # å¦‚æœæå–çš„é‡è¦å…§å®¹ä»ç„¶éé•·ï¼Œé€²è¡Œé€²ä¸€æ­¥æˆªæ–·
            if important_lines:
                processed_content = '\n'.join(important_lines)
                if len(processed_content) > target_length:
                    processed_content = processed_content[:target_length] + "...(å…§å®¹å·²æ™ºèƒ½æˆªæ–·)"
                
                news_content = processed_content
                google_control_applied = True
                logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] âœ… Googleæ¨¡å‹æ™ºèƒ½é•·åº¦æ§åˆ¶å®Œæˆï¼Œå¾{original_length}å­—ç¬¦å£“ç¸®è‡³{len(news_content)}å­—ç¬¦")
            else:
                # å¦‚æœæ²’æœ‰é‡è¦è¡Œï¼Œç›´æ¥æˆªæ–·åˆ°ç›®æ¨™é•·åº¦
                news_content = news_content[:target_length] + "...(å…§å®¹å·²å¼·åˆ¶æˆªæ–·)"
                google_control_applied = True
                logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] âš ï¸ Googleæ¨¡å‹å¼·åˆ¶æˆªæ–·è‡³{target_length}å­—ç¬¦")
        
        # è¨ˆç®—æœ€çµ‚çš„æ ¼å¼åŒ–çµæœé•·åº¦ï¼Œç¢ºä¿ç¸½é•·åº¦åˆç†
        base_format_length = 300  # æ ¼å¼åŒ–æ¨¡æ¿çš„å¤§æ¦‚é•·åº¦
        if is_google_model and (len(news_content) + base_format_length) > 4000:
            # å¦‚æœåŠ ä¸Šæ ¼å¼åŒ–å¾Œä»ç„¶éé•·ï¼Œé€²ä¸€æ­¥å£“ç¸®æ–°èå…§å®¹
            max_content_length = 3500
            if len(news_content) > max_content_length:
                news_content = news_content[:max_content_length] + "...(å·²å„ªåŒ–é•·åº¦)"
                google_control_applied = True
                logger.info(f"[çµ±ä¸€æ–°èå·¥å…·] ğŸ”§ Googleæ¨¡å‹æœ€çµ‚é•·åº¦å„ªåŒ–ï¼Œå…§å®¹é•·åº¦: {len(news_content)}å­—ç¬¦")
        
        formatted_result = f"""
=== ğŸ“° æ–°èæ•¸æ“šä¾†æº: {source} ===
ç²å–æ™‚é–“: {timestamp}
æ•¸æ“šé•·åº¦: {len(news_content)} å­—ç¬¦
{f"æ¨¡å‹é¡å‹: {model_info}" if model_info else ""}
{f"ğŸ”§ Googleæ¨¡å‹é•·åº¦æ§åˆ¶å·²æ‡‰ç”¨ (åŸé•·åº¦: {original_length} å­—ç¬¦)" if google_control_applied else ""}

=== ğŸ“‹ æ–°èå…§å®¹ ===
{news_content}

=== âœ… æ•¸æ“šç‹€æ…‹ ===
ç‹€æ…‹: æˆåŠŸç²å–
ä¾†æº: {source}
æ™‚é–“æˆ³: {timestamp}
"""
        return formatted_result.strip()


def create_unified_news_tool(toolkit):
    """å‰µå»ºçµ±ä¸€æ–°èå·¥å…·å‡½æ•¸"""
    analyzer = UnifiedNewsAnalyzer(toolkit)
    
    def get_stock_news_unified(stock_code: str, max_news: int = 100, model_info: str = ""):
        """
        çµ±ä¸€æ–°èç²å–å·¥å…·

        Args:
            stock_code (str): ç¾è‚¡è‚¡ç¥¨ä»£ç¢¼ (å¦‚ AAPLã€TSLAã€NVDA)
            max_news (int): æœ€å¤§æ–°èæ•¸é‡ï¼Œé»˜èª100
            model_info (str): ç•¶å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ï¼Œç”¨æ–¼ç‰¹æ®Šè™•ç†

        Returns:
            str: æ ¼å¼åŒ–çš„æ–°èå…§å®¹
        """
        if not stock_code:
            return "âŒ éŒ¯èª¤: æœªæä¾›è‚¡ç¥¨ä»£ç¢¼"
        
        return analyzer.get_stock_news_unified(stock_code, max_news, model_info)
    
    # è¨­ç½®å·¥å…·å±¬æ€§
    get_stock_news_unified.name = "get_stock_news_unified"
    get_stock_news_unified.description = """
çµ±ä¸€æ–°èç²å–å·¥å…· - ç²å–ç¾è‚¡å¸‚å ´çš„æ–°è

åŠŸèƒ½:
- å°ˆæ³¨æ–¼ç¾è‚¡æ–°èç²å–
- å„ªå…ˆOpenAI -> Googleè‹±æ–‡ -> FinnHub
- è¿”å›æ ¼å¼åŒ–çš„æ–°èå…§å®¹
- æ”¯æŒGoogleæ¨¡å‹çš„ç‰¹æ®Šé•·åº¦æ§åˆ¶
"""
    
    return get_stock_news_unified