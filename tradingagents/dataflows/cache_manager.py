#!/usr/bin/env python3
"""
è‚¡ç¥¨æ•¸æ“šç·©å­˜ç®¡ç†å™¨
æ”¯æŒæœ¬åœ°ç·©å­˜è‚¡ç¥¨æ•¸æ“šï¼Œæ¸›å°‘APIèª¿ç”¨ï¼Œæé«˜éŸ¿æ‡‰é€Ÿåº¦
"""

import os
import json
import pickle
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, Any, Union, List
import hashlib

# å°å…¥æ—¥èªŒæ¨¡å¡Š
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('agents')


class StockDataCache:
    """è‚¡ç¥¨æ•¸æ“šç·©å­˜ç®¡ç†å™¨ - æ”¯æŒç¾è‚¡æ•¸æ“šç·©å­˜å„ªåŒ–"""

    def __init__(self, cache_dir: str = None):
        """
        åˆå§‹åŒ–ç·©å­˜ç®¡ç†å™¨

        Args:
            cache_dir: ç·©å­˜ç›®éŒ„è·¯å¾‘ï¼Œé»˜èªç‚º tradingagents/dataflows/data_cache
        """
        if cache_dir is None:
            # ç²å–ç•¶å‰æ–‡ä»¶æ‰€åœ¨ç›®éŒ„
            current_dir = Path(__file__).parent
            cache_dir = current_dir / "data_cache"

        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

        # å‰µå»ºå­ç›®éŒ„ - ç¾è‚¡å¸‚å ´åˆ†é¡
        self.us_stock_dir = self.cache_dir / "us_stocks"
        self.us_news_dir = self.cache_dir / "us_news"
        self.us_fundamentals_dir = self.cache_dir / "us_fundamentals"
        self.metadata_dir = self.cache_dir / "metadata"

        # å‰µå»ºæ‰€æœ‰ç›®éŒ„
        for dir_path in [self.us_stock_dir, self.us_news_dir,
                        self.us_fundamentals_dir, self.metadata_dir]:
            dir_path.mkdir(exist_ok=True)

        # ç·©å­˜é…ç½® - ç¾è‚¡å¸‚å ´TTLè¨­å®š
        self.cache_config = {
            'us_stock_data': {
                'ttl_hours': 2,  # ç¾è‚¡æ•¸æ“šç·©å­˜2å°æ™‚ï¼ˆè€ƒæ…®åˆ°APIé™åˆ¶ï¼‰
                'max_files': 1000,
                'description': 'ç¾è‚¡æ­·å²æ•¸æ“š'
            },
            'us_news': {
                'ttl_hours': 6,  # ç¾è‚¡æ–°èç·©å­˜6å°æ™‚
                'max_files': 500,
                'description': 'ç¾è‚¡æ–°èæ•¸æ“š'
            },
            'us_fundamentals': {
                'ttl_hours': 24,  # ç¾è‚¡åŸºæœ¬é¢æ•¸æ“šç·©å­˜24å°æ™‚
                'max_files': 200,
                'description': 'ç¾è‚¡åŸºæœ¬é¢æ•¸æ“š'
            }
        }

        # å…§å®¹é•·åº¦é™åˆ¶é…ç½®ï¼ˆæ–‡ä»¶ç·©å­˜é»˜èªä¸é™åˆ¶ï¼‰
        self.content_length_config = {
            'max_content_length': int(os.getenv('MAX_CACHE_CONTENT_LENGTH', '50000')),  # 50Kå­—ç¬¦
            'long_text_providers': ['openai', 'google', 'anthropic'],  # æ”¯æ´é•·æ–‡æœ¬çš„æä¾›å•†
            'enable_length_check': os.getenv('ENABLE_CACHE_LENGTH_CHECK', 'false').lower() == 'true'  # æ–‡ä»¶ç·©å­˜é»˜èªä¸é™åˆ¶
        }

        logger.info(f"ğŸ“ ç·©å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œç·©å­˜ç›®éŒ„: {self.cache_dir}")
        logger.info(f"ğŸ—„ï¸ æ•¸æ“šåº«ç·©å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ç¾è‚¡æ•¸æ“š: å·²é…ç½®")

    def _determine_market_type(self, symbol: str) -> str:
        """æ ¹æ“šè‚¡ç¥¨ä»£ç¢¼åˆ¤æ–·å¸‚å ´é¡å‹ï¼ˆç›®å‰åƒ…æ”¯æ´ç¾è‚¡ï¼‰"""
        return 'us'

    def _check_provider_availability(self) -> List[str]:
        """æª¢æŸ¥å¯ç”¨çš„ LLM æä¾›å•†"""
        available_providers = []

        # æª¢æŸ¥ OpenAI
        openai_key = os.getenv("OPENAI_API_KEY")
        if openai_key and openai_key.strip():
            # ç°¡å–®çš„æ ¼å¼æª¢æŸ¥
            if openai_key.startswith('sk-') and len(openai_key) >= 40:
                available_providers.append('openai')
        
        # æª¢æŸ¥Google AI
        google_key = os.getenv("GOOGLE_API_KEY")
        if google_key and google_key.strip():
            available_providers.append('google')
        
        # æª¢æŸ¥Anthropic
        anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        if anthropic_key and anthropic_key.strip():
            available_providers.append('anthropic')
        
        return available_providers

    def should_skip_cache_for_content(self, content: str, data_type: str = "unknown") -> bool:
        """
        åˆ¤æ–·æ˜¯å¦å› ç‚ºå…§å®¹è¶…é•·è€Œè·³éç·©å­˜
        
        Args:
            content: è¦ç·©å­˜çš„å…§å®¹
            data_type: æ•¸æ“šé¡å‹ï¼ˆç”¨æ–¼æ—¥èªŒï¼‰
        
        Returns:
            bool: æ˜¯å¦æ‡‰è©²è·³éç·©å­˜
        """
        # å¦‚æœæœªå•Ÿç”¨é•·åº¦æª¢æŸ¥ï¼Œç›´æ¥è¿”å›False
        if not self.content_length_config['enable_length_check']:
            return False
        
        # æª¢æŸ¥å…§å®¹é•·åº¦
        content_length = len(content)
        max_length = self.content_length_config['max_content_length']
        
        if content_length <= max_length:
            return False
        
        # å…§å®¹è¶…é•·ï¼Œæª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„é•·æ–‡æœ¬è™•ç†æä¾›å•†
        available_providers = self._check_provider_availability()
        long_text_providers = self.content_length_config['long_text_providers']
        
        # æ‰¾åˆ°å¯ç”¨çš„é•·æ–‡æœ¬æä¾›å•†
        available_long_providers = [p for p in available_providers if p in long_text_providers]
        
        if not available_long_providers:
            logger.warning(f"âš ï¸ å…§å®¹éé•·({content_length:,}å­—ç¬¦ > {max_length:,}å­—ç¬¦)ä¸”ç„¡å¯ç”¨é•·æ–‡æœ¬æä¾›å•†ï¼Œè·³é{data_type}ç·©å­˜")
            logger.info(f"ğŸ’¡ å¯ç”¨æä¾›å•†: {available_providers}")
            logger.info(f"ğŸ’¡ é•·æ–‡æœ¬æä¾›å•†: {long_text_providers}")
            return True
        else:
            logger.info(f"âœ… å…§å®¹è¼ƒé•·({content_length:,}å­—ç¬¦)ä½†æœ‰å¯ç”¨é•·æ–‡æœ¬æä¾›å•†({available_long_providers})ï¼Œç¹¼çºŒç·©å­˜")
            return False
    
    def _generate_cache_key(self, data_type: str, symbol: str, **kwargs) -> str:
        """ç”Ÿæˆç·©å­˜éµ"""
        # å‰µå»ºä¸€å€‹åŒ…å«æ‰€æœ‰åƒæ•¸çš„å­—ç¬¦ä¸²
        params_str = f"{data_type}_{symbol}"
        for key, value in sorted(kwargs.items()):
            params_str += f"_{key}_{value}"
        
        # ä½¿ç”¨MD5ç”ŸæˆçŸ­çš„å”¯ä¸€æ¨™è­˜
        cache_key = hashlib.md5(params_str.encode()).hexdigest()[:12]
        return f"{symbol}_{data_type}_{cache_key}"
    
    def _get_cache_path(self, data_type: str, cache_key: str, file_format: str = "json", symbol: str = None) -> Path:
        """ç²å–ç·©å­˜æ–‡ä»¶è·¯å¾‘ - æ”¯æŒå¸‚å ´åˆ†é¡"""
        # çµ±ä¸€ä½¿ç”¨ç¾è‚¡å¸‚å ´é¡å‹
        market_type = 'us'

        # æ ¹æ“šæ•¸æ“šé¡å‹é¸æ“‡å°æ‡‰çš„ç¾è‚¡ç·©å­˜ç›®éŒ„
        if data_type == "stock_data":
            base_dir = self.us_stock_dir
        elif data_type == "news":
            base_dir = self.us_news_dir
        elif data_type == "fundamentals":
            base_dir = self.us_fundamentals_dir
        else:
            base_dir = self.cache_dir

        return base_dir / f"{cache_key}.{file_format}"
    
    def _get_metadata_path(self, cache_key: str) -> Path:
        """ç²å–å…ƒæ•¸æ“šæ–‡ä»¶è·¯å¾‘"""
        return self.metadata_dir / f"{cache_key}_meta.json"
    
    def _save_metadata(self, cache_key: str, metadata: Dict[str, Any]):
        """ä¿å­˜å…ƒæ•¸æ“š"""
        metadata_path = self._get_metadata_path(cache_key)
        metadata_path.parent.mkdir(parents=True, exist_ok=True)  # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        metadata['cached_at'] = datetime.now().isoformat()
        
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    def _load_metadata(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """åŠ è¼‰å…ƒæ•¸æ“š"""
        metadata_path = self._get_metadata_path(cache_key)
        if not metadata_path.exists():
            return None
        
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"âš ï¸ åŠ è¼‰å…ƒæ•¸æ“šå¤±æ•—: {e}")
            return None
    
    def is_cache_valid(self, cache_key: str, max_age_hours: int = None, symbol: str = None, data_type: str = None) -> bool:
        """æª¢æŸ¥ç·©å­˜æ˜¯å¦æœ‰æ•ˆ - æ”¯æŒæ™ºèƒ½TTLé…ç½®"""
        metadata = self._load_metadata(cache_key)
        if not metadata:
            return False

        # å¦‚æœæ²’æœ‰æŒ‡å®šTTLï¼Œæ ¹æ“šæ•¸æ“šé¡å‹å’Œå¸‚å ´è‡ªå‹•ç¢ºå®š
        if max_age_hours is None:
            if symbol and data_type:
                market_type = self._determine_market_type(symbol)
                cache_type = f"{market_type}_{data_type}"
                max_age_hours = self.cache_config.get(cache_type, {}).get('ttl_hours', 24)
            else:
                # å¾å…ƒæ•¸æ“šä¸­ç²å–ä¿¡æ¯
                symbol = metadata.get('symbol', '')
                data_type = metadata.get('data_type', 'stock_data')
                market_type = self._determine_market_type(symbol)
                cache_type = f"{market_type}_{data_type}"
                max_age_hours = self.cache_config.get(cache_type, {}).get('ttl_hours', 24)

        cached_at = datetime.fromisoformat(metadata['cached_at'])
        age = datetime.now() - cached_at

        is_valid = age.total_seconds() < max_age_hours * 3600

        if is_valid:
            market_type = self._determine_market_type(metadata.get('symbol', ''))
            cache_type = f"{market_type}_{metadata.get('data_type', 'stock_data')}"
            desc = self.cache_config.get(cache_type, {}).get('description', 'æ•¸æ“š')
            logger.info(f"âœ… ç·©å­˜æœ‰æ•ˆ: {desc} - {metadata.get('symbol')} (å‰©é¤˜ {max_age_hours - age.total_seconds()/3600:.1f}h)")

        return is_valid
    
    def save_stock_data(self, symbol: str, data: Union[pd.DataFrame, str],
                       start_date: str = None, end_date: str = None,
                       data_source: str = "unknown") -> str:
        """
        ä¿å­˜è‚¡ç¥¨æ•¸æ“šåˆ°ç·©å­˜ - æ”¯æŒç¾è‚¡åˆ†é¡å„²å­˜

        Args:
            symbol: è‚¡ç¥¨ä»£ç¢¼
            data: è‚¡ç¥¨æ•¸æ“šï¼ˆDataFrameæˆ–å­—ç¬¦ä¸²ï¼‰
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ
            data_source: æ•¸æ“šæºï¼ˆå¦‚ "tdx", "yfinance", "finnhub"ï¼‰

        Returns:
            cache_key: ç·©å­˜éµ
        """
        # æª¢æŸ¥å…§å®¹é•·åº¦æ˜¯å¦éœ€è¦è·³éç·©å­˜
        content_to_check = str(data)
        if self.should_skip_cache_for_content(content_to_check, "è‚¡ç¥¨æ•¸æ“š"):
            # ç”Ÿæˆä¸€å€‹è™›æ“¬çš„ç·©å­˜éµï¼Œä½†ä¸å¯¦éš›ä¿å­˜
            market_type = self._determine_market_type(symbol)
            cache_key = self._generate_cache_key("stock_data", symbol,
                                               start_date=start_date,
                                               end_date=end_date,
                                               source=data_source,
                                               market=market_type,
                                               skipped=True)
            logger.info(f"ğŸš« è‚¡ç¥¨æ•¸æ“šå› å…§å®¹éé•·è¢«è·³éç·©å­˜: {symbol} -> {cache_key}")
            return cache_key

        market_type = self._determine_market_type(symbol)
        cache_key = self._generate_cache_key("stock_data", symbol,
                                           start_date=start_date,
                                           end_date=end_date,
                                           source=data_source,
                                           market=market_type)

        # ä¿å­˜æ•¸æ“š
        if isinstance(data, pd.DataFrame):
            cache_path = self._get_cache_path("stock_data", cache_key, "csv", symbol)
            cache_path.parent.mkdir(parents=True, exist_ok=True)  # ç¢ºä¿ç›®éŒ„å­˜åœ¨
            data.to_csv(cache_path, index=True)
        else:
            cache_path = self._get_cache_path("stock_data", cache_key, "txt", symbol)
            cache_path.parent.mkdir(parents=True, exist_ok=True)  # ç¢ºä¿ç›®éŒ„å­˜åœ¨
            with open(cache_path, 'w', encoding='utf-8') as f:
                f.write(str(data))

        # ä¿å­˜å…ƒæ•¸æ“š
        metadata = {
            'symbol': symbol,
            'data_type': 'stock_data',
            'market_type': market_type,
            'start_date': start_date,
            'end_date': end_date,
            'data_source': data_source,
            'file_path': str(cache_path),
            'file_format': 'csv' if isinstance(data, pd.DataFrame) else 'txt',
            'content_length': len(content_to_check)
        }
        self._save_metadata(cache_key, metadata)

        # ç²å–æè¿°ä¿¡æ¯
        cache_type = f"{market_type}_stock_data"
        desc = self.cache_config.get(cache_type, {}).get('description', 'è‚¡ç¥¨æ•¸æ“š')
        logger.info(f"ğŸ’¾ {desc}å·²ç·©å­˜: {symbol} ({data_source}) -> {cache_key}")
        return cache_key
    
    def load_stock_data(self, cache_key: str) -> Optional[Union[pd.DataFrame, str]]:
        """å¾ç·©å­˜åŠ è¼‰è‚¡ç¥¨æ•¸æ“š"""
        metadata = self._load_metadata(cache_key)
        if not metadata:
            return None
        
        cache_path = Path(metadata['file_path'])
        if not cache_path.exists():
            return None
        
        try:
            if metadata['file_format'] == 'csv':
                return pd.read_csv(cache_path, index_col=0)
            else:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    return f.read()
        except Exception as e:
            logger.error(f"âš ï¸ åŠ è¼‰ç·©å­˜æ•¸æ“šå¤±æ•—: {e}")
            return None
    
    def find_cached_stock_data(self, symbol: str, start_date: str = None,
                              end_date: str = None, data_source: str = None,
                              max_age_hours: int = None) -> Optional[str]:
        """
        æŸ¥æ‰¾åŒ¹é…çš„ç·©å­˜æ•¸æ“š - æ”¯æŒæ™ºèƒ½å¸‚å ´åˆ†é¡æŸ¥æ‰¾

        Args:
            symbol: è‚¡ç¥¨ä»£ç¢¼
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ
            data_source: æ•¸æ“šæº
            max_age_hours: æœ€å¤§ç·©å­˜æ™‚é–“ï¼ˆå°æ™‚ï¼‰ï¼ŒNoneæ™‚ä½¿ç”¨æ™ºèƒ½é…ç½®

        Returns:
            cache_key: å¦‚æœæ‰¾åˆ°æœ‰æ•ˆç·©å­˜å‰‡è¿”å›ç·©å­˜éµï¼Œå¦å‰‡è¿”å›None
        """
        market_type = self._determine_market_type(symbol)

        # å¦‚æœæ²’æœ‰æŒ‡å®šTTLï¼Œä½¿ç”¨æ™ºèƒ½é…ç½®
        if max_age_hours is None:
            cache_type = f"{market_type}_stock_data"
            max_age_hours = self.cache_config.get(cache_type, {}).get('ttl_hours', 24)

        # ç”ŸæˆæŸ¥æ‰¾éµ
        search_key = self._generate_cache_key("stock_data", symbol,
                                            start_date=start_date,
                                            end_date=end_date,
                                            source=data_source,
                                            market=market_type)

        # æª¢æŸ¥ç²¾ç¢ºåŒ¹é…
        if self.is_cache_valid(search_key, max_age_hours, symbol, 'stock_data'):
            desc = self.cache_config.get(f"{market_type}_stock_data", {}).get('description', 'æ•¸æ“š')
            logger.info(f"ğŸ¯ æ‰¾åˆ°ç²¾ç¢ºåŒ¹é…çš„{desc}: {symbol} -> {search_key}")
            return search_key

        # å¦‚æœæ²’æœ‰ç²¾ç¢ºåŒ¹é…ï¼ŒæŸ¥æ‰¾éƒ¨åˆ†åŒ¹é…ï¼ˆç›¸åŒè‚¡ç¥¨ä»£ç¢¼çš„å…¶ä»–ç·©å­˜ï¼‰
        for metadata_file in self.metadata_dir.glob(f"*_meta.json"):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)

                if (metadata.get('symbol') == symbol and
                    metadata.get('data_type') == 'stock_data' and
                    metadata.get('market_type') == market_type and
                    (data_source is None or metadata.get('data_source') == data_source)):

                    cache_key = metadata_file.stem.replace('_meta', '')
                    if self.is_cache_valid(cache_key, max_age_hours, symbol, 'stock_data'):
                        desc = self.cache_config.get(f"{market_type}_stock_data", {}).get('description', 'æ•¸æ“š')
                        logger.info(f"ğŸ“‹ æ‰¾åˆ°éƒ¨åˆ†åŒ¹é…çš„{desc}: {symbol} -> {cache_key}")
                        return cache_key
            except Exception:
                continue

        desc = self.cache_config.get(f"{market_type}_stock_data", {}).get('description', 'æ•¸æ“š')
        logger.error(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„{desc}ç·©å­˜: {symbol}")
        return None
    
    def save_news_data(self, symbol: str, news_data: str, 
                      start_date: str = None, end_date: str = None,
                      data_source: str = "unknown") -> str:
        """ä¿å­˜æ–°èæ•¸æ“šåˆ°ç·©å­˜"""
        # æª¢æŸ¥å…§å®¹é•·åº¦æ˜¯å¦éœ€è¦è·³éç·©å­˜
        if self.should_skip_cache_for_content(news_data, "æ–°èæ•¸æ“š"):
            # ç”Ÿæˆä¸€å€‹è™›æ“¬çš„ç·©å­˜éµï¼Œä½†ä¸å¯¦éš›ä¿å­˜
            cache_key = self._generate_cache_key("news", symbol,
                                               start_date=start_date,
                                               end_date=end_date,
                                               source=data_source,
                                               skipped=True)
            logger.info(f"ğŸš« æ–°èæ•¸æ“šå› å…§å®¹éé•·è¢«è·³éç·©å­˜: {symbol} -> {cache_key}")
            return cache_key

        cache_key = self._generate_cache_key("news", symbol,
                                           start_date=start_date,
                                           end_date=end_date,
                                           source=data_source)
        
        cache_path = self._get_cache_path("news", cache_key, "txt")
        cache_path.parent.mkdir(parents=True, exist_ok=True)  # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        with open(cache_path, 'w', encoding='utf-8') as f:
            f.write(news_data)
        
        metadata = {
            'symbol': symbol,
            'data_type': 'news',
            'start_date': start_date,
            'end_date': end_date,
            'data_source': data_source,
            'file_path': str(cache_path),
            'file_format': 'txt',
            'content_length': len(news_data)
        }
        self._save_metadata(cache_key, metadata)
        
        logger.info(f"ğŸ“° æ–°èæ•¸æ“šå·²ç·©å­˜: {symbol} ({data_source}) -> {cache_key}")
        return cache_key
    
    def save_fundamentals_data(self, symbol: str, fundamentals_data: str,
                              data_source: str = "unknown") -> str:
        """ä¿å­˜åŸºæœ¬é¢æ•¸æ“šåˆ°ç·©å­˜"""
        # æª¢æŸ¥å…§å®¹é•·åº¦æ˜¯å¦éœ€è¦è·³éç·©å­˜
        if self.should_skip_cache_for_content(fundamentals_data, "åŸºæœ¬é¢æ•¸æ“š"):
            # ç”Ÿæˆä¸€å€‹è™›æ“¬çš„ç·©å­˜éµï¼Œä½†ä¸å¯¦éš›ä¿å­˜
            market_type = self._determine_market_type(symbol)
            cache_key = self._generate_cache_key("fundamentals", symbol,
                                               source=data_source,
                                               market=market_type,
                                               date=datetime.now().strftime("%Y-%m-%d"),
                                               skipped=True)
            logger.info(f"ğŸš« åŸºæœ¬é¢æ•¸æ“šå› å…§å®¹éé•·è¢«è·³éç·©å­˜: {symbol} -> {cache_key}")
            return cache_key

        market_type = self._determine_market_type(symbol)
        cache_key = self._generate_cache_key("fundamentals", symbol,
                                           source=data_source,
                                           market=market_type,
                                           date=datetime.now().strftime("%Y-%m-%d"))
        
        cache_path = self._get_cache_path("fundamentals", cache_key, "txt", symbol)
        cache_path.parent.mkdir(parents=True, exist_ok=True)  # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        with open(cache_path, 'w', encoding='utf-8') as f:
            f.write(fundamentals_data)
        
        metadata = {
            'symbol': symbol,
            'data_type': 'fundamentals',
            'data_source': data_source,
            'market_type': market_type,
            'file_path': str(cache_path),
            'file_format': 'txt',
            'content_length': len(fundamentals_data)
        }
        self._save_metadata(cache_key, metadata)
        
        desc = self.cache_config.get(f"{market_type}_fundamentals", {}).get('description', 'åŸºæœ¬é¢æ•¸æ“š')
        logger.info(f"ğŸ’¼ {desc}å·²ç·©å­˜: {symbol} ({data_source}) -> {cache_key}")
        return cache_key
    
    def load_fundamentals_data(self, cache_key: str) -> Optional[str]:
        """å¾ç·©å­˜åŠ è¼‰åŸºæœ¬é¢æ•¸æ“š"""
        metadata = self._load_metadata(cache_key)
        if not metadata:
            return None
        
        cache_path = Path(metadata['file_path'])
        if not cache_path.exists():
            return None
        
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            logger.error(f"âš ï¸ åŠ è¼‰åŸºæœ¬é¢ç·©å­˜æ•¸æ“šå¤±æ•—: {e}")
            return None
    
    def find_cached_fundamentals_data(self, symbol: str, data_source: str = None,
                                    max_age_hours: int = None) -> Optional[str]:
        """
        æŸ¥æ‰¾åŒ¹é…çš„åŸºæœ¬é¢ç·©å­˜æ•¸æ“š
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç¢¼
            data_source: æ•¸æ“šæºï¼ˆå¦‚ "openai", "finnhub"ï¼‰
            max_age_hours: æœ€å¤§ç·©å­˜æ™‚é–“ï¼ˆå°æ™‚ï¼‰ï¼ŒNoneæ™‚ä½¿ç”¨æ™ºèƒ½é…ç½®
        
        Returns:
            cache_key: å¦‚æœæ‰¾åˆ°æœ‰æ•ˆç·©å­˜å‰‡è¿”å›ç·©å­˜éµï¼Œå¦å‰‡è¿”å›None
        """
        market_type = self._determine_market_type(symbol)
        
        # å¦‚æœæ²’æœ‰æŒ‡å®šTTLï¼Œä½¿ç”¨æ™ºèƒ½é…ç½®
        if max_age_hours is None:
            cache_type = f"{market_type}_fundamentals"
            max_age_hours = self.cache_config.get(cache_type, {}).get('ttl_hours', 24)
        
        # æŸ¥æ‰¾åŒ¹é…çš„ç·©å­˜
        for metadata_file in self.metadata_dir.glob(f"*_meta.json"):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                if (metadata.get('symbol') == symbol and
                    metadata.get('data_type') == 'fundamentals' and
                    metadata.get('market_type') == market_type and
                    (data_source is None or metadata.get('data_source') == data_source)):
                    
                    cache_key = metadata_file.stem.replace('_meta', '')
                    if self.is_cache_valid(cache_key, max_age_hours, symbol, 'fundamentals'):
                        desc = self.cache_config.get(f"{market_type}_fundamentals", {}).get('description', 'åŸºæœ¬é¢æ•¸æ“š')
                        logger.info(f"ğŸ¯ æ‰¾åˆ°åŒ¹é…çš„{desc}ç·©å­˜: {symbol} ({data_source}) -> {cache_key}")
                        return cache_key
            except Exception:
                continue
        
        desc = self.cache_config.get(f"{market_type}_fundamentals", {}).get('description', 'åŸºæœ¬é¢æ•¸æ“š')
        logger.error(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„{desc}ç·©å­˜: {symbol} ({data_source})")
        return None
    
    def clear_old_cache(self, max_age_days: int = 7):
        """æ¸…ç†éæœŸç·©å­˜"""
        cutoff_time = datetime.now() - timedelta(days=max_age_days)
        cleared_count = 0
        
        for metadata_file in self.metadata_dir.glob("*_meta.json"):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                cached_at = datetime.fromisoformat(metadata['cached_at'])
                if cached_at < cutoff_time:
                    # åˆªé™¤æ•¸æ“šæ–‡ä»¶
                    data_file = Path(metadata['file_path'])
                    if data_file.exists():
                        data_file.unlink()
                    
                    # åˆªé™¤å…ƒæ•¸æ“šæ–‡ä»¶
                    metadata_file.unlink()
                    cleared_count += 1
                    
            except Exception as e:
                logger.warning(f"âš ï¸ æ¸…ç†ç·©å­˜æ™‚å‡ºéŒ¯: {e}")
        
        logger.info(f"ğŸ§¹ å·²æ¸…ç† {cleared_count} å€‹éæœŸç·©å­˜æ–‡ä»¶")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ç²å–ç·©å­˜çµ±è¨ˆä¿¡æ¯"""
        stats = {
            'total_files': 0,
            'stock_data_count': 0,
            'news_count': 0,
            'fundamentals_count': 0,
            'total_size_mb': 0,
            'skipped_count': 0  # æ–°å¢ï¼šè·³éçš„ç·©å­˜æ•¸é‡
        }
        
        for metadata_file in self.metadata_dir.glob("*_meta.json"):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                data_type = metadata.get('data_type', 'unknown')
                if data_type == 'stock_data':
                    stats['stock_data_count'] += 1
                elif data_type == 'news':
                    stats['news_count'] += 1
                elif data_type == 'fundamentals':
                    stats['fundamentals_count'] += 1
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºè·³éçš„ç·©å­˜ï¼ˆæ²’æœ‰å¯¦éš›æ–‡ä»¶ï¼‰
                data_file = Path(metadata.get('file_path', ''))
                if not data_file.exists():
                    stats['skipped_count'] += 1
                else:
                    # è¨ˆç®—æ–‡ä»¶å¤§å°
                    stats['total_size_mb'] += data_file.stat().st_size / (1024 * 1024)
                
                stats['total_files'] += 1
                
            except Exception:
                continue
        
        stats['total_size_mb'] = round(stats['total_size_mb'], 2)
        return stats

    def get_content_length_config_status(self) -> Dict[str, Any]:
        """ç²å–å…§å®¹é•·åº¦é…ç½®ç‹€æ…‹"""
        available_providers = self._check_provider_availability()
        long_text_providers = self.content_length_config['long_text_providers']
        available_long_providers = [p for p in available_providers if p in long_text_providers]
        
        return {
            'enabled': self.content_length_config['enable_length_check'],
            'max_content_length': self.content_length_config['max_content_length'],
            'max_content_length_formatted': f"{self.content_length_config['max_content_length']:,}å­—ç¬¦",
            'long_text_providers': long_text_providers,
            'available_providers': available_providers,
            'available_long_providers': available_long_providers,
            'has_long_text_support': len(available_long_providers) > 0,
            'will_skip_long_content': self.content_length_config['enable_length_check'] and len(available_long_providers) == 0
        }


# å…¨å±€ç·©å­˜å¯¦ä¾‹
_cache_instance = None

def get_cache() -> StockDataCache:
    """ç²å–å…¨å±€ç·©å­˜å¯¦ä¾‹"""
    global _cache_instance
    if _cache_instance is None:
        _cache_instance = StockDataCache()
    return _cache_instance
