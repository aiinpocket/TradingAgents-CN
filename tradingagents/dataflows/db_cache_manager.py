#!/usr/bin/env python3
"""
MongoDB + Redis æ•¸æ“šåº«ç·©å­˜ç®¡ç†å™¨
æä¾›é«˜æ€§èƒ½çš„è‚¡ç¥¨æ•¸æ“šç·©å­˜å’ŒæŒä¹…åŒ–å­˜å‚¨
"""

import os
import json
import pickle
import hashlib
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List, Union
import pandas as pd

# å°å…¥æ—¥èªŒæ¨¡å¡Š
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('agents')

# MongoDB
try:
    from pymongo import MongoClient
    from pymongo.errors import ConnectionFailure, ServerSelectionTimeoutError
    MONGODB_AVAILABLE = True
except ImportError:
    MONGODB_AVAILABLE = False
    logger.warning(f"âš ï¸ pymongo æœªå®‰è£ï¼ŒMongoDBåŠŸèƒ½ä¸å¯ç”¨")

# Redis
try:
    import redis
    from redis.exceptions import ConnectionError as RedisConnectionError
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    logger.warning(f"âš ï¸ redis æœªå®‰è£ï¼ŒRedisåŠŸèƒ½ä¸å¯ç”¨")


class DatabaseCacheManager:
    """MongoDB + Redis æ•¸æ“šåº«ç·©å­˜ç®¡ç†å™¨"""
    
    def __init__(self,
                 mongodb_url: Optional[str] = None,
                 redis_url: Optional[str] = None,
                 mongodb_db: str = "tradingagents",
                 redis_db: int = 0):
        """
        åˆå§‹åŒ–æ•¸æ“šåº«ç·©å­˜ç®¡ç†å™¨

        Args:
            mongodb_url: MongoDBé€£æ¥URLï¼Œé»˜èªä½¿ç”¨é…ç½®æ–‡ä»¶ç«¯å£
            redis_url: Redisé€£æ¥URLï¼Œé»˜èªä½¿ç”¨é…ç½®æ–‡ä»¶ç«¯å£
            mongodb_db: MongoDBæ•¸æ“šåº«å
            redis_db: Redisæ•¸æ“šåº«ç·¨è™Ÿ
        """
        # å¾é…ç½®æ–‡ä»¶ç²å–æ­£ç¢ºçš„ç«¯å£
        mongodb_port = os.getenv("MONGODB_PORT", "27018")
        redis_port = os.getenv("REDIS_PORT", "6380")
        mongodb_password = os.getenv("MONGODB_PASSWORD", "tradingagents123")
        redis_password = os.getenv("REDIS_PASSWORD", "tradingagents123")

        self.mongodb_url = mongodb_url or os.getenv("MONGODB_URL", f"mongodb://admin:{mongodb_password}@localhost:{mongodb_port}")
        self.redis_url = redis_url or os.getenv("REDIS_URL", f"redis://:{redis_password}@localhost:{redis_port}")
        self.mongodb_db_name = mongodb_db
        self.redis_db = redis_db
        
        # åˆå§‹åŒ–é€£æ¥
        self.mongodb_client = None
        self.mongodb_db = None
        self.redis_client = None
        
        self._init_mongodb()
        self._init_redis()
        
        logger.info(f"ğŸ—„ï¸ æ•¸æ“šåº«ç·©å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.error(f"   MongoDB: {'âœ… å·²é€£æ¥' if self.mongodb_client else 'âŒ æœªé€£æ¥'}")
        logger.error(f"   Redis: {'âœ… å·²é€£æ¥' if self.redis_client else 'âŒ æœªé€£æ¥'}")
    
    def _init_mongodb(self):
        """åˆå§‹åŒ–MongoDBé€£æ¥"""
        if not MONGODB_AVAILABLE:
            return
        
        try:
            self.mongodb_client = MongoClient(
                self.mongodb_url,
                serverSelectionTimeoutMS=5000,  # 5ç§’è¶…æ™‚
                connectTimeoutMS=5000
            )
            # æ¸¬è©¦é€£æ¥
            self.mongodb_client.admin.command('ping')
            self.mongodb_db = self.mongodb_client[self.mongodb_db_name]
            
            # å‰µå»ºç´¢å¼•
            self._create_mongodb_indexes()
            
            logger.info(f"âœ… MongoDBé€£æ¥æˆåŠŸ: {self.mongodb_url}")
            
        except Exception as e:
            logger.error(f"âŒ MongoDBé€£æ¥å¤±è´¥: {e}")
            self.mongodb_client = None
            self.mongodb_db = None
    
    def _init_redis(self):
        """åˆå§‹åŒ–Redisé€£æ¥"""
        if not REDIS_AVAILABLE:
            return
        
        try:
            self.redis_client = redis.from_url(
                self.redis_url,
                db=self.redis_db,
                socket_timeout=5,
                socket_connect_timeout=5,
                decode_responses=True
            )
            # æ¸¬è©¦é€£æ¥
            self.redis_client.ping()
            
            logger.info(f"âœ… Redisé€£æ¥æˆåŠŸ: {self.redis_url}")
            
        except Exception as e:
            logger.error(f"âŒ Redisé€£æ¥å¤±è´¥: {e}")
            self.redis_client = None
    
    def _create_mongodb_indexes(self):
        """å‰µå»ºMongoDBç´¢å¼•"""
        if self.mongodb_db is None:
            return
        
        try:
            # è‚¡ç¥¨æ•¸æ“šé›†åˆç´¢å¼•
            stock_collection = self.mongodb_db.stock_data
            stock_collection.create_index([
                ("symbol", 1),
                ("data_source", 1),
                ("start_date", 1),
                ("end_date", 1)
            ])
            stock_collection.create_index([("created_at", 1)])
            
            # æ–°èæ•¸æ“šé›†åˆç´¢å¼•
            news_collection = self.mongodb_db.news_data
            news_collection.create_index([
                ("symbol", 1),
                ("data_source", 1),
                ("date_range", 1)
            ])
            news_collection.create_index([("created_at", 1)])
            
            # åŸºæœ¬é¢æ•¸æ“šé›†åˆç´¢å¼•
            fundamentals_collection = self.mongodb_db.fundamentals_data
            fundamentals_collection.create_index([
                ("symbol", 1),
                ("data_source", 1),
                ("analysis_date", 1)
            ])
            fundamentals_collection.create_index([("created_at", 1)])
            
            logger.info(f"âœ… MongoDBç´¢å¼•å‰µå»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âš ï¸ MongoDBç´¢å¼•å‰µå»ºå¤±è´¥: {e}")
    
    def _generate_cache_key(self, data_type: str, symbol: str, **kwargs) -> str:
        """ç”Ÿæˆç·©å­˜é”®"""
        params_str = f"{data_type}_{symbol}"
        for key, value in sorted(kwargs.items()):
            params_str += f"_{key}_{value}"
        
        cache_key = hashlib.md5(params_str.encode()).hexdigest()[:16]
        return f"{data_type}:{symbol}:{cache_key}"
    
    def save_stock_data(self, symbol: str, data: Union[pd.DataFrame, str],
                       start_date: str = None, end_date: str = None,
                       data_source: str = "unknown", market_type: str = None) -> str:
        """
        ä¿å­˜è‚¡ç¥¨æ•¸æ“šåˆ°MongoDBå’ŒRedis
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç¢¼
            data: è‚¡ç¥¨æ•¸æ“š
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ
            data_source: æ•¸æ“šæº
            market_type: å¸‚å ´é¡å‹ (us/china)
        
        Returns:
            cache_key: ç·©å­˜é”®
        """
        cache_key = self._generate_cache_key("stock", symbol,
                                           start_date=start_date,
                                           end_date=end_date,
                                           source=data_source)
        
        # è‡ªå‹•æ¨æ–·å¸‚å ´é¡å‹
        if market_type is None:
            # æ ¹æ“šè‚¡ç¥¨ä»£ç¢¼æ ¼å¼æ¨æ–·å¸‚å ´é¡å‹
            import re

            if re.match(r'^\d{6}$', symbol):  # 6ä½æ•¸å­—ç‚ºAè‚¡
                market_type = "china"
            else:  # å…¶ä»–æ ¼å¼ç‚ºç¾è‚¡
                market_type = "us"
        
        # æº–å¤‡æ–‡æ¡£æ•¸æ“š
        doc = {
            "_id": cache_key,
            "symbol": symbol,
            "market_type": market_type,
            "data_type": "stock_data",
            "start_date": start_date,
            "end_date": end_date,
            "data_source": data_source,
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow()
        }
        
        # è™•ç†æ•¸æ“šæ ¼å¼
        if isinstance(data, pd.DataFrame):
            doc["data"] = data.to_json(orient='records', date_format='iso')
            doc["data_format"] = "dataframe_json"
        else:
            doc["data"] = str(data)
            doc["data_format"] = "text"
        
        # ä¿å­˜åˆ°MongoDBï¼ˆæŒä¹…åŒ–ï¼‰
        if self.mongodb_db is not None:
            try:
                collection = self.mongodb_db.stock_data
                collection.replace_one({"_id": cache_key}, doc, upsert=True)
                logger.info(f"ğŸ’¾ è‚¡ç¥¨æ•¸æ“šå·²ä¿å­˜åˆ°MongoDB: {symbol} -> {cache_key}")
            except Exception as e:
                logger.error(f"âš ï¸ MongoDBä¿å­˜å¤±è´¥: {e}")
        
        # ä¿å­˜åˆ°Redisï¼ˆå¿«é€Ÿç·©å­˜ï¼Œ6å°æ™‚éæœŸï¼‰
        if self.redis_client:
            try:
                redis_data = {
                    "data": doc["data"],
                    "data_format": doc["data_format"],
                    "symbol": symbol,
                    "data_source": data_source,
                    "created_at": doc["created_at"].isoformat()
                }
                self.redis_client.setex(
                    cache_key,
                    6 * 3600,  # 6å°æ™‚éæœŸ
                    json.dumps(redis_data, ensure_ascii=False)
                )
                logger.info(f"âš¡ è‚¡ç¥¨æ•¸æ“šå·²ç·©å­˜åˆ°Redis: {symbol} -> {cache_key}")
            except Exception as e:
                logger.error(f"âš ï¸ Redisç·©å­˜å¤±è´¥: {e}")
        
        return cache_key
    
    def load_stock_data(self, cache_key: str) -> Optional[Union[pd.DataFrame, str]]:
        """å¾Redisæˆ–MongoDBåŠ è¼‰è‚¡ç¥¨æ•¸æ“š"""
        
        # é¦–å…ˆå˜—è©¦å¾RedisåŠ è¼‰ï¼ˆæ›´å¿«ï¼‰
        if self.redis_client:
            try:
                redis_data = self.redis_client.get(cache_key)
                if redis_data:
                    data_dict = json.loads(redis_data)
                    logger.info(f"âš¡ å¾RedisåŠ è¼‰æ•¸æ“š: {cache_key}")
                    
                    if data_dict["data_format"] == "dataframe_json":
                        return pd.read_json(data_dict["data"], orient='records')
                    else:
                        return data_dict["data"]
            except Exception as e:
                logger.error(f"âš ï¸ RedisåŠ è¼‰å¤±è´¥: {e}")
        
        # å¦‚æœRedisæ²¡æœ‰ï¼Œå¾MongoDBåŠ è¼‰
        if self.mongodb_db is not None:
            try:
                collection = self.mongodb_db.stock_data
                doc = collection.find_one({"_id": cache_key})
                
                if doc:
                    logger.info(f"ğŸ’¾ å¾MongoDBåŠ è¼‰æ•¸æ“š: {cache_key}")
                    
                    # åŒæ™‚æ›´æ–°åˆ°Redisç·©å­˜
                    if self.redis_client:
                        try:
                            redis_data = {
                                "data": doc["data"],
                                "data_format": doc["data_format"],
                                "symbol": doc["symbol"],
                                "data_source": doc["data_source"],
                                "created_at": doc["created_at"].isoformat()
                            }
                            self.redis_client.setex(
                                cache_key,
                                6 * 3600,
                                json.dumps(redis_data, ensure_ascii=False)
                            )
                            logger.info(f"âš¡ æ•¸æ“šå·²åŒæ­¥åˆ°Redisç·©å­˜")
                        except Exception as e:
                            logger.error(f"âš ï¸ RedisåŒæ­¥å¤±è´¥: {e}")
                    
                    if doc["data_format"] == "dataframe_json":
                        return pd.read_json(doc["data"], orient='records')
                    else:
                        return doc["data"]
                        
            except Exception as e:
                logger.error(f"âš ï¸ MongoDBåŠ è¼‰å¤±è´¥: {e}")
        
        return None
    
    def find_cached_stock_data(self, symbol: str, start_date: str = None,
                              end_date: str = None, data_source: str = None,
                              max_age_hours: int = 6) -> Optional[str]:
        """æŸ¥æ‰¾åŒ¹é…çš„ç·©å­˜æ•¸æ“š"""
        
        # ç”Ÿæˆç²¾ç¢ºåŒ¹é…çš„ç·©å­˜é”®
        exact_key = self._generate_cache_key("stock", symbol,
                                           start_date=start_date,
                                           end_date=end_date,
                                           source=data_source)
        
        # æª¢æŸ¥Redisä¸­æ˜¯å¦æœ‰ç²¾ç¢ºåŒ¹é…
        if self.redis_client and self.redis_client.exists(exact_key):
            logger.info(f"âš¡ Redisä¸­æ‰¾åˆ°ç²¾ç¢ºåŒ¹é…: {symbol} -> {exact_key}")
            return exact_key
        
        # æª¢æŸ¥MongoDBä¸­çš„åŒ¹é…é …
        if self.mongodb_db is not None:
            try:
                collection = self.mongodb_db.stock_data
                cutoff_time = datetime.utcnow() - timedelta(hours=max_age_hours)
                
                query = {
                    "symbol": symbol,
                    "created_at": {"$gte": cutoff_time}
                }
                
                if data_source:
                    query["data_source"] = data_source
                if start_date:
                    query["start_date"] = start_date
                if end_date:
                    query["end_date"] = end_date
                
                doc = collection.find_one(query, sort=[("created_at", -1)])
                
                if doc:
                    cache_key = doc["_id"]
                    logger.info(f"ğŸ’¾ MongoDBä¸­æ‰¾åˆ°åŒ¹é…: {symbol} -> {cache_key}")
                    return cache_key
                    
            except Exception as e:
                logger.error(f"âš ï¸ MongoDBæŸ¥è©¢å¤±è´¥: {e}")
        
        logger.error(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆç·©å­˜: {symbol}")
        return None

    def save_news_data(self, symbol: str, news_data: str,
                      start_date: str = None, end_date: str = None,
                      data_source: str = "unknown") -> str:
        """ä¿å­˜æ–°èæ•¸æ“šåˆ°MongoDBå’ŒRedis"""
        cache_key = self._generate_cache_key("news", symbol,
                                           start_date=start_date,
                                           end_date=end_date,
                                           source=data_source)

        doc = {
            "_id": cache_key,
            "symbol": symbol,
            "data_type": "news_data",
            "date_range": f"{start_date}_{end_date}",
            "start_date": start_date,
            "end_date": end_date,
            "data_source": data_source,
            "data": news_data,
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow()
        }

        # ä¿å­˜åˆ°MongoDB
        if self.mongodb_db is not None:
            try:
                collection = self.mongodb_db.news_data
                collection.replace_one({"_id": cache_key}, doc, upsert=True)
                logger.info(f"ğŸ“° æ–°èæ•¸æ“šå·²ä¿å­˜åˆ°MongoDB: {symbol} -> {cache_key}")
            except Exception as e:
                logger.error(f"âš ï¸ MongoDBä¿å­˜å¤±è´¥: {e}")

        # ä¿å­˜åˆ°Redisï¼ˆ24å°æ™‚éæœŸï¼‰
        if self.redis_client:
            try:
                redis_data = {
                    "data": news_data,
                    "symbol": symbol,
                    "data_source": data_source,
                    "created_at": doc["created_at"].isoformat()
                }
                self.redis_client.setex(
                    cache_key,
                    24 * 3600,  # 24å°æ™‚éæœŸ
                    json.dumps(redis_data, ensure_ascii=False)
                )
                logger.info(f"âš¡ æ–°èæ•¸æ“šå·²ç·©å­˜åˆ°Redis: {symbol} -> {cache_key}")
            except Exception as e:
                logger.error(f"âš ï¸ Redisç·©å­˜å¤±è´¥: {e}")

        return cache_key

    def save_fundamentals_data(self, symbol: str, fundamentals_data: str,
                              analysis_date: str = None,
                              data_source: str = "unknown") -> str:
        """ä¿å­˜åŸºæœ¬é¢æ•¸æ“šåˆ°MongoDBå’ŒRedis"""
        if not analysis_date:
            analysis_date = datetime.now().strftime("%Y-%m-%d")

        cache_key = self._generate_cache_key("fundamentals", symbol,
                                           date=analysis_date,
                                           source=data_source)

        doc = {
            "_id": cache_key,
            "symbol": symbol,
            "data_type": "fundamentals_data",
            "analysis_date": analysis_date,
            "data_source": data_source,
            "data": fundamentals_data,
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow()
        }

        # ä¿å­˜åˆ°MongoDB
        if self.mongodb_db is not None:
            try:
                collection = self.mongodb_db.fundamentals_data
                collection.replace_one({"_id": cache_key}, doc, upsert=True)
                logger.info(f"ğŸ’¼ åŸºæœ¬é¢æ•¸æ“šå·²ä¿å­˜åˆ°MongoDB: {symbol} -> {cache_key}")
            except Exception as e:
                logger.error(f"âš ï¸ MongoDBä¿å­˜å¤±è´¥: {e}")

        # ä¿å­˜åˆ°Redisï¼ˆ24å°æ™‚éæœŸï¼‰
        if self.redis_client:
            try:
                redis_data = {
                    "data": fundamentals_data,
                    "symbol": symbol,
                    "data_source": data_source,
                    "analysis_date": analysis_date,
                    "created_at": doc["created_at"].isoformat()
                }
                self.redis_client.setex(
                    cache_key,
                    24 * 3600,  # 24å°æ™‚éæœŸ
                    json.dumps(redis_data, ensure_ascii=False)
                )
                logger.info(f"âš¡ åŸºæœ¬é¢æ•¸æ“šå·²ç·©å­˜åˆ°Redis: {symbol} -> {cache_key}")
            except Exception as e:
                logger.error(f"âš ï¸ Redisç·©å­˜å¤±è´¥: {e}")

        return cache_key

    def get_cache_stats(self) -> Dict[str, Any]:
        """ç²å–ç·©å­˜çµ±è¨ˆä¿¡æ¯"""
        stats = {
            "mongodb": {"available": self.mongodb_db is not None, "collections": {}},
            "redis": {"available": self.redis_client is not None, "keys": 0, "memory_usage": "N/A"}
        }

        # MongoDBçµ±è¨ˆ
        if self.mongodb_db is not None:
            try:
                for collection_name in ["stock_data", "news_data", "fundamentals_data"]:
                    collection = self.mongodb_db[collection_name]
                    count = collection.count_documents({})
                    size = self.mongodb_db.command("collStats", collection_name).get("size", 0)
                    stats["mongodb"]["collections"][collection_name] = {
                        "count": count,
                        "size_mb": round(size / (1024 * 1024), 2)
                    }
            except Exception as e:
                logger.error(f"âš ï¸ MongoDBçµ±è¨ˆç²å–å¤±è´¥: {e}")

        # Redisçµ±è¨ˆ
        if self.redis_client:
            try:
                info = self.redis_client.info()
                stats["redis"]["keys"] = info.get("db0", {}).get("keys", 0)
                stats["redis"]["memory_usage"] = f"{info.get('used_memory_human', 'N/A')}"
            except Exception as e:
                logger.error(f"âš ï¸ Redisçµ±è¨ˆç²å–å¤±è´¥: {e}")

        return stats

    def clear_old_cache(self, max_age_days: int = 7):
        """æ¸…ç†éæœŸç·©å­˜"""
        cutoff_time = datetime.utcnow() - timedelta(days=max_age_days)
        cleared_count = 0

        # æ¸…ç†MongoDB
        if self.mongodb_db is not None:
            try:
                for collection_name in ["stock_data", "news_data", "fundamentals_data"]:
                    collection = self.mongodb_db[collection_name]
                    result = collection.delete_many({"created_at": {"$lt": cutoff_time}})
                    cleared_count += result.deleted_count
                    logger.info(f"ğŸ§¹ MongoDB {collection_name} æ¸…ç†äº† {result.deleted_count} æ¢è¨˜éŒ„")
            except Exception as e:
                logger.error(f"âš ï¸ MongoDBæ¸…ç†å¤±è´¥: {e}")

        # Redisæœƒè‡ªå‹•éæœŸï¼Œä¸éœ€è¦æ‰‹å‹•æ¸…ç†
        logger.info(f"ğŸ§¹ æ€»å…±æ¸…ç†äº† {cleared_count} æ¢éæœŸè¨˜éŒ„")
        return cleared_count

    def close(self):
        """é—œé—­æ•¸æ“šåº«é€£æ¥"""
        if self.mongodb_client:
            self.mongodb_client.close()
            logger.info(f"ğŸ”’ MongoDBé€£æ¥å·²é—œé—­")

        if self.redis_client:
            self.redis_client.close()
            logger.info(f"ğŸ”’ Redisé€£æ¥å·²é—œé—­")


# å…¨å±€æ•¸æ“šåº«ç·©å­˜å¯¦ä¾‹
_db_cache_instance = None

def get_db_cache() -> DatabaseCacheManager:
    """ç²å–å…¨å±€æ•¸æ“šåº«ç·©å­˜å¯¦ä¾‹"""
    global _db_cache_instance
    if _db_cache_instance is None:
        _db_cache_instance = DatabaseCacheManager()
    return _db_cache_instance
