#!/usr/bin/env python3
"""
å¯¦æ™‚æ–°èæ•¸æ“šç²å–å·¥å…·
è§£æ±ºæ–°èæ»¯å¾Œæ€§å•é¡Œ
"""

import requests
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import time
import os
from dataclasses import dataclass

# å°å…¥æ—¥èªŒæ¨¡å¡Š
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('agents')



@dataclass
class NewsItem:
    """æ–°èé …ç›®æ•¸æ“šçµæ§‹"""
    title: str
    content: str
    source: str
    publish_time: datetime
    url: str
    urgency: str  # high, medium, low
    relevance_score: float


class RealtimeNewsAggregator:
    """å¯¦æ™‚æ–°èèšåˆå™¨"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'TradingAgents-CN/1.0'
        }
        
        # APIå¯†é‘°é…ç½®
        self.finnhub_key = os.getenv('FINNHUB_API_KEY')
        self.alpha_vantage_key = os.getenv('ALPHA_VANTAGE_API_KEY')
        self.newsapi_key = os.getenv('NEWSAPI_KEY')
        
    def get_realtime_stock_news(self, ticker: str, hours_back: int = 6, max_news: int = 10) -> List[NewsItem]:
        """
        ç²å–å¯¦æ™‚è‚¡ç¥¨æ–°è
        å„ªå…ˆç´šï¼šå°ˆæ¥­API > æ–°èAPI > æœç´¢å¼•æ“
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç¢¼
            hours_back: å›æº¯å°æ™‚æ•¸
            max_news: æœ€å¤§æ–°èæ•¸é‡ï¼Œé»˜èª10æ¢
        """
        logger.info(f"[æ–°èèšåˆå™¨] é–‹å§‹ç²å– {ticker} çš„å¯¦æ™‚æ–°èï¼Œå›æº¯æ™‚é–“: {hours_back}å°æ™‚")
        start_time = datetime.now()
        all_news = []
        
        # 1. FinnHubå¯¦æ™‚æ–°è (æœ€é«˜å„ªå…ˆç´š)
        logger.info(f"[æ–°èèšåˆå™¨] å˜—è©¦å¾ FinnHub ç²å– {ticker} çš„æ–°è")
        finnhub_start = datetime.now()
        finnhub_news = self._get_finnhub_realtime_news(ticker, hours_back)
        finnhub_time = (datetime.now() - finnhub_start).total_seconds()
        
        if finnhub_news:
            logger.info(f"[æ–°èèšåˆå™¨] æˆåŠŸå¾ FinnHub ç²å– {len(finnhub_news)} æ¢æ–°èï¼Œè€—æ™‚: {finnhub_time:.2f}ç§’")
        else:
            logger.info(f"[æ–°èèšåˆå™¨] FinnHub æœªè¿”å›æ–°èï¼Œè€—æ™‚: {finnhub_time:.2f}ç§’")
            
        all_news.extend(finnhub_news)
        
        # 2. Alpha Vantageæ–°è
        logger.info(f"[æ–°èèšåˆå™¨] å˜—è©¦å¾ Alpha Vantage ç²å– {ticker} çš„æ–°è")
        av_start = datetime.now()
        av_news = self._get_alpha_vantage_news(ticker, hours_back)
        av_time = (datetime.now() - av_start).total_seconds()
        
        if av_news:
            logger.info(f"[æ–°èèšåˆå™¨] æˆåŠŸå¾ Alpha Vantage ç²å– {len(av_news)} æ¢æ–°èï¼Œè€—æ™‚: {av_time:.2f}ç§’")
        else:
            logger.info(f"[æ–°èèšåˆå™¨] Alpha Vantage æœªè¿”å›æ–°èï¼Œè€—æ™‚: {av_time:.2f}ç§’")
            
        all_news.extend(av_news)
        
        # 3. NewsAPI (å¦‚æœé…ç½®äº†)
        if self.newsapi_key:
            logger.info(f"[æ–°èèšåˆå™¨] å˜—è©¦å¾ NewsAPI ç²å– {ticker} çš„æ–°è")
            newsapi_start = datetime.now()
            newsapi_news = self._get_newsapi_news(ticker, hours_back)
            newsapi_time = (datetime.now() - newsapi_start).total_seconds()
            
            if newsapi_news:
                logger.info(f"[æ–°èèšåˆå™¨] æˆåŠŸå¾ NewsAPI ç²å– {len(newsapi_news)} æ¢æ–°èï¼Œè€—æ™‚: {newsapi_time:.2f}ç§’")
            else:
                logger.info(f"[æ–°èèšåˆå™¨] NewsAPI æœªè¿”å›æ–°èï¼Œè€—æ™‚: {newsapi_time:.2f}ç§’")
                
            all_news.extend(newsapi_news)
        else:
            logger.info(f"[æ–°èèšåˆå™¨] NewsAPI å¯†é‘°æœªé…ç½®ï¼Œè·³éæ­¤æ–°èæº")
        
        # 4. ä¸­æ–‡è²¡ç¶“æ–°èæº
        logger.info(f"[æ–°èèšåˆå™¨] å˜—è©¦ç²å– {ticker} çš„ä¸­æ–‡è²¡ç¶“æ–°è")
        chinese_start = datetime.now()
        chinese_news = self._get_chinese_finance_news(ticker, hours_back)
        chinese_time = (datetime.now() - chinese_start).total_seconds()
        
        if chinese_news:
            logger.info(f"[æ–°èèšåˆå™¨] æˆåŠŸç²å– {len(chinese_news)} æ¢ä¸­æ–‡è²¡ç¶“æ–°èï¼Œè€—æ™‚: {chinese_time:.2f}ç§’")
        else:
            logger.info(f"[æ–°èèšåˆå™¨] æœªç²å–åˆ°ä¸­æ–‡è²¡ç¶“æ–°èï¼Œè€—æ™‚: {chinese_time:.2f}ç§’")
            
        all_news.extend(chinese_news)
        
        # å»é‡å’Œæ’åº
        logger.info(f"[æ–°èèšåˆå™¨] é–‹å§‹å° {len(all_news)} æ¢æ–°èé€²è¡Œå»é‡å’Œæ’åº")
        dedup_start = datetime.now()
        unique_news = self._deduplicate_news(all_news)
        sorted_news = sorted(unique_news, key=lambda x: x.publish_time, reverse=True)
        dedup_time = (datetime.now() - dedup_start).total_seconds()
        
        # è¨˜éŒ„å»é‡çµæœ
        removed_count = len(all_news) - len(unique_news)
        logger.info(f"[æ–°èèšåˆå™¨] æ–°èå»é‡å®Œæˆï¼Œç§»é™¤äº† {removed_count} æ¢é‡è¤‡æ–°èï¼Œå‰©é¤˜ {len(sorted_news)} æ¢ï¼Œè€—æ™‚: {dedup_time:.2f}ç§’")
        
        # è¨˜éŒ„ç¸½é«”æƒ…æ³
        total_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"[æ–°èèšåˆå™¨] {ticker} çš„æ–°èèšåˆå®Œæˆï¼Œç¸½å…±ç²å– {len(sorted_news)} æ¢æ–°èï¼Œç¸½è€—æ™‚: {total_time:.2f}ç§’")
        
        # é™åˆ¶æ–°èæ•¸é‡ç‚ºæœ€æ–°çš„max_newsæ¢
        if len(sorted_news) > max_news:
            original_count = len(sorted_news)
            sorted_news = sorted_news[:max_news]
            logger.info(f"[æ–°èèšåˆå™¨] ğŸ“° æ–°èæ•¸é‡é™åˆ¶: å¾{original_count}æ¢é™åˆ¶ç‚º{max_news}æ¢æœ€æ–°æ–°è")
        
        # è¨˜éŒ„ä¸€äº›æ–°èæ¨™é¡Œç¤ºä¾‹
        if sorted_news:
            sample_titles = [item.title for item in sorted_news[:3]]
            logger.info(f"[æ–°èèšåˆå™¨] æ–°èæ¨™é¡Œç¤ºä¾‹: {', '.join(sample_titles)}")
        
        return sorted_news
    
    def _get_finnhub_realtime_news(self, ticker: str, hours_back: int) -> List[NewsItem]:
        """ç²å–FinnHubå¯¦æ™‚æ–°è"""
        if not self.finnhub_key:
            return []
        
        try:
            # è¨ˆç®—æ™‚é–“ç¯„åœ
            end_time = datetime.now()
            start_time = end_time - timedelta(hours=hours_back)
            
            # FinnHub APIèª¿ç”¨
            url = "https://finnhub.io/api/v1/company-news"
            params = {
                'symbol': ticker,
                'from': start_time.strftime('%Y-%m-%d'),
                'to': end_time.strftime('%Y-%m-%d'),
                'token': self.finnhub_key
            }
            
            response = requests.get(url, params=params, headers=self.headers)
            response.raise_for_status()
            
            news_data = response.json()
            news_items = []
            
            for item in news_data:
                # æª¢æŸ¥æ–°èæ™‚æ•ˆæ€§
                publish_time = datetime.fromtimestamp(item.get('datetime', 0))
                if publish_time < start_time:
                    continue
                
                # è©•ä¼°ç·Šæ€¥ç¨‹åº¦
                urgency = self._assess_news_urgency(item.get('headline', ''), item.get('summary', ''))
                
                news_items.append(NewsItem(
                    title=item.get('headline', ''),
                    content=item.get('summary', ''),
                    source=item.get('source', 'FinnHub'),
                    publish_time=publish_time,
                    url=item.get('url', ''),
                    urgency=urgency,
                    relevance_score=self._calculate_relevance(item.get('headline', ''), ticker)
                ))
            
            return news_items
            
        except Exception as e:
            logger.error(f"FinnHubæ–°èç²å–å¤±æ•—: {e}")
            return []
    
    def _get_alpha_vantage_news(self, ticker: str, hours_back: int) -> List[NewsItem]:
        """ç²å–Alpha Vantageæ–°è"""
        if not self.alpha_vantage_key:
            return []
        
        try:
            url = "https://www.alphavantage.co/query"
            params = {
                'function': 'NEWS_SENTIMENT',
                'tickers': ticker,
                'apikey': self.alpha_vantage_key,
                'limit': 50
            }
            
            response = requests.get(url, params=params, headers=self.headers)
            response.raise_for_status()
            
            data = response.json()
            news_items = []
            
            if 'feed' in data:
                for item in data['feed']:
                    # è§£ææ™‚é–“
                    time_str = item.get('time_published', '')
                    try:
                        publish_time = datetime.strptime(time_str, '%Y%m%dT%H%M%S')
                    except:
                        continue
                    
                    # æª¢æŸ¥æ™‚æ•ˆæ€§
                    if publish_time < datetime.now() - timedelta(hours=hours_back):
                        continue
                    
                    urgency = self._assess_news_urgency(item.get('title', ''), item.get('summary', ''))
                    
                    news_items.append(NewsItem(
                        title=item.get('title', ''),
                        content=item.get('summary', ''),
                        source=item.get('source', 'Alpha Vantage'),
                        publish_time=publish_time,
                        url=item.get('url', ''),
                        urgency=urgency,
                        relevance_score=self._calculate_relevance(item.get('title', ''), ticker)
                    ))
            
            return news_items
            
        except Exception as e:
            logger.error(f"Alpha Vantageæ–°èç²å–å¤±æ•—: {e}")
            return []
    
    def _get_newsapi_news(self, ticker: str, hours_back: int) -> List[NewsItem]:
        """ç²å–NewsAPIæ–°è"""
        try:
            # æ§‹å»ºæœç´¢æŸ¥è©¢
            company_names = {
                'AAPL': 'Apple',
                'TSLA': 'Tesla', 
                'NVDA': 'NVIDIA',
                'MSFT': 'Microsoft',
                'GOOGL': 'Google'
            }
            
            query = f"{ticker} OR {company_names.get(ticker, ticker)}"
            
            url = "https://newsapi.org/v2/everything"
            params = {
                'q': query,
                'language': 'en',
                'sortBy': 'publishedAt',
                'from': (datetime.now() - timedelta(hours=hours_back)).isoformat(),
                'apiKey': self.newsapi_key
            }
            
            response = requests.get(url, params=params, headers=self.headers)
            response.raise_for_status()
            
            data = response.json()
            news_items = []
            
            for item in data.get('articles', []):
                # è§£ææ™‚é–“
                time_str = item.get('publishedAt', '')
                try:
                    publish_time = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
                except:
                    continue
                
                urgency = self._assess_news_urgency(item.get('title', ''), item.get('description', ''))
                
                news_items.append(NewsItem(
                    title=item.get('title', ''),
                    content=item.get('description', ''),
                    source=item.get('source', {}).get('name', 'NewsAPI'),
                    publish_time=publish_time,
                    url=item.get('url', ''),
                    urgency=urgency,
                    relevance_score=self._calculate_relevance(item.get('title', ''), ticker)
                ))
            
            return news_items
            
        except Exception as e:
            logger.error(f"NewsAPIæ–°èç²å–å¤±æ•—: {e}")
            return []
    
    def _get_chinese_finance_news(self, ticker: str, hours_back: int) -> List[NewsItem]:
        """ç²å–è²¡ç¶“æ–°èï¼ˆå·²ç§»é™¤ä¸­åœ‹æ•¸æ“šæºä¾è³´ï¼‰"""
        logger.info(f"[è²¡ç¶“æ–°è] é–‹å§‹ç²å– {ticker} çš„è²¡ç¶“æ–°èï¼Œå›æº¯æ™‚é–“: {hours_back}å°æ™‚")
        start_time = datetime.now()

        try:
            news_items = []

            # è²¡è¯ç¤¾RSS (å¦‚æœå¯ç”¨)
            logger.info(f"[ä¸­æ–‡è²¡ç¶“æ–°è] é–‹å§‹ç²å–è²¡è¯ç¤¾RSSæ–°è")
            rss_start_time = datetime.now()
            rss_sources = [
                "https://www.cls.cn/api/sw?app=CailianpressWeb&os=web&sv=7.7.5",
                # å¯ä»¥æ·»åŠ æ›´å¤šRSSæº
            ]
            
            rss_success_count = 0
            rss_error_count = 0
            total_rss_items = 0
            
            for rss_url in rss_sources:
                try:
                    logger.info(f"[ä¸­æ–‡è²¡ç¶“æ–°è] å˜—è©¦è§£æRSSæº: {rss_url}")
                    rss_item_start = datetime.now()
                    items = self._parse_rss_feed(rss_url, ticker, hours_back)
                    rss_item_time = (datetime.now() - rss_item_start).total_seconds()
                    
                    if items:
                        logger.info(f"[ä¸­æ–‡è²¡ç¶“æ–°è] æˆåŠŸå¾RSSæºç²å– {len(items)} æ¢æ–°èï¼Œè€—æ™‚: {rss_item_time:.2f}ç§’")
                        news_items.extend(items)
                        total_rss_items += len(items)
                        rss_success_count += 1
                    else:
                        logger.info(f"[ä¸­æ–‡è²¡ç¶“æ–°è] RSSæºæœªè¿”å›ç›¸é—œæ–°èï¼Œè€—æ™‚: {rss_item_time:.2f}ç§’")
                except Exception as rss_e:
                    logger.error(f"[ä¸­æ–‡è²¡ç¶“æ–°è] è§£æRSSæºå¤±æ•—: {rss_e}")
                    rss_error_count += 1
                    continue
            
            # è¨˜éŒ„RSSç²å–ç¸½çµ
            rss_total_time = (datetime.now() - rss_start_time).total_seconds()
            logger.info(f"[ä¸­æ–‡è²¡ç¶“æ–°è] RSSæ–°èç²å–å®Œæˆï¼ŒæˆåŠŸæº: {rss_success_count}å€‹ï¼Œå¤±æ•—æº: {rss_error_count}å€‹ï¼Œç²å–æ–°è: {total_rss_items}æ¢ï¼Œç¸½è€—æ™‚: {rss_total_time:.2f}ç§’")
            
            # è¨˜éŒ„ä¸­æ–‡è²¡ç¶“æ–°èç²å–ç¸½çµ
            total_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"[ä¸­æ–‡è²¡ç¶“æ–°è] {ticker} çš„ä¸­æ–‡è²¡ç¶“æ–°èç²å–å®Œæˆï¼Œç¸½å…±ç²å– {len(news_items)} æ¢æ–°èï¼Œç¸½è€—æ™‚: {total_time:.2f}ç§’")
            
            return news_items
            
        except Exception as e:
            logger.error(f"[ä¸­æ–‡è²¡ç¶“æ–°è] ä¸­æ–‡è²¡ç¶“æ–°èç²å–å¤±æ•—: {e}")
            return []
    
    def _parse_rss_feed(self, rss_url: str, ticker: str, hours_back: int) -> List[NewsItem]:
        """è§£æRSSæº"""
        logger.info(f"[RSSè§£æ] é–‹å§‹è§£æRSSæº: {rss_url}ï¼Œè‚¡ç¥¨: {ticker}ï¼Œå›æº¯æ™‚é–“: {hours_back}å°æ™‚")
        start_time = datetime.now()
        
        try:
            # å¯¦éš›å¯¦ç¾éœ€è¦ä½¿ç”¨feedparseråº«
            # é€™è£¡æ˜¯ç°¡åŒ–å¯¦ç¾ï¼Œå¯¦éš›é …ç›®ä¸­æ‡‰è©²æ›¿æ›ç‚ºçœŸå¯¦çš„RSSè§£æé‚è¼¯
            import feedparser
            
            logger.info(f"[RSSè§£æ] å˜—è©¦ç²å–RSSæºå…§å®¹")
            feed = feedparser.parse(rss_url)
            
            if not feed or not feed.entries:
                logger.warning(f"[RSSè§£æ] RSSæºæœªè¿”å›æœ‰æ•ˆå…§å®¹")
                return []
            
            logger.info(f"[RSSè§£æ] æˆåŠŸç²å–RSSæºï¼ŒåŒ…å« {len(feed.entries)} æ¢æ¢ç›®")
            news_items = []
            processed_count = 0
            skipped_count = 0
            
            for entry in feed.entries:
                try:
                    # è§£ææ™‚é–“
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        publish_time = datetime.fromtimestamp(time.mktime(entry.published_parsed))
                    else:
                        logger.warning(f"[RSSè§£æ] æ¢ç›®ç¼ºå°‘ç™¼å¸ƒæ™‚é–“ï¼Œä½¿ç”¨ç•¶å‰æ™‚é–“")
                        publish_time = datetime.now()
                    
                    # æª¢æŸ¥æ™‚æ•ˆæ€§
                    if publish_time < datetime.now() - timedelta(hours=hours_back):
                        skipped_count += 1
                        continue
                    
                    title = entry.title if hasattr(entry, 'title') else ''
                    content = entry.description if hasattr(entry, 'description') else ''
                    
                    # æª¢æŸ¥ç›¸é—œæ€§
                    if ticker.lower() not in title.lower() and ticker.lower() not in content.lower():
                        skipped_count += 1
                        continue
                    
                    # è©•ä¼°ç·Šæ€¥ç¨‹åº¦
                    urgency = self._assess_news_urgency(title, content)
                    
                    news_items.append(NewsItem(
                        title=title,
                        content=content,
                        source='è²¡è¯ç¤¾',
                        publish_time=publish_time,
                        url=entry.link if hasattr(entry, 'link') else '',
                        urgency=urgency,
                        relevance_score=self._calculate_relevance(title, ticker)
                    ))
                    processed_count += 1
                except Exception as e:
                    logger.error(f"[RSSè§£æ] è™•ç†RSSæ¢ç›®å¤±æ•—: {e}")
                    continue
            
            total_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"[RSSè§£æ] RSSæºè§£æå®Œæˆï¼ŒæˆåŠŸ: {processed_count}æ¢ï¼Œè·³é: {skipped_count}æ¢ï¼Œè€—æ™‚: {total_time:.2f}ç§’")
            return news_items
        except ImportError:
            logger.error(f"[RSSè§£æ] feedparseråº«æœªå®‰è£ï¼Œç„¡æ³•è§£æRSSæº")
            return []
        except Exception as e:
            logger.error(f"[RSSè§£æ] è§£æRSSæºå¤±æ•—: {e}")
            return []
    
    def _assess_news_urgency(self, title: str, content: str) -> str:
        """è©•ä¼°æ–°èç·Šæ€¥ç¨‹åº¦"""
        text = (title + ' ' + content).lower()
        
        # é«˜ç·Šæ€¥åº¦é—œéµè©
        high_urgency_keywords = [
            'breaking', 'urgent', 'alert', 'emergency', 'halt', 'suspend',
            'çªç™¼', 'ç·Šæ€¥', 'æš«åœ', 'åœç‰Œ', 'é‡å¤§'
        ]
        
        # ä¸­ç­‰ç·Šæ€¥åº¦é—œéµè©
        medium_urgency_keywords = [
            'earnings', 'report', 'announce', 'launch', 'merger', 'acquisition',
            'è²¡å ±', 'ç™¼å¸ƒ', 'å®£å¸ƒ', 'ä¸¦è³¼', 'æ”¶è³¼'
        ]
        
        # æª¢æŸ¥é«˜ç·Šæ€¥åº¦é—œéµè©
        for keyword in high_urgency_keywords:
            if keyword in text:
                logger.debug(f"[ç·Šæ€¥åº¦è©•ä¼°] æª¢æ¸¬åˆ°é«˜ç·Šæ€¥åº¦é—œéµè© '{keyword}' åœ¨æ–°èä¸­: {title[:50]}...")
                return 'high'
        
        # æª¢æŸ¥ä¸­ç­‰ç·Šæ€¥åº¦é—œéµè©
        for keyword in medium_urgency_keywords:
            if keyword in text:
                logger.debug(f"[ç·Šæ€¥åº¦è©•ä¼°] æª¢æ¸¬åˆ°ä¸­ç­‰ç·Šæ€¥åº¦é—œéµè© '{keyword}' åœ¨æ–°èä¸­: {title[:50]}...")
                return 'medium'
        
        logger.debug(f"[ç·Šæ€¥åº¦è©•ä¼°] æœªæª¢æ¸¬åˆ°ç·Šæ€¥é—œéµè©ï¼Œè©•ä¼°ç‚ºä½ç·Šæ€¥åº¦: {title[:50]}...")
        return 'low'
    
    def _calculate_relevance(self, title: str, ticker: str) -> float:
        """è¨ˆç®—æ–°èç›¸é—œæ€§åˆ†æ•¸"""
        text = title.lower()
        ticker_lower = ticker.lower()
        
        # åŸºç¤ç›¸é—œæ€§ - è‚¡ç¥¨ä»£ç¢¼ç›´æ¥å‡ºç¾åœ¨æ¨™é¡Œä¸­
        if ticker_lower in text:
            logger.debug(f"[ç›¸é—œæ€§è¨ˆç®—] è‚¡ç¥¨ä»£ç¢¼ {ticker} ç›´æ¥å‡ºç¾åœ¨æ¨™é¡Œä¸­ï¼Œç›¸é—œæ€§è©•åˆ†: 1.0ï¼Œæ¨™é¡Œ: {title[:50]}...")
            return 1.0
        
        # å…¬å¸åç¨±åŒ¹é…
        company_names = {
            'aapl': ['apple', 'iphone', 'ipad', 'mac'],
            'tsla': ['tesla', 'elon musk', 'electric vehicle'],
            'nvda': ['nvidia', 'gpu', 'ai chip'],
            'msft': ['microsoft', 'windows', 'azure'],
            'googl': ['google', 'alphabet', 'search']
        }
        
        # æª¢æŸ¥å…¬å¸ç›¸é—œé—œéµè©
        if ticker_lower in company_names:
            for name in company_names[ticker_lower]:
                if name in text:
                    logger.debug(f"[ç›¸é—œæ€§è¨ˆç®—] æª¢æ¸¬åˆ°å…¬å¸ç›¸é—œé—œéµè© '{name}' åœ¨æ¨™é¡Œä¸­ï¼Œç›¸é—œæ€§è©•åˆ†: 0.8ï¼Œæ¨™é¡Œ: {title[:50]}...")
                    return 0.8
        
        # æå–è‚¡ç¥¨ä»£ç¢¼çš„ç´”æ•¸å­—éƒ¨åˆ†ï¼ˆé©ç”¨æ–¼ä¸­åœ‹è‚¡ç¥¨ï¼‰
        pure_code = ''.join(filter(str.isdigit, ticker))
        if pure_code and pure_code in text:
            logger.debug(f"[ç›¸é—œæ€§è¨ˆç®—] è‚¡ç¥¨ä»£ç¢¼æ•¸å­—éƒ¨åˆ† {pure_code} å‡ºç¾åœ¨æ¨™é¡Œä¸­ï¼Œç›¸é—œæ€§è©•åˆ†: 0.9ï¼Œæ¨™é¡Œ: {title[:50]}...")
            return 0.9
        
        logger.debug(f"[ç›¸é—œæ€§è¨ˆç®—] æœªæª¢æ¸¬åˆ°æ˜ç¢ºç›¸é—œæ€§ï¼Œä½¿ç”¨é»˜èªè©•åˆ†: 0.3ï¼Œæ¨™é¡Œ: {title[:50]}...")
        return 0.3  # é»˜èªç›¸é—œæ€§
    
    def _deduplicate_news(self, news_items: List[NewsItem]) -> List[NewsItem]:
        """å»é‡æ–°è"""
        logger.info(f"[æ–°èå»é‡] é–‹å§‹å° {len(news_items)} æ¢æ–°èé€²è¡Œå»é‡è™•ç†")
        start_time = datetime.now()
        
        seen_titles = set()
        unique_news = []
        duplicate_count = 0
        short_title_count = 0
        
        for item in news_items:
            # ç°¡å–®çš„æ¨™é¡Œå»é‡
            title_key = item.title.lower().strip()
            
            # æª¢æŸ¥æ¨™é¡Œé•·åº¦
            if len(title_key) <= 10:
                logger.debug(f"[æ–°èå»é‡] è·³éæ¨™é¡ŒéçŸ­çš„æ–°è: '{item.title}'ï¼Œä¾†æº: {item.source}")
                short_title_count += 1
                continue
                
            # æª¢æŸ¥æ˜¯å¦é‡è¤‡
            if title_key in seen_titles:
                logger.debug(f"[æ–°èå»é‡] æª¢æ¸¬åˆ°é‡è¤‡æ–°è: '{item.title[:50]}...'ï¼Œä¾†æº: {item.source}")
                duplicate_count += 1
                continue
                
            # æ·»åŠ åˆ°çµæœé›†
            seen_titles.add(title_key)
            unique_news.append(item)
        
        # è¨˜éŒ„å»é‡çµæœ
        time_taken = (datetime.now() - start_time).total_seconds()
        logger.info(f"[æ–°èå»é‡] å»é‡å®Œæˆï¼ŒåŸå§‹æ–°è: {len(news_items)}æ¢ï¼Œå»é‡å¾Œ: {len(unique_news)}æ¢ï¼Œ")
        logger.info(f"[æ–°èå»é‡] å»é™¤é‡è¤‡: {duplicate_count}æ¢ï¼Œæ¨™é¡ŒéçŸ­: {short_title_count}æ¢ï¼Œè€—æ™‚: {time_taken:.2f}ç§’")
        
        return unique_news
    
    def format_news_report(self, news_items: List[NewsItem], ticker: str) -> str:
        """æ ¼å¼åŒ–æ–°èå ±å‘Š"""
        logger.info(f"[æ–°èå ±å‘Š] é–‹å§‹ç‚º {ticker} ç”Ÿæˆæ–°èå ±å‘Š")
        start_time = datetime.now()
        
        if not news_items:
            logger.warning(f"[æ–°èå ±å‘Š] æœªç²å–åˆ° {ticker} çš„å¯¦æ™‚æ–°èæ•¸æ“š")
            return f"æœªç²å–åˆ°{ticker}çš„å¯¦æ™‚æ–°èæ•¸æ“šã€‚"
        
        # æŒ‰ç·Šæ€¥ç¨‹åº¦åˆ†çµ„
        high_urgency = [n for n in news_items if n.urgency == 'high']
        medium_urgency = [n for n in news_items if n.urgency == 'medium']
        low_urgency = [n for n in news_items if n.urgency == 'low']
        
        # è¨˜éŒ„æ–°èåˆ†é¡æƒ…æ³
        logger.info(f"[æ–°èå ±å‘Š] {ticker} æ–°èåˆ†é¡çµ±è¨ˆ: é«˜ç·Šæ€¥åº¦ {len(high_urgency)}æ¢, ä¸­ç·Šæ€¥åº¦ {len(medium_urgency)}æ¢, ä½ç·Šæ€¥åº¦ {len(low_urgency)}æ¢")
        
        # è¨˜éŒ„æ–°èä¾†æºåˆ†å¸ƒ
        news_sources = {}
        for item in news_items:
            source = item.source
            if source in news_sources:
                news_sources[source] += 1
            else:
                news_sources[source] = 1
        
        sources_info = ", ".join([f"{source}: {count}æ¢" for source, count in news_sources.items()])
        logger.info(f"[æ–°èå ±å‘Š] {ticker} æ–°èä¾†æºåˆ†å¸ƒ: {sources_info}")
        
        report = f"# {ticker} å¯¦æ™‚æ–°èåˆ†æå ±å‘Š\n\n"
        report += f"ğŸ“… ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        report += f"ğŸ“Š æ–°èç¸½æ•¸: {len(news_items)}æ¢\n\n"
        
        if high_urgency:
            report += "## ğŸš¨ ç·Šæ€¥æ–°è\n\n"
            for news in high_urgency[:3]:  # æœ€å¤šé¡¯ç¤º3æ¢
                report += f"### {news.title}\n"
                report += f"**ä¾†æº**: {news.source} | **æ™‚é–“**: {news.publish_time.strftime('%H:%M')}\n"
                report += f"{news.content}\n\n"
        
        if medium_urgency:
            report += "## ğŸ“¢ é‡è¦æ–°è\n\n"
            for news in medium_urgency[:5]:  # æœ€å¤šé¡¯ç¤º5æ¢
                report += f"### {news.title}\n"
                report += f"**ä¾†æº**: {news.source} | **æ™‚é–“**: {news.publish_time.strftime('%H:%M')}\n"
                report += f"{news.content}\n\n"
        
        # æ·»åŠ æ™‚æ•ˆæ€§èªªæ˜
        latest_news = max(news_items, key=lambda x: x.publish_time)
        time_diff = datetime.now() - latest_news.publish_time
        
        report += f"\n## â° æ•¸æ“šæ™‚æ•ˆæ€§\n"
        report += f"æœ€æ–°æ–°èç™¼å¸ƒæ–¼: {time_diff.total_seconds() / 60:.0f}åˆ†é˜å‰\n"
        
        if time_diff.total_seconds() < 1800:  # 30åˆ†é˜å…§
            report += "ğŸŸ¢ æ•¸æ“šæ™‚æ•ˆæ€§: å„ªç§€ (30åˆ†é˜å…§)\n"
        elif time_diff.total_seconds() < 3600:  # 1å°æ™‚å…§
            report += "ğŸŸ¡ æ•¸æ“šæ™‚æ•ˆæ€§: è‰¯å¥½ (1å°æ™‚å…§)\n"
        else:
            report += "ğŸ”´ æ•¸æ“šæ™‚æ•ˆæ€§: ä¸€èˆ¬ (è¶…é1å°æ™‚)\n"
        
        # è¨˜éŒ„å ±å‘Šç”Ÿæˆå®Œæˆä¿¡æ¯
        end_time = datetime.now()
        time_taken = (end_time - start_time).total_seconds()
        report_length = len(report)
        
        logger.info(f"[æ–°èå ±å‘Š] {ticker} æ–°èå ±å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: {time_taken:.2f}ç§’ï¼Œå ±å‘Šé•·åº¦: {report_length}å­—ç¬¦")
        
        # è¨˜éŒ„æ™‚æ•ˆæ€§ä¿¡æ¯
        time_diff_minutes = time_diff.total_seconds() / 60
        logger.info(f"[æ–°èå ±å‘Š] {ticker} æ–°èæ™‚æ•ˆæ€§: æœ€æ–°æ–°èç™¼å¸ƒæ–¼ {time_diff_minutes:.1f}åˆ†é˜å‰")
        
        return report


def get_realtime_stock_news(ticker: str, curr_date: str, hours_back: int = 6) -> str:
    """
    ç²å–å¯¦æ™‚è‚¡ç¥¨æ–°èçš„ä¸»è¦æ¥å£å‡½æ•¸
    """
    logger.info(f"[æ–°èåˆ†æ] ========== å‡½æ•¸å…¥å£ ==========")
    logger.info(f"[æ–°èåˆ†æ] å‡½æ•¸: get_realtime_stock_news")
    logger.info(f"[æ–°èåˆ†æ] åƒæ•¸: ticker={ticker}, curr_date={curr_date}, hours_back={hours_back}")
    logger.info(f"[æ–°èåˆ†æ] é–‹å§‹ç²å– {ticker} çš„å¯¦æ™‚æ–°èï¼Œæ—¥æœŸ: {curr_date}, å›æº¯æ™‚é–“: {hours_back}å°æ™‚")
    start_total_time = datetime.now()
    logger.info(f"[æ–°èåˆ†æ] é–‹å§‹æ™‚é–“: {start_total_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
    
    # è‚¡ç¥¨é¡å‹åˆ¤æ–· - çµ±ä¸€è¦–ç‚ºç¾è‚¡
    logger.info(f"[æ–°èåˆ†æ] ========== æ­¥é©Ÿ1: è‚¡ç¥¨é¡å‹åˆ¤æ–· ==========")
    stock_type = "ç¾è‚¡"
    is_china_stock = False
    logger.info(f"[æ–°èåˆ†æ] åŸå§‹ticker: {ticker}")
    logger.info(f"[æ–°èåˆ†æ] æœ€çµ‚åˆ¤æ–·çµæœ - è‚¡ç¥¨ {ticker} é¡å‹: {stock_type}")
    
    # ä½¿ç”¨å¯¦æ™‚æ–°èèšåˆå™¨
    logger.info(f"[æ–°èåˆ†æ] ========== æ­¥é©Ÿ3: å¯¦æ™‚æ–°èèšåˆå™¨ ==========")
    aggregator = RealtimeNewsAggregator()
    logger.info(f"[æ–°èåˆ†æ] æˆåŠŸå‰µå»ºå¯¦æ™‚æ–°èèšåˆå™¨å¯¦ä¾‹")
    try:
        logger.info(f"[æ–°èåˆ†æ] å˜—è©¦ä½¿ç”¨å¯¦æ™‚æ–°èèšåˆå™¨ç²å– {ticker} çš„æ–°è")
        start_time = datetime.now()
        logger.info(f"[æ–°èåˆ†æ] èšåˆå™¨èª¿ç”¨é–‹å§‹æ™‚é–“: {start_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
        
        # ç²å–å¯¦æ™‚æ–°è
        news_items = aggregator.get_realtime_stock_news(ticker, hours_back, max_news=10)
        
        end_time = datetime.now()
        time_taken = (end_time - start_time).total_seconds()
        logger.info(f"[æ–°èåˆ†æ] èšåˆå™¨èª¿ç”¨çµæŸæ™‚é–“: {end_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
        logger.info(f"[æ–°èåˆ†æ] èšåˆå™¨èª¿ç”¨è€—æ™‚: {time_taken:.2f}ç§’")
        logger.info(f"[æ–°èåˆ†æ] èšåˆå™¨è¿”å›æ•¸æ“šé¡å‹: {type(news_items)}")
        logger.info(f"[æ–°èåˆ†æ] èšåˆå™¨è¿”å›æ•¸æ“š: {news_items}")
        
        # å¦‚æœæˆåŠŸç²å–åˆ°æ–°è
        if news_items and len(news_items) > 0:
            news_count = len(news_items)
            logger.info(f"[æ–°èåˆ†æ] å¯¦æ™‚æ–°èèšåˆå™¨æˆåŠŸç²å– {news_count} æ¢ {ticker} çš„æ–°èï¼Œè€—æ™‚ {time_taken:.2f} ç§’")
            
            # è¨˜éŒ„ä¸€äº›æ–°èæ¨™é¡Œç¤ºä¾‹
            sample_titles = [item.title for item in news_items[:3]]
            logger.info(f"[æ–°èåˆ†æ] æ–°èæ¨™é¡Œç¤ºä¾‹: {', '.join(sample_titles)}")
            
            # æ ¼å¼åŒ–å ±å‘Š
            logger.info(f"[æ–°èåˆ†æ] é–‹å§‹æ ¼å¼åŒ–æ–°èå ±å‘Š")
            report = aggregator.format_news_report(news_items, ticker)
            logger.info(f"[æ–°èåˆ†æ] å ±å‘Šæ ¼å¼åŒ–å®Œæˆï¼Œé•·åº¦: {len(report)} å­—ç¬¦")
            
            total_time_taken = (datetime.now() - start_total_time).total_seconds()
            logger.info(f"[æ–°èåˆ†æ] æˆåŠŸç”Ÿæˆ {ticker} çš„æ–°èå ±å‘Šï¼Œç¸½è€—æ™‚ {total_time_taken:.2f} ç§’ï¼Œæ–°èä¾†æº: å¯¦æ™‚æ–°èèšåˆå™¨")
            logger.info(f"[æ–°èåˆ†æ] ========== å¯¦æ™‚æ–°èèšåˆå™¨ç²å–æˆåŠŸï¼Œå‡½æ•¸å³å°‡è¿”å› ==========")
            return report
        else:
            logger.warning(f"[æ–°èåˆ†æ] å¯¦æ™‚æ–°èèšåˆå™¨æœªç²å–åˆ° {ticker} çš„æ–°èï¼Œè€—æ™‚ {time_taken:.2f} ç§’ï¼Œå˜—è©¦ä½¿ç”¨å‚™ç”¨æ–°èæº")
            # å¦‚æœæ²’æœ‰ç²å–åˆ°æ–°èï¼Œç¹¼çºŒå˜—è©¦å‚™ç”¨æ–¹æ¡ˆ
    except Exception as e:
        logger.error(f"[æ–°èåˆ†æ] å¯¦æ™‚æ–°èèšåˆå™¨ç²å–å¤±æ•—: {e}ï¼Œå°‡å˜—è©¦å‚™ç”¨æ–°èæº")
        logger.error(f"[æ–°èåˆ†æ] ç•°å¸¸è©³æƒ…: {type(e).__name__}: {str(e)}")
        import traceback
        logger.error(f"[æ–°èåˆ†æ] ç•°å¸¸å †æ£§: {traceback.format_exc()}")
        # ç™¼ç”Ÿç•°å¸¸æ™‚ï¼Œç¹¼çºŒå˜—è©¦å‚™ç”¨æ–¹æ¡ˆ
    
    # å‚™ç”¨æ–¹æ¡ˆï¼šå˜—è©¦ä½¿ç”¨ Google æ–°è
    try:
        from tradingagents.dataflows.interface import get_google_news
        
        # ç¾è‚¡ä½¿ç”¨è‹±æ–‡é—œéµè©é€²è¡Œæœç´¢
        search_query = f"{ticker} stock news"
        logger.info(f"[æ–°èåˆ†æ] é–‹å§‹å¾Googleç²å– {ticker} çš„æ–°èæ•¸æ“šï¼ŒæŸ¥è©¢: {search_query}")
        
        start_time = datetime.now()
        google_news = get_google_news(search_query, curr_date, 1)
        end_time = datetime.now()
        time_taken = (end_time - start_time).total_seconds()
        
        if google_news and len(google_news.strip()) > 0:
            # ä¼°ç®—ç²å–çš„æ–°èæ•¸é‡
            news_lines = google_news.strip().split('\n')
            news_count = sum(1 for line in news_lines if line.startswith('###'))
            
            logger.info(f"[æ–°èåˆ†æ] æˆåŠŸç²å– Google æ–°èï¼Œä¼°è¨ˆ {news_count} æ¢æ–°èï¼Œè€—æ™‚ {time_taken:.2f} ç§’")
            
            # è¨˜éŒ„ä¸€äº›æ–°èæ¨™é¡Œç¤ºä¾‹
            sample_titles = [line.replace('### ', '') for line in news_lines if line.startswith('### ')][:3]
            if sample_titles:
                logger.info(f"[æ–°èåˆ†æ] æ–°èæ¨™é¡Œç¤ºä¾‹: {', '.join(sample_titles)}")
                
            logger.info(f"[æ–°èåˆ†æ] æˆåŠŸç”Ÿæˆ Google æ–°èå ±å‘Šï¼Œæ–°èä¾†æº: Google")
            return google_news
        else:
            logger.warning(f"[æ–°èåˆ†æ] Google æ–°èæœªç²å–åˆ° {ticker} çš„æ–°èæ•¸æ“šï¼Œè€—æ™‚ {time_taken:.2f} ç§’")
    except Exception as e:
        logger.error(f"[æ–°èåˆ†æ] Google æ–°èç²å–å¤±æ•—: {e}ï¼Œæ‰€æœ‰å‚™ç”¨æ–¹æ¡ˆå‡å·²å˜—è©¦")
    
    # æ‰€æœ‰æ–¹æ³•éƒ½å¤±æ•—ï¼Œè¿”å›éŒ¯èª¤ä¿¡æ¯
    total_time_taken = (datetime.now() - start_total_time).total_seconds()
    logger.error(f"[æ–°èåˆ†æ] {ticker} çš„æ‰€æœ‰æ–°èç²å–æ–¹æ³•å‡å·²å¤±æ•—ï¼Œç¸½è€—æ™‚ {total_time_taken:.2f} ç§’")
    
    # è¨˜éŒ„è©³ç´°çš„å¤±æ•—ä¿¡æ¯
    failure_details = {
        "è‚¡ç¥¨ä»£ç¢¼": ticker,
        "è‚¡ç¥¨é¡å‹": stock_type,
        "åˆ†ææ—¥æœŸ": curr_date,
        "å›æº¯æ™‚é–“": f"{hours_back}å°æ™‚",
        "ç¸½è€—æ™‚": f"{total_time_taken:.2f}ç§’"
    }
    logger.error(f"[æ–°èåˆ†æ] æ–°èç²å–å¤±æ•—è©³æƒ…: {failure_details}")
    
    return f"""
å¯¦æ™‚æ–°èç²å–å¤±æ•— - {ticker}
åˆ†ææ—¥æœŸ: {curr_date}

âŒ éŒ¯èª¤ä¿¡æ¯: æ‰€æœ‰å¯ç”¨çš„æ–°èæºéƒ½æœªèƒ½ç²å–åˆ°ç›¸é—œæ–°è

ğŸ’¡ å‚™ç”¨å»ºè­°:
1. æª¢æŸ¥ç¶²çµ¡é€£æ¥å’ŒAPIå¯†é‘°é…ç½®
2. ä½¿ç”¨åŸºç¤æ–°èåˆ†æä½œç‚ºå‚™é¸
3. é—œè¨»å®˜æ–¹è²¡ç¶“åª’é«”çš„æœ€æ–°å ±é“
4. è€ƒæ…®ä½¿ç”¨å°ˆæ¥­é‡‘èçµ‚ç«¯ç²å–å¯¦æ™‚æ–°è

è¨»: å¯¦æ™‚æ–°èç²å–ä¾è³´å¤–éƒ¨APIæœå‹™çš„å¯ç”¨æ€§ã€‚
"""
