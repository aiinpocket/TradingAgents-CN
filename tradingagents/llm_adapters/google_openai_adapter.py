"""
Google AI OpenAIå…¼å®¹é©é…å™¨
ç‚º TradingAgents æä¾›Google AI (Gemini)æ¨¡å‹çš„ OpenAI å…¼å®¹æ¥å£
è§£æ±ºGoogleæ¨¡å‹å·¥å…·èª¿ç”¨æ ¼å¼ä¸åŒ¹é…çš„å•é¡Œ
"""

import os
from typing import Any, Dict, List, Optional, Union, Sequence
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.tools import BaseTool
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, SystemMessage
from langchain_core.outputs import LLMResult
from pydantic import Field, SecretStr
from ..config.config_manager import token_tracker

# å°å…¥æ—¥èªŒæ¨¡å¡Š
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('agents')


class ChatGoogleOpenAI(ChatGoogleGenerativeAI):
    """
    Google AI OpenAI å…¼å®¹é©é…å™¨
    ç¹¼æ‰¿ ChatGoogleGenerativeAIï¼Œå„ªåŒ–å·¥å…·èª¿ç”¨å’Œå…§å®¹æ ¼å¼è™•ç†
    è§£æ±ºGoogleæ¨¡å‹å·¥å…·èª¿ç”¨è¿”å›æ ¼å¼ä¸ç³»çµ±æœŸæœ›ä¸åŒ¹é…çš„å•é¡Œ
    """
    
    def __init__(self, **kwargs):
        """åˆå§‹åŒ– Google AI OpenAI å…¼å®¹å®¢æˆ¶ç«¯"""
        
        # è¨­ç½® Google AI çš„é»˜èªé…ç½®
        kwargs.setdefault("temperature", 0.1)
        kwargs.setdefault("max_tokens", 2000)
        
        # æª¢æŸ¥ API å¯†é‘°
        google_api_key = kwargs.get("google_api_key") or os.getenv("GOOGLE_API_KEY")
        if not google_api_key:
            raise ValueError(
                "Google API key not found. Please set GOOGLE_API_KEY environment variable "
                "or pass google_api_key parameter."
            )
        
        kwargs["google_api_key"] = google_api_key
        
        # èª¿ç”¨çˆ¶é¡åˆå§‹åŒ–
        super().__init__(**kwargs)

        logger.info(f"âœ… Google AI OpenAI å…¼å®¹é©é…å™¨åˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"   æ¨¡å‹: {kwargs.get('model', 'gemini-pro')}")
        logger.info(f"   æº«åº¦: {kwargs.get('temperature', 0.1)}")
        logger.info(f"   æœ€å¤§Token: {kwargs.get('max_tokens', 2000)}")
    
    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, **kwargs) -> LLMResult:
        """é‡å¯«ç”Ÿæˆæ–¹æ³•ï¼Œå„ªåŒ–å·¥å…·èª¿ç”¨è™•ç†å’Œå…§å®¹æ ¼å¼"""
        
        try:
            # èª¿ç”¨çˆ¶é¡çš„ç”Ÿæˆæ–¹æ³•
            result = super()._generate(messages, stop, **kwargs)
            
            # å„ªåŒ–è¿”å›å…§å®¹æ ¼å¼
            if result and result.generations:
                for generation in result.generations:
                    if hasattr(generation, 'message') and generation.message:
                        # å„ªåŒ–æ¶ˆæ¯å…§å®¹æ ¼å¼
                        self._optimize_message_content(generation.message)
            
            # è¿½è¹¤ token ä½¿ç”¨é‡
            self._track_token_usage(result, kwargs)
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Google AI ç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›ä¸€å€‹åŒ…å«éŒ¯èª¤ä¿¡æ¯çš„çµæœï¼Œè€Œä¸æ˜¯æŠ›å‡ºç•°å¸¸
            from langchain_core.outputs import ChatGeneration
            error_message = AIMessage(content=f"Google AI èª¿ç”¨å¤±è´¥: {str(e)}")
            error_generation = ChatGeneration(message=error_message)
            return LLMResult(generations=[[error_generation]])
    
    def _optimize_message_content(self, message: BaseMessage):
        """å„ªåŒ–æ¶ˆæ¯å…§å®¹æ ¼å¼ï¼Œç¢ºä¿åŒ…å«æ–°èç‰¹å¾é—œé”®è©"""
        
        if not isinstance(message, AIMessage) or not message.content:
            return
        
        content = message.content
        
        # æª¢æŸ¥æ˜¯å¦æ˜¯å·¥å…·èª¿ç”¨è¿”å›çš„æ–°èå…§å®¹
        if self._is_news_content(content):
            # å„ªåŒ–æ–°èå…§å®¹æ ¼å¼ï¼Œæ·»åŠ å¿…è¦çš„é—œé”®è©
            optimized_content = self._enhance_news_content(content)
            message.content = optimized_content
            
            logger.debug(f"ğŸ”§ [Googleé©é…å™¨] å„ªåŒ–æ–°èå…§å®¹æ ¼å¼")
            logger.debug(f"   åŸå§‹é•·åº¦: {len(content)} å­—ç¬¦")
            logger.debug(f"   å„ªåŒ–å¾Œé•·åº¦: {len(optimized_content)} å­—ç¬¦")
    
    def _is_news_content(self, content: str) -> bool:
        """åˆ¤æ–·å…§å®¹æ˜¯å¦ç‚ºæ–°èå…§å®¹"""
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ–°èç›¸é—œçš„é—œé”®è©
        news_indicators = [
            "è‚¡ç¥¨", "å…¬å¸", "å¸‚å ´", "æŠ•è³‡", "è²¡ç¶“", "è­‰åˆ¸", "äº¤æ˜“",
            "æ¶¨è·Œ", "æ¥­ç»©", "è²¡å ±", "åˆ†æ", "é æ¸¬", "æ¶ˆæ¯", "å…¬å‘Š"
        ]
        
        return any(indicator in content for indicator in news_indicators) and len(content) > 200
    
    def _enhance_news_content(self, content: str) -> str:
        """å¢å¼ºæ–°èå…§å®¹ï¼Œæ·»åŠ å¿…è¦çš„æ ¼å¼åŒ–ä¿¡æ¯"""
        
        import datetime
        current_date = datetime.datetime.now().strftime("%Y-%m-%d")
        
        # å¦‚æœå…§å®¹ç¼ºå°‘å¿…è¦çš„æ–°èç‰¹å¾ï¼Œæ·»åŠ å®ƒå€‘
        enhanced_content = content
        
        # æ·»åŠ ç™¼å¸ƒæ™‚é–“ä¿¡æ¯ï¼ˆå¦‚æœç¼ºå°‘ï¼‰
        if "ç™¼å¸ƒæ™‚é–“" not in content and "æ™‚é–“" not in content:
            enhanced_content = f"ç™¼å¸ƒæ™‚é–“: {current_date}\n\n{enhanced_content}"
        
        # æ·»åŠ æ–°èæ¨™é¡Œæ¨™è¯†ï¼ˆå¦‚æœç¼ºå°‘ï¼‰
        if "æ–°èæ¨™é¡Œ" not in content and "æ¨™é¡Œ" not in content:
            # å˜—è©¦å¾å…§å®¹ä¸­æå–ç¬¬ä¸€è¡Œä½œç‚ºæ¨™é¡Œ
            lines = enhanced_content.split('\n')
            if lines:
                first_line = lines[0].strip()
                if len(first_line) < 100:  # å¯èƒ½æ˜¯æ¨™é¡Œ
                    enhanced_content = f"æ–°èæ¨™é¡Œ: {first_line}\n\n{enhanced_content}"
        
        # æ·»åŠ æ–‡ç« ä¾†æºä¿¡æ¯ï¼ˆå¦‚æœç¼ºå°‘ï¼‰
        if "æ–‡ç« ä¾†æº" not in content and "ä¾†æº" not in content:
            enhanced_content = f"{enhanced_content}\n\næ–‡ç« ä¾†æº: Google AI æ™ºèƒ½åˆ†æ"
        
        return enhanced_content
    
    def _track_token_usage(self, result: LLMResult, kwargs: Dict[str, Any]):
        """è¿½è¹¤ token ä½¿ç”¨é‡"""
        
        try:
            # å¾çµæœä¸­æå– token ä½¿ç”¨ä¿¡æ¯
            if hasattr(result, 'llm_output') and result.llm_output:
                token_usage = result.llm_output.get('token_usage', {})
                
                input_tokens = token_usage.get('prompt_tokens', 0)
                output_tokens = token_usage.get('completion_tokens', 0)
                
                if input_tokens > 0 or output_tokens > 0:
                    # ç”Ÿæˆæœƒè©±ID
                    session_id = kwargs.get('session_id', f"google_openai_{hash(str(kwargs))%10000}")
                    analysis_type = kwargs.get('analysis_type', 'stock_analysis')
                    
                    # ä½¿ç”¨ TokenTracker è¨˜éŒ„ä½¿ç”¨é‡
                    token_tracker.track_usage(
                        provider="google",
                        model_name=self.model,
                        input_tokens=input_tokens,
                        output_tokens=output_tokens,
                        session_id=session_id,
                        analysis_type=analysis_type
                    )
                    
                    logger.debug(f"ğŸ“Š [Googleé©é…å™¨] Tokenä½¿ç”¨é‡: è¼¸å…¥={input_tokens}, è¼¸å‡º={output_tokens}")
                    
        except Exception as track_error:
            # token è¿½è¹¤å¤±è´¥ä¸æ‡‰è¯¥å½±éŸ¿ä¸»è¦åŠŸèƒ½
            logger.error(f"âš ï¸ Googleé©é…å™¨ Token è¿½è¹¤å¤±è´¥: {track_error}")


# æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
GOOGLE_OPENAI_MODELS = {
    # Gemini 2.5 ç³»åˆ— - æœ€æ–°é©—è­‰æ¨¡å‹
    "gemini-2.5-pro": {
        "description": "Gemini 2.5 Pro - æœ€æ–°æ——èˆ°æ¨¡å‹ï¼ŒåŠŸèƒ½å¼ºå¤§ (16.68s)",
        "context_length": 32768,
        "supports_function_calling": True,
        "recommended_for": ["è¤‡é›œæ¨ç†", "å°ˆæ¥­åˆ†æ", "é«˜è´¨é‡è¼¸å‡º"],
        "avg_response_time": 16.68
    },
    "gemini-2.5-flash": {
        "description": "Gemini 2.5 Flash - æœ€æ–°å¿«é€Ÿæ¨¡å‹ (2.73s)",
        "context_length": 32768,
        "supports_function_calling": True,
        "recommended_for": ["å¿«é€ŸéŸ¿æ‡‰", "å¯¦æ™‚åˆ†æ", "é«˜é »ä½¿ç”¨"],
        "avg_response_time": 2.73
    },
    "gemini-2.5-flash-lite-preview-06-17": {
        "description": "Gemini 2.5 Flash Lite Preview - è¶…å¿«éŸ¿æ‡‰ (1.45s)",
        "context_length": 32768,
        "supports_function_calling": True,
        "recommended_for": ["è¶…å¿«éŸ¿æ‡‰", "å¯¦æ™‚äº¤äº’", "é«˜é »èª¿ç”¨"],
        "avg_response_time": 1.45
    },
    # Gemini 2.0 ç³»åˆ—
    "gemini-2.0-flash": {
        "description": "Gemini 2.0 Flash - æ–°ä¸€ä»£å¿«é€Ÿæ¨¡å‹ (1.87s)",
        "context_length": 32768,
        "supports_function_calling": True,
        "recommended_for": ["å¿«é€ŸéŸ¿æ‡‰", "å¯¦æ™‚åˆ†æ"],
        "avg_response_time": 1.87
    },
    # Gemini 1.5 ç³»åˆ—
    "gemini-1.5-pro": {
        "description": "Gemini 1.5 Pro - å¼ºå¤§æ€§èƒ½ï¼Œå¹³è¡¡é¸æ“‡ (2.25s)",
        "context_length": 32768,
        "supports_function_calling": True,
        "recommended_for": ["è¤‡é›œåˆ†æ", "å°ˆæ¥­ä»»å‹™", "æ·±åº¦æ€è€ƒ"],
        "avg_response_time": 2.25
    },
    "gemini-1.5-flash": {
        "description": "Gemini 1.5 Flash - å¿«é€ŸéŸ¿æ‡‰ï¼Œå¤‡ç”¨é¸æ“‡ (2.87s)",
        "context_length": 32768,
        "supports_function_calling": True,
        "recommended_for": ["å¿«é€Ÿä»»å‹™", "æ—¥å¸¸å°è©±", "ç°¡å–®åˆ†æ"],
        "avg_response_time": 2.87
    },
    # ç¶“å…¸æ¨¡å‹
    "gemini-pro": {
        "description": "Gemini Pro - ç¶“å…¸æ¨¡å‹ï¼Œç©©å®šå¯é ",
        "context_length": 32768,
        "supports_function_calling": True,
        "recommended_for": ["é€šç”¨ä»»å‹™", "ç©©å®šæ€§è¦æ±‚é«˜çš„å ´æ™¯"]
    }
}


def get_available_google_models() -> Dict[str, Dict[str, Any]]:
    """ç²å–å¯ç”¨çš„ Google AI æ¨¡å‹åˆ—è¡¨"""
    return GOOGLE_OPENAI_MODELS


def create_google_openai_llm(
    model: str = "gemini-2.5-flash-lite-preview-06-17",
    google_api_key: Optional[str] = None,
    temperature: float = 0.1,
    max_tokens: int = 2000,
    **kwargs
) -> ChatGoogleOpenAI:
    """å‰µå»º Google AI OpenAI å…¼å®¹ LLM å¯¦ä¾‹çš„ä¾¿æ·å‡½æ•¸"""
    
    return ChatGoogleOpenAI(
        model=model,
        google_api_key=google_api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        **kwargs
    )


def test_google_openai_connection(
    model: str = "gemini-2.0-flash",
    google_api_key: Optional[str] = None
) -> bool:
    """æ¸¬è©¦ Google AI OpenAI å…¼å®¹æ¥å£é€£æ¥"""
    
    try:
        logger.info(f"ğŸ§ª æ¸¬è©¦ Google AI OpenAI å…¼å®¹æ¥å£é€£æ¥")
        logger.info(f"   æ¨¡å‹: {model}")
        
        # å‰µå»ºå®¢æˆ¶ç«¯
        llm = create_google_openai_llm(
            model=model,
            google_api_key=google_api_key,
            max_tokens=50
        )
        
        # ç™¼é€æ¸¬è©¦æ¶ˆæ¯
        response = llm.invoke("ä½ å¥½ï¼Œè«‹ç°¡å–®ä»‹ç´¹ä¸€ä¸‹ä½ è‡ªå·±ã€‚")
        
        if response and hasattr(response, 'content') and response.content:
            logger.info(f"âœ… Google AI OpenAI å…¼å®¹æ¥å£é€£æ¥æˆåŠŸ")
            logger.info(f"   éŸ¿æ‡‰: {response.content[:100]}...")
            return True
        else:
            logger.error(f"âŒ Google AI OpenAI å…¼å®¹æ¥å£éŸ¿æ‡‰ç‚ºç©º")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Google AI OpenAI å…¼å®¹æ¥å£é€£æ¥å¤±è´¥: {e}")
        return False


def test_google_openai_function_calling(
    model: str = "gemini-2.5-flash-lite-preview-06-17",
    google_api_key: Optional[str] = None
) -> bool:
    """æ¸¬è©¦ Google AI OpenAI å…¼å®¹æ¥å£çš„ Function Calling"""
    
    try:
        logger.info(f"ğŸ§ª æ¸¬è©¦ Google AI Function Calling")
        logger.info(f"   æ¨¡å‹: {model}")
        
        # å‰µå»ºå®¢æˆ¶ç«¯
        llm = create_google_openai_llm(
            model=model,
            google_api_key=google_api_key,
            max_tokens=200
        )
        
        # å®šç¾©æ¸¬è©¦å·¥å…·
        from langchain_core.tools import tool
        
        @tool
        def test_news_tool(query: str) -> str:
            """æ¸¬è©¦æ–°èå·¥å…·ï¼Œè¿”å›æ¨¡æ“¬æ–°èå…§å®¹"""
            return f"""ç™¼å¸ƒæ™‚é–“: 2024-01-15
æ–°èæ¨™é¡Œ: {query}ç›¸é—œå¸‚å ´å‹•æ…‹
æ–‡ç« ä¾†æº: æ¸¬è©¦æ–°èæº

é€™æ˜¯ä¸€æ¢é—œæ–¼{query}çš„æ¸¬è©¦æ–°èå…§å®¹ã€‚è¯¥å…¬å¸è¿‘æœŸè¡¨ç¾è‰¯å¥½ï¼Œå¸‚å ´å‰æ™¯çœ‹å¥½ã€‚
æŠ•è³‡è€…å°æ­¤è¡¨ç¤ºé—œè¨»ï¼Œåˆ†æå¸«çµ¦å‡ºç©æ¥µè©•åƒ¹ã€‚"""
        
        # ç»‘å®šå·¥å…·
        llm_with_tools = llm.bind_tools([test_news_tool])
        
        # æ¸¬è©¦å·¥å…·èª¿ç”¨
        response = llm_with_tools.invoke("è«‹ä½¿ç”¨test_news_toolæŸ¥è©¢'è˜‹æœå…¬å¸'çš„æ–°è")
        
        logger.info(f"âœ… Google AI Function Calling æ¸¬è©¦å®Œæˆ")
        logger.info(f"   éŸ¿æ‡‰é¡å‹: {type(response)}")
        
        if hasattr(response, 'tool_calls') and response.tool_calls:
            logger.info(f"   å·¥å…·èª¿ç”¨æ•¸é‡: {len(response.tool_calls)}")
            return True
        else:
            logger.info(f"   éŸ¿æ‡‰å…§å®¹: {getattr(response, 'content', 'No content')}")
            return True  # å³ä½¿æ²¡æœ‰å·¥å…·èª¿ç”¨ä¹Ÿç®—æˆåŠŸï¼Œå› ç‚ºæ¨¡å‹å¯èƒ½é¸æ“‡ä¸èª¿ç”¨å·¥å…·
            
    except Exception as e:
        logger.error(f"âŒ Google AI Function Calling æ¸¬è©¦å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    """æ¸¬è©¦è…³æœ¬"""
    logger.info(f"ğŸ§ª Google AI OpenAI å…¼å®¹é©é…å™¨æ¸¬è©¦")
    logger.info(f"=" * 50)
    
    # æ¸¬è©¦é€£æ¥
    connection_ok = test_google_openai_connection()
    
    if connection_ok:
        # æ¸¬è©¦ Function Calling
        function_calling_ok = test_google_openai_function_calling()
        
        if function_calling_ok:
            logger.info(f"\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼Google AI OpenAI å…¼å®¹é©é…å™¨å·¥ä½œæ­£å¸¸")
        else:
            logger.error(f"\nâš ï¸ Function Calling æ¸¬è©¦å¤±è´¥")
    else:
        logger.error(f"\nâŒ é€£æ¥æ¸¬è©¦å¤±è´¥")