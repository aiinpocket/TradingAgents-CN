"""
DeepSeek LLMé©é…å™¨ï¼Œæ”¯æŒTokenä½¿ç”¨çµ±è¨ˆ
"""

import os
import time
from typing import Any, Dict, List, Optional, Union
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, SystemMessage
from langchain_core.outputs import ChatGeneration, ChatResult
from langchain_openai import ChatOpenAI
from langchain_core.callbacks import CallbackManagerForLLMRun

# å°å…¥çµ±ä¸€æ—¥èªŒç³»çµ±
from tradingagents.utils.logging_init import setup_llm_logging

# å°å…¥æ—¥èªŒæ¨¡å¡Š
from tradingagents.utils.logging_manager import get_logger, get_logger_manager
logger = get_logger('agents')
logger = setup_llm_logging()

# å°å…¥tokenè·Ÿè¹¤å™¨
try:
    from tradingagents.config.config_manager import token_tracker
    TOKEN_TRACKING_ENABLED = True
    logger.info("âœ… Tokenè·Ÿè¹¤åŠŸèƒ½å·²å•Ÿç”¨")
except ImportError:
    TOKEN_TRACKING_ENABLED = False
    logger.warning("âš ï¸ Tokenè·Ÿè¹¤åŠŸèƒ½æœªå•Ÿç”¨")


class ChatDeepSeek(ChatOpenAI):
    """
    DeepSeekèŠå¤©æ¨¡å‹é©é…å™¨ï¼Œæ”¯æŒTokenä½¿ç”¨çµ±è¨ˆ
    
    ç¹¼æ‰¿è‡ªChatOpenAIï¼Œæ·»åŠ äº†Tokenä½¿ç”¨é‡çµ±è¨ˆåŠŸèƒ½
    """
    
    def __init__(
        self,
        model: str = "deepseek-chat",
        api_key: Optional[str] = None,
        base_url: str = "https://api.deepseek.com",
        temperature: float = 0.1,
        max_tokens: Optional[int] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–DeepSeeké©é…å™¨
        
        Args:
            model: æ¨¡å‹åç¨±ï¼Œé»˜èªç‚ºdeepseek-chat
            api_key: APIå¯†é‘°ï¼Œå¦‚æœä¸æä¾›å‰‡å¾ç’°å¢ƒè®Šé‡DEEPSEEK_API_KEYç²å–
            base_url: APIåŸºç¡€URL
            temperature: æº«åº¦åƒæ•¸
            max_tokens: æœ€å¤§tokenæ•¸
            **kwargs: å…¶ä»–åƒæ•¸
        """
        
        # ç²å–APIå¯†é‘°
        if api_key is None:
            api_key = os.getenv("DEEPSEEK_API_KEY")
            if not api_key:
                raise ValueError("DeepSeek APIå¯†é‘°æœªæ‰¾åˆ°ã€‚è«‹è¨­ç½®DEEPSEEK_API_KEYç’°å¢ƒè®Šé‡æˆ–å‚³å…¥api_keyåƒæ•¸ã€‚")
        
        # åˆå§‹åŒ–çˆ¶é¡
        super().__init__(
            model=model,
            openai_api_key=api_key,
            openai_api_base=base_url,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
        
        self.model_name = model
        
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """
        ç”ŸæˆèŠå¤©éŸ¿æ‡‰ï¼Œä¸¦è¨˜éŒ„tokenä½¿ç”¨é‡
        """

        # è¨˜éŒ„é–‹å§‹æ™‚é–“
        start_time = time.time()

        # æå–ä¸¦ç§»é™¤è‡ªå®šç¾©åƒæ•¸ï¼Œé¿å…å‚³éçµ¦çˆ¶é¡
        session_id = kwargs.pop('session_id', None)
        analysis_type = kwargs.pop('analysis_type', None)

        try:
            # èª¿ç”¨çˆ¶é¡æ–¹æ³•ç”ŸæˆéŸ¿æ‡‰
            result = super()._generate(messages, stop, run_manager, **kwargs)
            
            # æå–tokenä½¿ç”¨é‡
            input_tokens = 0
            output_tokens = 0
            
            # å˜—è©¦å¾éŸ¿æ‡‰ä¸­æå–tokenä½¿ç”¨é‡
            if hasattr(result, 'llm_output') and result.llm_output:
                token_usage = result.llm_output.get('token_usage', {})
                if token_usage:
                    input_tokens = token_usage.get('prompt_tokens', 0)
                    output_tokens = token_usage.get('completion_tokens', 0)
            
            # å¦‚æœæ²¡æœ‰ç²å–åˆ°tokenä½¿ç”¨é‡ï¼Œé€²è¡Œä¼°ç®—
            if input_tokens == 0 and output_tokens == 0:
                input_tokens = self._estimate_input_tokens(messages)
                output_tokens = self._estimate_output_tokens(result)
                logger.debug(f"ğŸ” [DeepSeek] ä½¿ç”¨ä¼°ç®—token: è¼¸å…¥={input_tokens}, è¼¸å‡º={output_tokens}")
            else:
                logger.info(f"ğŸ“Š [DeepSeek] å¯¦é™…tokenä½¿ç”¨: è¼¸å…¥={input_tokens}, è¼¸å‡º={output_tokens}")
            
            # è¨˜éŒ„tokenä½¿ç”¨é‡
            if TOKEN_TRACKING_ENABLED and (input_tokens > 0 or output_tokens > 0):
                try:
                    # ä½¿ç”¨æå–çš„åƒæ•¸æˆ–ç”Ÿæˆé»˜èªå€¼
                    if session_id is None:
                        session_id = f"deepseek_{hash(str(messages))%10000}"
                    if analysis_type is None:
                        analysis_type = 'stock_analysis'

                    # è¨˜éŒ„ä½¿ç”¨é‡
                    usage_record = token_tracker.track_usage(
                        provider="deepseek",
                        model_name=self.model_name,
                        input_tokens=input_tokens,
                        output_tokens=output_tokens,
                        session_id=session_id,
                        analysis_type=analysis_type
                    )

                    if usage_record:
                        if usage_record.cost == 0.0:
                            logger.warning(f"âš ï¸ [DeepSeek] æˆæœ¬è¨ˆç®—ç‚º0ï¼Œå¯èƒ½é…ç½®æœ‰å•é¡Œ")
                        else:
                            logger.info(f"ğŸ’° [DeepSeek] æœ¬æ¬¡èª¿ç”¨æˆæœ¬: Â¥{usage_record.cost:.6f}")

                        # ä½¿ç”¨çµ±ä¸€æ—¥èªŒç®¡ç†å™¨çš„Tokenè¨˜éŒ„æ–¹æ³•
                        logger_manager = get_logger_manager()
                        logger_manager.log_token_usage(
                            logger, "deepseek", self.model_name,
                            input_tokens, output_tokens, usage_record.cost,
                            session_id
                        )
                    else:
                        logger.warning(f"âš ï¸ [DeepSeek] æœªå‰µå»ºä½¿ç”¨è¨˜éŒ„")

                except Exception as track_error:
                    logger.error(f"âš ï¸ [DeepSeek] Tokençµ±è¨ˆå¤±è´¥: {track_error}", exc_info=True)
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ [DeepSeek] èª¿ç”¨å¤±è´¥: {e}", exc_info=True)
            raise
    
    def _estimate_input_tokens(self, messages: List[BaseMessage]) -> int:
        """
        ä¼°ç®—è¼¸å…¥tokenæ•¸é‡
        
        Args:
            messages: è¼¸å…¥æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            ä¼°ç®—çš„è¼¸å…¥tokenæ•¸é‡
        """
        total_chars = 0
        for message in messages:
            if hasattr(message, 'content'):
                total_chars += len(str(message.content))
        
        # ç²—ç•¥ä¼°ç®—ï¼šä¸­æ–‡ç´„1.5å­—ç¬¦/tokenï¼Œè‹±æ–‡ç´„4å­—ç¬¦/token
        # é€™é‡Œä½¿ç”¨ä¿å®ˆä¼°ç®—ï¼š2å­—ç¬¦/token
        estimated_tokens = max(1, total_chars // 2)
        return estimated_tokens
    
    def _estimate_output_tokens(self, result: ChatResult) -> int:
        """
        ä¼°ç®—è¼¸å‡ºtokenæ•¸é‡
        
        Args:
            result: èŠå¤©çµæœ
            
        Returns:
            ä¼°ç®—çš„è¼¸å‡ºtokenæ•¸é‡
        """
        total_chars = 0
        for generation in result.generations:
            if hasattr(generation, 'message') and hasattr(generation.message, 'content'):
                total_chars += len(str(generation.message.content))
        
        # ç²—ç•¥ä¼°ç®—ï¼š2å­—ç¬¦/token
        estimated_tokens = max(1, total_chars // 2)
        return estimated_tokens
    
    def invoke(
        self,
        input: Union[str, List[BaseMessage]],
        config: Optional[Dict] = None,
        **kwargs: Any,
    ) -> AIMessage:
        """
        èª¿ç”¨æ¨¡å‹ç”ŸæˆéŸ¿æ‡‰
        
        Args:
            input: è¼¸å…¥æ¶ˆæ¯
            config: é…ç½®åƒæ•¸
            **kwargs: å…¶ä»–åƒæ•¸ï¼ˆåŒ…æ‹¬session_idå’Œanalysis_typeï¼‰
            
        Returns:
            AIæ¶ˆæ¯éŸ¿æ‡‰
        """
        
        # è™•ç†è¼¸å…¥
        if isinstance(input, str):
            messages = [HumanMessage(content=input)]
        else:
            messages = input
        
        # èª¿ç”¨ç”Ÿæˆæ–¹æ³•
        result = self._generate(messages, **kwargs)
        
        # è¿”å›ç¬¬ä¸€å€‹ç”Ÿæˆçµæœçš„æ¶ˆæ¯
        if result.generations:
            return result.generations[0].message
        else:
            return AIMessage(content="")


def create_deepseek_llm(
    model: str = "deepseek-chat",
    temperature: float = 0.1,
    max_tokens: Optional[int] = None,
    **kwargs
) -> ChatDeepSeek:
    """
    å‰µå»ºDeepSeek LLMå¯¦ä¾‹çš„ä¾¿æ·å‡½æ•¸
    
    Args:
        model: æ¨¡å‹åç¨±
        temperature: æº«åº¦åƒæ•¸
        max_tokens: æœ€å¤§tokenæ•¸
        **kwargs: å…¶ä»–åƒæ•¸
        
    Returns:
        ChatDeepSeekå¯¦ä¾‹
    """
    return ChatDeepSeek(
        model=model,
        temperature=temperature,
        max_tokens=max_tokens,
        **kwargs
    )


# ç‚ºäº†å‘å¾Œå…¼å®¹ï¼Œæä¾›åˆ¥å
DeepSeekLLM = ChatDeepSeek
